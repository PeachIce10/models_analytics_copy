Below is a clean, unambiguous, end-to-end outline using explicit action tags.

Legend
	•	[PROVISION SYSTEM] provision external infra (outside repo)
	•	[ADD NEW FOLDER] create folder (lists required files inside)
	•	[ADD NEW FILE] create file
	•	[EDIT EXISTING FILE] modify code inside an existing file

⸻

0) System Packages to Provision (must have)
	•	[PROVISION SYSTEM] Container Registry — store versioned images for all analytics/jobs/** and analytics/signals.
	•	[PROVISION SYSTEM] Kubernetes Cluster — run CronJobs (batch jobs) and Deployment/Service (serving).
	•	[PROVISION SYSTEM] Secrets Manager / K8s Secrets — hold DSNs and credentials (CORE_PG_DSN, CORE_CH_DSN, SIGNALS_PG_DSN, SIGNALS_CH_DSN).
	•	[PROVISION SYSTEM] Postgres — inputs + serving outputs (customer_segments, product_signals, and new datasets below).
	•	[PROVISION SYSTEM] ClickHouse — wide scans; stores affinity_edges.
	•	[PROVISION SYSTEM] Observability (Prometheus + Grafana or cloud metrics) — scrape metrics (jobs + signals), alert on SLO breaches.
	•	[PROVISION SYSTEM] CI/CD Runners (GitHub/GitLab) — build/test, docker build/push, apply K8s manifests.
	•	[PROVISION SYSTEM] ANN Engine (OpenSearch k-NN or Milvus or FAISS-serving) — required for #7/#2/#12 if you want ANN lookup ≤50 ms at serving.
	•	[PROVISION SYSTEM] Optimizer Runtime (OR-Tools or CBC/GLPK) — required for #4 knapsack and #9 markdown DP/MILP batch jobs.

⸻

1) Repo Root: analytics/ (what to keep vs change)

1.1 analytics/common/
	•	[EDIT EXISTING FILE] analytics/common/config/load.go
Purpose: config loader for both signals.yaml and per-model YAMLs.
Add inside this file:
	•	LoadSignalsConfig(path): read YAML → apply env overrides for dsn_env fields → validate required keys → return SignalsConfig.
	•	LoadJobConfig(path): same pattern for per-model YAMLs → validate required keys and job_slo.
	•	applyEnvOverrides(cfg): for each dsn_env, set the actual DSN value from os.Getenv.
	•	validateSignals(cfg): require serving.<dataset>.store, serving.<dataset>.dsn_env, slo.freshness_hours, slo.min_rows, slo.timeouts_ms, slo.paging.max_limit.
	•	validateJob(cfg): require inputs[].store, inputs[].dsn_env, output.table, output.store, output.dsn_env, window, model_version, job_slo.{max_runtime_sec,max_retries,backoff_sec}.
	•	Fail fast (return error) on any missing key or empty env.
	•	[EDIT EXISTING FILE] analytics/common/datastore/postgres.go
Purpose: read inputs, upsert outputs in Postgres.
Add inside this file:
	•	All queries use context.WithTimeout.
	•	Helpers: SelectFreshness(table), SelectCount(table), UpsertRows(table, rows, keys=[id,model_version,as_of]).
	•	New readers you’ll need: returns, price history, search logs (for CLV/Elasticity/PLP).
	•	Logging with rows affected; bubble up errors with context.
	•	[EDIT EXISTING FILE] analytics/common/datastore/clickhouse.go
Purpose: wide scans + affinity writes.
Add inside this file: same timeouts + SelectFreshness/SelectCount helpers; efficient inserts; error wrapping.
	•	[EDIT EXISTING FILE] analytics/common/timewin/window.go
Purpose: rolling window helpers.
Add inside this file: presets last_30d, last_90d, last_180d, last_365d + Custom(start,end).

⸻

1.2 analytics/configs/
	•	[EDIT EXISTING FILE] analytics/configs/signals.yaml
Purpose: bind datasets to stores + SLOs for serving.
Add inside this file:
	•	serving for every dataset you’ll serve (see §3 datasets list).
	•	slo: freshness_hours, min_rows, timeouts_ms, paging.max_limit.
	•	Confirm DSN env names match your Secrets.
	•	[ADD NEW FILE] analytics/configs/<model>.yaml (one per model you implement)
Purpose: per-job IO and bounds.
Contents template:
	•	inputs: array of sources with {store: postgres|clickhouse, dsn_env: CORE_*} and any table/filters.
	•	window: one of last_30d/90d/180d/365d or custom.
	•	output: {table: <dataset_name>, store: postgres|clickhouse, dsn_env: SIGNALS_*}
	•	model_version: semantic tag (e.g., clv-1.0.0).
	•	job_slo: {max_runtime_sec, max_retries, backoff_sec}.

⸻

1.3 analytics/jobs/ (batch compute)

Global job pattern (applies to all jobs below):
	•	[ADD NEW FILE] cmd/main.go
	•	Load configs/<model>.yaml via LoadJobConfig.
	•	Create context with job_slo.max_runtime_sec.
	•	Read inputs using common/datastore/* by store setting.
	•	Run pipeline/<model>_pipeline.go.
	•	Run pipeline/<model>_validate.go (required IDs, dedupe by (id,model_version,as_of), numeric bounds).
	•	Upsert outputs using Postgres helper (even if inputs from CH).
	•	Retry transient failures (max_retries, backoff_sec).
	•	Log: rows written, duration, window, model_version; exit non-zero on failure.
	•	[ADD NEW FILE] pipeline/<model>_pipeline.go
	•	Implement the model’s computation (pure code; no network except DB reads).
	•	Keep money in cents; keep probabilities [0,1].
	•	Return well-typed rows matching the schema.
	•	[ADD NEW FILE] pipeline/<model>_validate.go
	•	Enforce required fields, non-negative numbers, dedupe keys.
	•	Enforce sensible bounds (e.g., cover days ≥ 0).
	•	[ADD NEW FILE] Dockerfile (in each job folder)
	•	Build static Go binary; copy configs mount path; set non-root user.
	•	[ADD NEW FILE] deploy/k8s/jobs/<model>-cronjob.yaml (manifests folder; see §2.2)
	•	Reference image tag; schedule; env (DSNs); mount configs.
	•	Set activeDeadlineSeconds, backoffLimit, resource requests/limits.

Jobs to add (one folder each) aligned to your 15 models
(These are anticipated additions—they are part of the plan, not unexpected gaps.)

	•	[ADD NEW FOLDER] analytics/jobs/customer/clv_bgnbd/ — CLV scoring (BG/NBD + Gamma–Gamma) → outputs tiers_clv.
	•	[ADD NEW FOLDER] analytics/jobs/search/ranknet_train/ — learn rank weights from logs → outputs rank_weights.
	•	[ADD NEW FOLDER] analytics/jobs/campaign/promotion_uplift/ — incremental GM + assignment → outputs promotion_assignments.
	•	[ADD NEW FOLDER] analytics/jobs/cart/upsell_propensity/ — upsell decision scores → outputs upsell_scores.
	•	[ADD NEW FOLDER] analytics/jobs/pdp/cross_sell/ — complements lists → outputs cross_sell.
	•	[ADD NEW FOLDER] analytics/jobs/assortment/bundle_optimizer/ — bundle picks → outputs bundles.
	•	[ADD NEW FOLDER] analytics/jobs/customer/personas_hdbscan/ — persona labels → outputs personas.
	•	[ADD NEW FOLDER] analytics/jobs/pricing/markdown_optimizer/ — markdown ladder → outputs markdown_plans.
	•	[ADD NEW FOLDER] analytics/jobs/pricing/elasticity/ — price elasticity → outputs elasticity.
	•	[ADD NEW FOLDER] analytics/jobs/customer/churn_propensity/ — churn risk → outputs churn_risk.
	•	[ADD NEW FOLDER] analytics/jobs/customer/lifecycle_hsmm/ — lifecycle state → outputs lifecycle_state.
	•	[ADD NEW FOLDER] analytics/jobs/product/similarity_index/ — build/load ANN index artifacts (vectors or normalized co-buy); publish similarity_artifacts.
	•	(You already have) analytics/jobs/customer/rfm/, analytics/jobs/product/velocity/, analytics/jobs/product/abcxyz/, analytics/jobs/product/affinity/ — keep and align to the same pattern.

⸻

1.4 analytics/schemas/models/ (output contracts)

Add a schema JSON for every dataset you will output or serve:
	•	[ADD NEW FILE] tiers_clv.schema.json — { user_id, clv_180_cents, tier, model_version, as_of }.
	•	[ADD NEW FILE] rank_weights.schema.json — { feature, weight, intercept?, model_version, as_of }.
	•	[ADD NEW FILE] promotion_assignments.schema.json — { user_id, arm, delta_gm_cents, ec_cents, model_version, as_of }.
	•	[ADD NEW FILE] upsell_scores.schema.json — { hero_product_id, candidate_id, net_score_cents, model_version, as_of }.
	•	[ADD NEW FILE] cross_sell.schema.json — { hero_product_id, complements:[{id,score}], model_version, as_of }.
	•	[ADD NEW FILE] bundles.schema.json — { slow_id, hero_id, score_cents, constraints, model_version, as_of }.
	•	[ADD NEW FILE] markdown_plans.schema.json — { sku_id, weeks:[{price,margin,qty}], total_gm_cents, model_version, as_of }.
	•	[ADD NEW FILE] elasticity.schema.json — { sku_id, elasticity, conf_low?, conf_high?, model_version, as_of }.
	•	[ADD NEW FILE] personas.schema.json — { user_id, persona_id, stability, model_version, as_of }.
	•	[ADD NEW FILE] churn_risk.schema.json — { user_id, p_churn, decision_value_cents, model_version, as_of }.
	•	[ADD NEW FILE] lifecycle_state.schema.json — { user_id, state, time_in_state, state_probs?, model_version, as_of }.
	•	[ADD NEW FILE] similarity_artifacts.schema.json — { index_id, version, path, built_at } (metadata row if you track artifacts in DB).
	•	(Already exist and remain) customer_segments.schema.json, product_signals.schema.json, affinity_edges.schema.json.

⸻

1.5 analytics/signals/ (serving: gRPC + GraphQL)
	•	[EDIT EXISTING FILE] signals/cmd/main.go
	•	Load signals.yaml with LoadSignalsConfig.
	•	Set default per-request timeout from slo.timeouts_ms.request_default.
	•	Expose GET /healthz (process + store pings OK) and GET /readyz (boot checks + freshness within SLO).
	•	Log one boot line: dataset→store binding and freshness snapshot.
	•	[EDIT EXISTING FILE] signals/internal/store/factory.go
	•	On boot: verify each serving.<dataset> has a matching store impl (*_store_pg.go or *_store_ch.go).
	•	Ping DSNs with timeout; probe SelectFreshness and SelectCount per dataset; fail fast if any breach.
	•	Hold bindings for later use by services and ops resolver.
	•	[EDIT EXISTING FILE] signals/internal/services/*_signals_service.go
	•	Enforce paging from slo.paging.max_limit.
	•	Emit metrics: signals_dataset_ok{dataset}, signals_dataset_rows{dataset}, signals_dataset_max_age_hours{dataset}.
	•	[EDIT EXISTING FILE] signals/internal/validators/*_validator.go
	•	Required IDs present; numeric clamps; paging limits.
	•	[ADD NEW FILE] signals/internal/graph/schema/ops.graphqls
	•	type OpsDatasetStatus { name: String!, store: String!, ok: Boolean!, rows: Int!, maxAgeHours: Int!, lastAsOf: Time, message: String }
	•	type OpsStatus { datasets: [OpsDatasetStatus!]!, startedAt: Time!, version: String! }
	•	extend type Query { opsStatus: OpsStatus! }
	•	[ADD NEW FILE] signals/internal/graph/schema/ops.resolver.go
	•	Build status from factory bindings; run fast COUNT + MAX(as_of); compare to SLO; return statuses.

Serving new datasets (add these only for datasets you plan to query via signals):
	•	For each dataset below, add proto + service + validator + GraphQL schema/resolver if you need it served:
	•	tiers_clv → [ADD NEW FILE] signals/proto/tiers_clv.proto • [ADD NEW FILE] signals/internal/services/tiers_clv_service.go • [ADD NEW FILE] signals/internal/graph/schema/tiers_clv.graphqls • [ADD NEW FILE] tiers_clv.resolver.go • validator as needed.
	•	rank_weights → add proto/service if you want to fetch weights online.
	•	promotion_assignments, upsell_scores, cross_sell, bundles, markdown_plans, elasticity, personas, churn_risk, lifecycle_state, similarity_artifacts → same pattern as needed.

⸻

2) Containerization & Deployment

2.1 Dockerfiles
	•	[ADD NEW FILE] analytics/signals/Dockerfile — build service; expose GraphQL/gRPC ports; run as non-root.
	•	[ADD NEW FILE] analytics/jobs/<domain>/<model>/Dockerfile — build static binary; set entrypoint to cmd/main.

2.2 K8s Manifests
	•	[ADD NEW FILE] deploy/k8s/services/signals-deployment.yaml — Deployment with liveness/readiness, env DSNs, resource limits.
	•	[ADD NEW FILE] deploy/k8s/services/signals-service.yaml — Service/Ingress for /graphql and gRPC port.
	•	[ADD NEW FILE] deploy/k8s/secrets/core-dsns.yaml — K8s Secret holding CORE_PG_DSN, CORE_CH_DSN.
	•	[ADD NEW FILE] deploy/k8s/secrets/signals-dsns.yaml — Secret holding SIGNALS_PG_DSN, SIGNALS_CH_DSN.
	•	[ADD NEW FILE] deploy/k8s/jobs/<model>-cronjob.yaml — one per job: image, schedule, env, config mount, deadlines, retries.

2.3 CI/CD & Tooling
	•	[ADD NEW FILE] .github/workflows/ci.yml — go build/test; docker build; push to registry (tag = model_version).
	•	[ADD NEW FILE] .github/workflows/deploy.yml — kubectl/Helm/Terraform apply for manifests.
	•	[ADD NEW FILE] Makefile — local build, docker, push, deploy.
	•	[ADD NEW FILE] docs/registry-policy.md — image repos, tag format, promotion rules.
	•	[ADD NEW FILE] env/.env.example — DSN var names expected by jobs/services.

2.4 Observability
	•	[ADD NEW FILE] deploy/observability/prometheus-scrape.yaml — scrape jobs and signals.
	•	[ADD NEW FILE] dashboards/analytics-ops.json — Grafana board (dataset_ok, rows, maxAgeHours, job rows, runtime).
	•	[EDIT EXISTING FILE] jobs & signals code — instrument counters/gauges per safety.yaml (e.g., job_rows_written{job}, job_runtime_seconds{job}).

⸻

3) Datasets you will output/serve (map from models → tables)
	•	#1 CLV → tiers_clv (Postgres): user_id, clv_180_cents, tier, model_version, as_of.
	•	#2/#8 Ranking → rank_weights (Postgres): learned feature weights; used by serving or batch scoring.
	•	#3 Cart Re-rank → upsell_scores (Postgres).
	•	#4 Promo Uplift → promotion_assignments (Postgres).
	•	#5 Personas → personas (Postgres).
	•	#6 Affinities → enrich existing; may also write product_signals.
	•	#7 Similarity → similarity_artifacts (meta); neighbors re-served from ANN engine or from DB.
	•	#9 Markdown → markdown_plans (Postgres).
	•	#10 Elasticity → elasticity (Postgres).
	•	#11 Upsell → upsell_scores (already listed).
	•	#12 Cross-sell → cross_sell (Postgres).
	•	#13 Bundles → bundles (Postgres).
	•	#14 Churn → churn_risk (Postgres).
	•	#15 Lifecycle → lifecycle_state (Postgres).

⸻

4) Periodic Parameter Adjustment (where “training/fit” jobs go)

(No LLMs. These are plain batch jobs that compute parameters from logs.)
	•	RankNet weights (#8):
	•	[ADD NEW FOLDER] analytics/jobs/search/ranknet_train/ (see §1.3 pattern).
	•	[ADD NEW FILE] analytics/configs/ranknet_train.yaml — inputs=search logs; output=rank_weights.
	•	[ADD NEW FILE] deploy/k8s/jobs/ranknet-train-cronjob.yaml — nightly or weekly.
	•	CLV hyperparams (#1):
	•	Either embed fitting into clv_bgnbd job once per window, or
	•	[ADD NEW FOLDER] analytics/jobs/customer/clv_fit/ — fit r, α, a, b, p, q, γ; output small clv_params table; scoring job reads them.
	•	Add YAML + CronJob as above.
	•	Promotion uplift model (#4), churn model (#14), upsell model (#11):
	•	If you use learned models (GBM/logit), create corresponding *_train jobs to produce coefficients; otherwise keep rule-based and skip.
	•	Similarity index (#7):
	•	[ADD NEW FOLDER] analytics/jobs/product/similarity_index/ — builds ANN index files and loads into the ANN engine; records metadata row in similarity_artifacts.

⸻

5) Security / Access (brief)
	•	[PROVISION SYSTEM] Secrets store DSNs;
	•	[ADD NEW FILE] deploy/k8s/rbac/serviceaccounts.yaml (if needed) — least privilege;
	•	DB roles: read-only for signals; read+write for jobs.

⸻

6) Runbooks (brief)
	•	Boot fails: read signals logs; fix signals.yaml mapping or DSNs; re-deploy.
	•	Stale dataset: check opsStatus; re-run the job; confirm rows and as_of.
	•	Backfill: run CronJob with override window and model_version bump.

⸻

Final Notes
	•	All “training/fit” mentions are plain batch code deployed like any other job (CronJob). No LLMs.
	•	Whenever I write [EDIT EXISTING FILE], it means put the new logic inside that file (not a new file).
	•	Whenever I write [ADD NEW FILE]/[ADD NEW FOLDER], it means create it at that exact path.

This outline is exhaustive and tagged so you can implement without ambiguity.









Yes—here’s exactly what to add for any new model, then two concrete examples.

Minimal file set per model (place these)
	•	configs/<model>.yaml — per-job IO, window, model_version, job_slo.
	•	schemas/models/<dataset>.schema.json — output row contract (includes id, model_version, as_of).
	•	jobs/<domain>/<model>/
	•	cmd/main.go — load config → run pipeline → validate → upsert.
	•	pipeline/<model>_pipeline.go — core computation.
	•	pipeline/<model>_validate.go — required keys, dedupe (id,model_version,as_of).
	•	Dockerfile
	•	deploy/k8s/jobs/<model>-cronjob.yaml — schedule, image, env (DSNs).
	•	(Serve it via signals?) add:
	•	signals/proto/<dataset>.proto
	•	signals/internal/services/<dataset>_service.go
	•	signals/internal/graph/schema/<dataset>.graphqls
	•	signals/internal/graph/schema/<dataset>.resolver.go
	•	Edit configs/signals.yaml to bind <dataset> → store/DSN.

⸻

Example A — #1 CLV (BG/NBD + Gamma–Gamma)

Dataset: tiers_clv
	•	configs/clv_bgnbd.yaml
	•	schemas/models/tiers_clv.schema.json
	•	jobs/customer/clv_bgnbd/
	•	cmd/main.go
	•	pipeline/clv_bgnbd_pipeline.go
	•	pipeline/clv_bgnbd_validate.go
	•	Dockerfile
	•	deploy/k8s/jobs/clv-bgnbd-cronjob.yaml
	•	(Serve via signals)
	•	signals/proto/tiers_clv.proto
	•	signals/internal/services/tiers_clv_service.go
	•	signals/internal/graph/schema/tiers_clv.graphqls
	•	signals/internal/graph/schema/tiers_clv.resolver.go
	•	Edit configs/signals.yaml → add serving.tiers_clv.

⸻

Example B — #12 Cross-sell (complements for hero item)

Dataset: cross_sell
	•	configs/cross_sell.yaml
	•	schemas/models/cross_sell.schema.json
	•	jobs/pdp/cross_sell/
	•	cmd/main.go
	•	pipeline/cross_sell_pipeline.go
	•	pipeline/cross_sell_validate.go
	•	Dockerfile
	•	deploy/k8s/jobs/cross-sell-cronjob.yaml
	•	(Serve via signals)
	•	signals/proto/cross_sell.proto
	•	signals/internal/services/cross_sell_service.go
	•	signals/internal/graph/schema/cross_sell.graphqls
	•	signals/internal/graph/schema/cross_sell.resolver.go
	•	Edit configs/signals.yaml → add serving.cross_sell.

⸻

Notes (same for any two you pick)
	•	If a model also needs periodic parameter fitting (e.g., RankNet weights), add a sibling jobs/<domain>/<model>_train/ folder + its config and CronJob; the serving/scoring job reads those params.
	•	Don’t forget DSN secrets in K8s and image tags in your CronJobs.