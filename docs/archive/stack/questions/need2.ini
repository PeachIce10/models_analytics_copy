Legend
	•	[PROVISION SYSTEM] provision external infra (outside repo)
	•	[ADD NEW FOLDER] create folder (lists required files inside)
	•	[ADD NEW FILE] create file
	•	[EDIT EXISTING FILE] modify code inside an existing file

⸻

0) System Packages to Provision (must have)
	•	[PROVISION SYSTEM] Container Registry — store versioned images for all analytics/jobs/** and analytics/signals.
	•	[PROVISION SYSTEM] Kubernetes Cluster — run CronJobs (batch jobs) and Deployment/Service (serving).
	•	[PROVISION SYSTEM] Secrets Manager / K8s Secrets — hold DSNs and credentials (CORE_PG_DSN, CORE_CH_DSN, SIGNALS_PG_DSN, SIGNALS_CH_DSN).
	•	[PROVISION SYSTEM] Postgres — inputs + serving outputs (customer_segments, product_signals, and new datasets below).
	•	[PROVISION SYSTEM] ClickHouse — wide scans; stores affinity_edges.
	•	[PROVISION SYSTEM] Observability (Prometheus + Grafana or cloud metrics) — scrape metrics (jobs + signals), alert on SLO breaches.
	•	[PROVISION SYSTEM] CI/CD Runners (GitHub/GitLab) — build/test, docker build/push, apply K8s manifests.
	•	[PROVISION SYSTEM] ANN Engine (OpenSearch k-NN or Milvus or FAISS-serving) — required for #7/#2/#12 if you want ANN lookup ≤50 ms at serving.
	•	[PROVISION SYSTEM] Optimizer Runtime (OR-Tools or CBC/GLPK) — required for #4 knapsack and #9 markdown DP/MILP batch jobs.

⸻

1) Repo Root: analytics/ (what to keep vs change)

1.1 analytics/common/
	•	[EDIT EXISTING FILE] analytics/common/config/load.go
Purpose: config loader for both signals.yaml and per-model YAMLs.
Add inside this file:
	•	LoadSignalsConfig(path): read YAML → apply env overrides for dsn_env fields → validate required keys → return SignalsConfig.
	•	LoadJobConfig(path): same pattern for per-model YAMLs → validate required keys and job_slo.
	•	applyEnvOverrides(cfg): for each dsn_env, set the actual DSN value from os.Getenv.
	•	validateSignals(cfg): require serving.<dataset>.store, serving.<dataset>.dsn_env, slo.freshness_hours, slo.min_rows, slo.timeouts_ms, slo.paging.max_limit.
	•	validateJob(cfg): require inputs[].store, inputs[].dsn_env, output.table, output.store, output.dsn_env, window, model_version, job_slo.{max_runtime_sec,max_retries,backoff_sec}.
	•	Fail fast (return error) on any missing key or empty env.
	•	[EDIT EXISTING FILE] analytics/common/datastore/postgres.go
Purpose: read inputs, upsert outputs in Postgres.
Add inside this file:
	•	All queries use context.WithTimeout.
	•	Helpers: SelectFreshness(table), SelectCount(table), UpsertRows(table, rows, keys=[id,model_version,as_of]).
	•	New readers you’ll need: returns, price history, search logs (for CLV/Elasticity/PLP).
	•	Logging with rows affected; bubble up errors with context.
	•	[EDIT EXISTING FILE] analytics/common/datastore/clickhouse.go
Purpose: wide scans + affinity writes.
Add inside this file: same timeouts + SelectFreshness/SelectCount helpers; efficient inserts; error wrapping.
	•	[EDIT EXISTING FILE] analytics/common/timewin/window.go
Purpose: rolling window helpers.
Add inside this file: presets last_30d, last_90d, last_180d, last_365d + Custom(start,end).

⸻

1.2 analytics/configs/
	•	[EDIT EXISTING FILE] analytics/configs/signals.yaml
Purpose: bind datasets to stores + SLOs for serving.
Add inside this file:
	•	serving for every dataset you’ll serve (see §3 datasets list).
	•	slo: freshness_hours, min_rows, timeouts_ms, paging.max_limit.
	•	Confirm DSN env names match your Secrets.
	•	[ADD NEW FILE] analytics/configs/<model>.yaml (one per model you implement)
Purpose: per-job IO and bounds.
Contents template:
	•	inputs: array of sources with {store: postgres|clickhouse, dsn_env: CORE_*} and any table/filters.
	•	window: one of last_30d/90d/180d/365d or custom.
	•	output: {table: <dataset_name>, store: postgres|clickhouse, dsn_env: SIGNALS_*}
	•	model_version: semantic tag (e.g., clv-1.0.0).
	•	job_slo: {max_runtime_sec, max_retries, backoff_sec}.

⸻

1.3 analytics/jobs/ (batch compute)

Global job pattern (applies to all jobs below):
	•	[ADD NEW FILE] cmd/main.go
	•	Load configs/<model>.yaml via LoadJobConfig.
	•	Create context with job_slo.max_runtime_sec.
	•	Read inputs using common/datastore/* by store setting.
	•	Run pipeline/<model>_pipeline.go.
	•	Run pipeline/<model>_validate.go (required IDs, dedupe by (id,model_version,as_of), numeric bounds).
	•	Upsert outputs using Postgres helper (even if inputs from CH).
	•	Retry transient failures (max_retries, backoff_sec).
	•	Log: rows written, duration, window, model_version; exit non-zero on failure.
	•	[ADD NEW FILE] pipeline/<model>_pipeline.go
	•	Implement the model’s computation (pure code; no network except DB reads).
	•	Keep money in cents; keep probabilities [0,1].
	•	Return well-typed rows matching the schema.
	•	[ADD NEW FILE] pipeline/<model>_validate.go
	•	Enforce required fields, non-negative numbers, dedupe keys.
	•	Enforce sensible bounds (e.g., cover days ≥ 0).
	•	[ADD NEW FILE] Dockerfile (in each job folder)
	•	Build static Go binary; copy configs mount path; set non-root user.
	•	[ADD NEW FILE] deploy/k8s/jobs/<model>-cronjob.yaml (manifests folder; see §2.2)
	•	Reference image tag; schedule; env (DSNs); mount configs.
	•	Set activeDeadlineSeconds, backoffLimit, resource requests/limits.

Jobs to add (one folder each) aligned to your 15 models
(These are anticipated additions—they are part of the plan, not unexpected gaps.)

	•	[ADD NEW FOLDER] analytics/jobs/customer/clv_bgnbd/ — CLV scoring (BG/NBD + Gamma–Gamma) → outputs tiers_clv.
	•	[ADD NEW FOLDER] analytics/jobs/search/ranknet_train/ — learn rank weights from logs → outputs rank_weights.
	•	[ADD NEW FOLDER] analytics/jobs/campaign/promotion_uplift/ — incremental GM + assignment → outputs promotion_assignments.
	•	[ADD NEW FOLDER] analytics/jobs/cart/upsell_propensity/ — upsell decision scores → outputs upsell_scores.
	•	[ADD NEW FOLDER] analytics/jobs/pdp/cross_sell/ — complements lists → outputs cross_sell.
	•	[ADD NEW FOLDER] analytics/jobs/assortment/bundle_optimizer/ — bundle picks → outputs bundles.
	•	[ADD NEW FOLDER] analytics/jobs/customer/personas_hdbscan/ — persona labels → outputs personas.
	•	[ADD NEW FOLDER] analytics/jobs/pricing/markdown_optimizer/ — markdown ladder → outputs markdown_plans.
	•	[ADD NEW FOLDER] analytics/jobs/pricing/elasticity/ — price elasticity → outputs elasticity.
	•	[ADD NEW FOLDER] analytics/jobs/customer/churn_propensity/ — churn risk → outputs churn_risk.
	•	[ADD NEW FOLDER] analytics/jobs/customer/lifecycle_hsmm/ — lifecycle state → outputs lifecycle_state.
	•	[ADD NEW FOLDER] analytics/jobs/product/similarity_index/ — build/load ANN index artifacts (vectors or normalized co-buy); publish similarity_artifacts.
	•	(You already have) analytics/jobs/customer/rfm/, analytics/jobs/product/velocity/, analytics/jobs/product/abcxyz/, analytics/jobs/product/affinity/ — keep and align to the same pattern.

⸻

1.4 analytics/schemas/models/ (output contracts)

Add a schema JSON for every dataset you will output or serve:
	•	[ADD NEW FILE] tiers_clv.schema.json — { user_id, clv_180_cents, tier, model_version, as_of }.
	•	[ADD NEW FILE] rank_weights.schema.json — { feature, weight, intercept?, model_version, as_of }.
	•	[ADD NEW FILE] promotion_assignments.schema.json — { user_id, arm, delta_gm_cents, ec_cents, model_version, as_of }.
	•	[ADD NEW FILE] upsell_scores.schema.json — { hero_product_id, candidate_id, net_score_cents, model_version, as_of }.
	•	[ADD NEW FILE] cross_sell.schema.json — { hero_product_id, complements:[{id,score}], model_version, as_of }.
	•	[ADD NEW FILE] bundles.schema.json — { slow_id, hero_id, score_cents, constraints, model_version, as_of }.
	•	[ADD NEW FILE] markdown_plans.schema.json — { sku_id, weeks:[{price,margin,qty}], total_gm_cents, model_version, as_of }.
	•	[ADD NEW FILE] elasticity.schema.json — { sku_id, elasticity, conf_low?, conf_high?, model_version, as_of }.
	•	[ADD NEW FILE] personas.schema.json — { user_id, persona_id, stability, model_version, as_of }.
	•	[ADD NEW FILE] churn_risk.schema.json — { user_id, p_churn, decision_value_cents, model_version, as_of }.
	•	[ADD NEW FILE] lifecycle_state.schema.json — { user_id, state, time_in_state, state_probs?, model_version, as_of }.
	•	[ADD NEW FILE] similarity_artifacts.schema.json — { index_id, version, path, built_at } (metadata row if you track artifacts in DB).
	•	(Already exist and remain) customer_segments.schema.json, product_signals.schema.json, affinity_edges.schema.json.

⸻

1.5 analytics/signals/ (serving: gRPC + GraphQL)
	•	[EDIT EXISTING FILE] signals/cmd/main.go
	•	Load signals.yaml with LoadSignalsConfig.
	•	Set default per-request timeout from slo.timeouts_ms.request_default.
	•	Expose GET /healthz (process + store pings OK) and GET /readyz (boot checks + freshness within SLO).
	•	Log one boot line: dataset→store binding and freshness snapshot.
	•	[EDIT EXISTING FILE] signals/internal/store/factory.go
	•	On boot: verify each serving.<dataset> has a matching store impl (*_store_pg.go or *_store_ch.go).
	•	Ping DSNs with timeout; probe SelectFreshness and SelectCount per dataset; fail fast if any breach.
	•	Hold bindings for later use by services and ops resolver.
	•	[EDIT EXISTING FILE] signals/internal/services/*_signals_service.go
	•	Enforce paging from slo.paging.max_limit.
	•	Emit metrics: signals_dataset_ok{dataset}, signals_dataset_rows{dataset}, signals_dataset_max_age_hours{dataset}.
	•	[EDIT EXISTING FILE] signals/internal/validators/*_validator.go
	•	Required IDs present; numeric clamps; paging limits.
	•	[ADD NEW FILE] signals/internal/graph/schema/ops.graphqls
	•	type OpsDatasetStatus { name: String!, store: String!, ok: Boolean!, rows: Int!, maxAgeHours: Int!, lastAsOf: Time, message: String }
	•	type OpsStatus { datasets: [OpsDatasetStatus!]!, startedAt: Time!, version: String! }
	•	extend type Query { opsStatus: OpsStatus! }
	•	[ADD NEW FILE] signals/internal/graph/schema/ops.resolver.go
	•	Build status from factory bindings; run fast COUNT + MAX(as_of); compare to SLO; return statuses.

Serving new datasets (add these only for datasets you plan to query via signals):
	•	For each dataset below, add proto + service + validator + GraphQL schema/resolver if you need it served:
	•	tiers_clv → [ADD NEW FILE] signals/proto/tiers_clv.proto • [ADD NEW FILE] signals/internal/services/tiers_clv_service.go • [ADD NEW FILE] signals/internal/graph/schema/tiers_clv.graphqls • [ADD NEW FILE] tiers_clv.resolver.go • validator as needed.
	•	rank_weights → add proto/service if you want to fetch weights online.
	•	promotion_assignments, upsell_scores, cross_sell, bundles, markdown_plans, elasticity, personas, churn_risk, lifecycle_state, similarity_artifacts → same pattern as needed.

⸻

2) Containerization & Deployment

2.1 Dockerfiles
	•	[ADD NEW FILE] analytics/signals/Dockerfile — build service; expose GraphQL/gRPC ports; run as non-root.
	•	[ADD NEW FILE] analytics/jobs/<domain>/<model>/Dockerfile — build static binary; set entrypoint to cmd/main.

2.2 K8s Manifests
	•	[ADD NEW FILE] deploy/k8s/services/signals-deployment.yaml — Deployment with liveness/readiness, env DSNs, resource limits.
	•	[ADD NEW FILE] deploy/k8s/services/signals-service.yaml — Service/Ingress for /graphql and gRPC port.
	•	[ADD NEW FILE] deploy/k8s/secrets/core-dsns.yaml — K8s Secret holding CORE_PG_DSN, CORE_CH_DSN.
	•	[ADD NEW FILE] deploy/k8s/secrets/signals-dsns.yaml — Secret holding SIGNALS_PG_DSN, SIGNALS_CH_DSN.
	•	[ADD NEW FILE] deploy/k8s/jobs/<model>-cronjob.yaml — one per job: image, schedule, env, config mount, deadlines, retries.

2.3 CI/CD & Tooling
	•	[ADD NEW FILE] .github/workflows/ci.yml — go build/test; docker build; push to registry (tag = model_version).
	•	[ADD NEW FILE] .github/workflows/deploy.yml — kubectl/Helm/Terraform apply for manifests.
	•	[ADD NEW FILE] Makefile — local build, docker, push, deploy.
	•	[ADD NEW FILE] docs/registry-policy.md — image repos, tag format, promotion rules.
	•	[ADD NEW FILE] env/.env.example — DSN var names expected by jobs/services.

2.4 Observability
	•	[ADD NEW FILE] deploy/observability/prometheus-scrape.yaml — scrape jobs and signals.
	•	[ADD NEW FILE] dashboards/analytics-ops.json — Grafana board (dataset_ok, rows, maxAgeHours, job rows, runtime).
	•	[EDIT EXISTING FILE] jobs & signals code — instrument counters/gauges per safety.yaml (e.g., job_rows_written{job}, job_runtime_seconds{job}).

⸻

3) Datasets you will output/serve (map from models → tables)
	•	#1 CLV → tiers_clv (Postgres): user_id, clv_180_cents, tier, model_version, as_of.
	•	#2/#8 Ranking → rank_weights (Postgres): learned feature weights; used by serving or batch scoring.
	•	#3 Cart Re-rank → upsell_scores (Postgres).
	•	#4 Promo Uplift → promotion_assignments (Postgres).
	•	#5 Personas → personas (Postgres).
	•	#6 Affinities → enrich existing; may also write product_signals.
	•	#7 Similarity → similarity_artifacts (meta); neighbors re-served from ANN engine or from DB.
	•	#9 Markdown → markdown_plans (Postgres).
	•	#10 Elasticity → elasticity (Postgres).
	•	#11 Upsell → upsell_scores (already listed).
	•	#12 Cross-sell → cross_sell (Postgres).
	•	#13 Bundles → bundles (Postgres).
	•	#14 Churn → churn_risk (Postgres).
	•	#15 Lifecycle → lifecycle_state (Postgres).

⸻

4) Periodic Parameter Adjustment (where “training/fit” jobs go)

(No LLMs. These are plain batch jobs that compute parameters from logs.)
	•	RankNet weights (#8):
	•	[ADD NEW FOLDER] analytics/jobs/search/ranknet_train/ (see §1.3 pattern).
	•	[ADD NEW FILE] analytics/configs/ranknet_train.yaml — inputs=search logs; output=rank_weights.
	•	[ADD NEW FILE] deploy/k8s/jobs/ranknet-train-cronjob.yaml — nightly or weekly.
	•	CLV hyperparams (#1):
	•	Either embed fitting into clv_bgnbd job once per window, or
	•	[ADD NEW FOLDER] analytics/jobs/customer/clv_fit/ — fit r, α, a, b, p, q, γ; output small clv_params table; scoring job reads them.
	•	Add YAML + CronJob as above.
	•	Promotion uplift model (#4), churn model (#14), upsell model (#11):
	•	If you use learned models (GBM/logit), create corresponding *_train jobs to produce coefficients; otherwise keep rule-based and skip.
	•	Similarity index (#7):
	•	[ADD NEW FOLDER] analytics/jobs/product/similarity_index/ — builds ANN index files and loads into the ANN engine; records metadata row in similarity_artifacts.

⸻

5) Security / Access (brief)
	•	[PROVISION SYSTEM] Secrets store DSNs;
	•	[ADD NEW FILE] deploy/k8s/rbac/serviceaccounts.yaml (if needed) — least privilege;
	•	DB roles: read-only for signals; read+write for jobs.

⸻

6) Runbooks (brief)
	•	Boot fails: read signals logs; fix signals.yaml mapping or DSNs; re-deploy.
	•	Stale dataset: check opsStatus; re-run the job; confirm rows and as_of.
	•	Backfill: run CronJob with override window and model_version bump.

⸻

Final Notes
	•	All “training/fit” mentions are plain batch code deployed like any other job (CronJob). No LLMs.
	•	Whenever I write [EDIT EXISTING FILE], it means put the new logic inside that file (not a new file).
	•	Whenever I write [ADD NEW FILE]/[ADD NEW FOLDER], it means create it at that exact path.








Minimal file set per model (place these)
	•	configs/<model>.yaml — per-job IO, window, model_version, job_slo.
	•	schemas/models/<dataset>.schema.json — output row contract (includes id, model_version, as_of).
	•	jobs/<domain>/<model>/
	•	cmd/main.go — load config → run pipeline → validate → upsert.
	•	pipeline/<model>_pipeline.go — core computation.
	•	pipeline/<model>_validate.go — required keys, dedupe (id,model_version,as_of).
	•	Dockerfile
	•	deploy/k8s/jobs/<model>-cronjob.yaml — schedule, image, env (DSNs).
	•	(Serve it via signals?) add:
	•	signals/proto/<dataset>.proto
	•	signals/internal/services/<dataset>_service.go
	•	signals/internal/graph/schema/<dataset>.graphqls
	•	signals/internal/graph/schema/<dataset>.resolver.go
	•	Edit configs/signals.yaml to bind <dataset> → store/DSN.

⸻

Example A — #1 CLV (BG/NBD + Gamma–Gamma)

Dataset: tiers_clv
	•	configs/clv_bgnbd.yaml
	•	schemas/models/tiers_clv.schema.json
	•	jobs/customer/clv_bgnbd/
	•	cmd/main.go
	•	pipeline/clv_bgnbd_pipeline.go
	•	pipeline/clv_bgnbd_validate.go
	•	Dockerfile
	•	deploy/k8s/jobs/clv-bgnbd-cronjob.yaml
	•	(Serve via signals)
	•	signals/proto/tiers_clv.proto
	•	signals/internal/services/tiers_clv_service.go
	•	signals/internal/graph/schema/tiers_clv.graphqls
	•	signals/internal/graph/schema/tiers_clv.resolver.go
	•	Edit configs/signals.yaml → add serving.tiers_clv.

⸻

Example B — #12 Cross-sell (complements for hero item)

Dataset: cross_sell
	•	configs/cross_sell.yaml
	•	schemas/models/cross_sell.schema.json
	•	jobs/pdp/cross_sell/
	•	cmd/main.go
	•	pipeline/cross_sell_pipeline.go
	•	pipeline/cross_sell_validate.go
	•	Dockerfile
	•	deploy/k8s/jobs/cross-sell-cronjob.yaml
	•	(Serve via signals)
	•	signals/proto/cross_sell.proto
	•	signals/internal/services/cross_sell_service.go
	•	signals/internal/graph/schema/cross_sell.graphqls
	•	signals/internal/graph/schema/cross_sell.resolver.go
	•	Edit configs/signals.yaml → add serving.cross_sell.

⸻

Notes (same for any two you pick)
	•	If a model also needs periodic parameter fitting (e.g., RankNet weights), add a sibling jobs/<domain>/<model>_train/ folder + its config and CronJob; the serving/scoring job reads those params.
	•	Don’t forget DSN secrets in K8s and image tags in your CronJobs.





Yes—I’ll map exactly where each piece lives in your layout, using your patterns. I’ll show the math file, the storage file, and the parameter-update file for two concrete models (CLV and Cross-sell), then a 3-step pattern you can reuse for any of the 15.

⸻

A) Example: #1 CLV (BG/NBD + Gamma–Gamma)

Math (from your PDFs) — where it goes
	•	jobs/customer/clv_bgnbd/pipeline/clv_bgnbd_pipeline.go
	•	Implements BG/NBD expected transactions and Gamma–Gamma monetary; multiplies to CLV for horizon H.
	•	Reads inputs via common/datastore/* per configs/clv_bgnbd.yaml.

Orchestrate + validate + write
	•	jobs/customer/clv_bgnbd/cmd/main.go
	•	Loads configs/clv_bgnbd.yaml → runs pipeline → calls validate → upserts rows.
	•	jobs/customer/clv_bgnbd/pipeline/clv_bgnbd_validate.go
	•	Ensures {user_id, clv_180_cents, tier, model_version, as_of} present; dedup on (user_id, model_version, as_of).
	•	schemas/models/tiers_clv.schema.json
	•	Contract for the output row (CLV + tier).
	•	Writes via common/datastore/postgres.go::UpsertRows(...) into tiers_clv.

Serving (no math here)
	•	signals/internal/services/tiers_clv_service.go + GraphQL/proto (if you expose CLV)
	•	Reads tiers_clv and returns rows; request validation in signals/internal/validators/*.

Parameters & periodic adjustment
	•	If fixed hyperparameters (r, α, a, b, p, q, γ): store them in configs/clv_bgnbd.yaml; bump model_version when changed.
	•	If fitted:
	•	Training/fit job: jobs/customer/clv_fit/
	•	pipeline/clv_fit_pipeline.go: MLE fit of BG/NBD + GG params from the last W days.
	•	Output table: clv_params with schemas/models/clv_params.schema.json.
	•	Config: configs/clv_fit.yaml; Cron: deploy/k8s/jobs/clv-fit-cronjob.yaml.
	•	Scoring job (clv_bgnbd) reads the latest clv_params before computing CLV.
	•	You tweak cadence (weekly/monthly) by editing the CronJob; you can also adjust tier thresholds in clv_bgnbd.yaml.

⸻

B) Example: #12 Cross-sell (complements for hero item)

Math (from your PDFs) — where it goes
	•	jobs/pdp/cross_sell/pipeline/cross_sell_pipeline.go
	•	Computes latent score z from features (cosine, lift, substitute flag), converts to attach probability, applies low-stock penalty, outputs Net; selects top-N per hero.

Orchestrate + validate + write
	•	jobs/pdp/cross_sell/cmd/main.go → load configs/cross_sell.yaml → run pipeline → validate → upsert.
	•	jobs/pdp/cross_sell/pipeline/cross_sell_validate.go → required keys {hero_id, complement_id, net_cents, model_version, as_of}; dedupe.
	•	schemas/models/cross_sell.schema.json → row contract.
	•	Writes via common/datastore/postgres.go::UpsertRows(...) into cross_sell.

Serving (no math here)
	•	signals/internal/services/cross_sell_service.go (+ GraphQL/proto if exposed) → reads cross_sell.

Parameters & periodic adjustment
	•	If rule-based: keep coefficients (e.g., beta0, beta1, beta2, beta3, alpha, m0, N) in configs/cross_sell.yaml; bump model_version when edited.
	•	If data-fit:
	•	Training job: jobs/pdp/cross_sell_train/
	•	pipeline/cross_sell_train_pipeline.go: fit coefficients from recent attach outcomes; write cross_sell_params (with schemas/models/cross_sell_params.schema.json).
	•	Config: configs/cross_sell_train.yaml; Cron: deploy/k8s/jobs/cross-sell-train-cronjob.yaml.
	•	Scoring job (cross_sell) loads latest cross_sell_params at start and scores with them.

⸻

Reusable pattern (for any model)
	1.	Math/Scoring code (PDF → code)
	•	jobs/<domain>/<model>/pipeline/<model>_pipeline.go — pure compute.
	•	jobs/<domain>/<model>/pipeline/<model>_validate.go — row checks, dedupe, bounds.
	•	jobs/<domain>/<model>/cmd/main.go — load config → pipeline → validate → upsert.
	•	Output schema in schemas/models/<dataset>.schema.json; rows written via common/datastore/postgres.go.
	2.	Serving (if needed)
	•	signals/internal/services/<dataset>_service.go (+ proto + GraphQL schema/resolver).
	•	No math; just read rows, validate request, return.
	3.	Parameter updates (if you adjust from results)
	•	Config-only tweaks: edit configs/<model>.yaml (knobs), bump model_version, rerun job.
	•	Data-driven fit: add a sibling train job jobs/<domain>/<model>_train/ that computes new params from logs, writes a <model>_params table (with schema). Your scoring job reads those params at runtime.
	•	Wire Cron in deploy/k8s/jobs/<model>-train-cronjob.yaml to control cadence.

This keeps your anemic models clean (structs only), math isolated to pipeline files, persistence centralized via datastore helpers, serving immutable in signals, and parameter adjustment either config-based or via dedicated train jobs.



Short answer: Yes—your layout supports automatic, per-tenant parameter updates.
It becomes automatic once you add the “train” jobs + Cron and read the latest params per tenant in scoring.

How to wire it (per model)
	1.	Train job (per-tenant fit)
	•	Job: jobs/<domain>/<model>_train/…
	•	Input: tenant-scoped data (use store_id/merchant_id in queries).
	•	Output table: <model>_params with columns like
store_id, model_version, as_of, param_blob/json (or columns), quality_metrics.
	•	Schedule: Cron in deploy/k8s/jobs/<model>-train-cronjob.yaml (e.g., nightly).
	2.	Scoring job (uses latest params)
	•	Job: jobs/<domain>/<model>/…
	•	At start: read latest params for the tenant
WHERE store_id = :store AND as_of = (SELECT MAX(as_of)… ).
	•	Compute: generate outputs (e.g., CLV, cross-sell) using those params.
	•	Write: tenant-scoped rows with model_version, as_of.
	3.	Serving (signals)
	•	Resolvers/services pass the tenant context (store_id) → read outputs (and, if needed, show param version/freshness).
	•	No heavy math; just fetch the tenant’s latest rows.

Multi-tenancy details (what makes it “automatic” per tenant)
	•	Partition everything by store_id (and merchant_id if used).
	•	Train job loops all tenants (one process, many partitions) or sharded per tenant—either is fine.
	•	Freshness SLOs in configs/signals.yaml catch stale tenants; Ops query shows red if a tenant’s params/outputs lag.
	•	Fail-safe: if a tenant’s train fails, scoring uses the last good params; signals keeps serving last good outputs.

Cadence & triggers
	•	Default: time-based Cron (nightly/weekly) → automatic updates.
	•	Optional: add “retrain if drift” logic in the train job (e.g., fit only if error/volume changed past a threshold).
	•	Manual override: bump model_version in config to roll a new param set; jobs write alongside and switch reads by version.

Where each piece lives (files)
	•	Training math: jobs/<domain>/<model>_train/pipeline/<model>_train_pipeline.go
	•	Scoring math: jobs/<domain>/<model>/pipeline/<model>_pipeline.go
	•	Param table schema: schemas/models/<model>_params.schema.json
	•	Cron: deploy/k8s/jobs/<model>-train-cronjob.yaml
	•	Tenant scoping: in datastore helpers/queries and services, using your existing context keys

Bottom line: once you add the train jobs and point scoring to “latest params per store_id,” the system continuously adapts per tenant with no manual steps, and your serving stays fast and stable.




Yes. Those are exactly the cases where you’d expose data via GraphQL so the UI can fetch per-request and update the page in real time.

Models that typically need online fetching (GraphQL/gRPC)
	•	#2 Recommender (PDP/PLP) – fetch ranked items for the current user/page.
	•	#3 Basket / Session Re-rank – fetch add-ons for the current cart.
	•	#7 Similarity / Neighbors – “you may also like” on PDP/PLP.
	•	#8 PLP/Search Ranking – results listing; often service-side but UI still queries the ranked list.
	•	#11 Upsell Propensity – show/decline upsell in checkout.
	•	#12 Cross-sell – complements for the current hero item.

(All served by signals with thin services + GraphQL queries; tiny per-request math allowed.)

Models that are usually offline (no GraphQL unless you want dashboards)
	•	#1 CLV, #5 Personas, #6 Affinities (can be served if you drive personalization from UI),
	•	#4 Promotion Uplift (assignments read server-side),
	•	#9 Markdown, #10 Elasticity, #13 Bundles,
	•	#14 Churn, #15 Lifecycle (used for triggers/targeting).

Rule of thumb: if the page needs it now to render or react to user behavior, expose it via GraphQL; otherwise keep it batch-only and consume server-side.






Both—driven by context + precomputed data. Here’s how the system “knows” what to do:

How it decides at request time
	•	Context keys (from middleware): storeID, merchantID, userID, sessionID/cartID, page type (PDP/PLP/Cart/Checkout).
	•	Resolver selects dataset(s):
	•	PDP: similarity + cross_sell (and filter by stock/price).
	•	PLP/Search: ranked list (rank_weights + product signals).
	•	Cart/Checkout: basket re-rank or upsell_scores with guardrails.
	•	Scope: queries are per-tenant (store) and, when relevant, per-customer/session.

What’s per-customer vs aggregated
	•	Per-customer (user-level): CLV, affinities, churn_risk, lifecycle_state.
	•	Per-session (cart/page): basket re-rank, upsell decision, cross-sell for current hero.
	•	Per-product / pair / global: product_signals, affinity_edges, rank_weights, markdown_plans, elasticity, bundles.

Where the numbers come from
	•	Jobs compute and refresh:
	•	Per-tenant parameters (weights, priors) via train jobs on a schedule.
	•	Per-customer / per-product outputs via scoring jobs (partitioned by storeID, and userID when applicable).
	•	Services combine: lightweight, real-time filters (stock, price, OOS caps) on top of the precomputed rows.

Fallback logic
	•	Try per-customer/session first → if sparse, use segment/cohort → else use tenant-level defaults.
	•	Guardrails (e.g., don’t harm base conversion) are checked per request.

Net: models adapt per tenant continuously (scheduled jobs) and act per customer/session at runtime (GraphQL calls), using the right dataset for the page and the current context.


Short answer: they’re not all bare-bones—several already depend on per-customer/session interaction signals. But yes, some flows/feature pipelines you’re imagining (e.g., “if they clicked X or filled Y, infer Z persona”) are not yet defined and would need one small aggregation job added.

What you have now (by model)

Mostly “formula/optimization” (math + standard inputs)
	•	#1 CLV, #9 Markdown, #10 Elasticity, #13 Bundle
These run with transactional data (orders, prices, stock) and store-level params. No special nav tracking required.

Already interaction-driven (per customer/session/page)
	•	#2 Recommender, #3 Basket re-rank, #6 Affinities, #7 Similarity, #8 PLP RankNet, #11 Upsell, #12 Cross-sell, #14 Churn, #15 Lifecycle
These expect logs like views/clicks/searches/ATC/purchases (by user/session, with timestamps). Affinities is explicitly “navigational” via time-decayed events; RankNet uses search logs; Basket/Upsell/Cross-sell use current cart/page context.

“Persona” can use nav patterns—if you feed them
	•	#5 Personas (HDBSCAN): your spec lists R/F/M + engagement + category shares; you can (and should) add navigation features (e.g., brand/category view time, click depth, form submits) to make personas reflect browsing intent—this is just features, not a new model.

What’s missing if you want rules like “clicked X → persona Y”

You need a tiny feature aggregation job that turns raw events into per-customer signals the models can consume.
	•	Add once (if you want richer nav signals):
	•	configs/customer_signals.yaml (inputs: events; output: customer_signals)
	•	jobs/customer/signals/{cmd/main.go, pipeline/customer_signals_pipeline.go, customer_signals_validate.go}
	•	schemas/models/customer_signals.schema.json (e.g., views_by_brand, views_by_category, click_depth, form_flags, recency)
	•	Optionally expose via signals (customer_signals GraphQL), or just read it inside #5 Personas, #6 Affinities, #14 Churn, #15 Lifecycle.

With that in place, your “if they clicked this object or filled that form” logic becomes features feeding Personas/Affinities/Churn/Lifecycle. You can also add simple rules in the Basket/Upsell/Cross-sell services (lightweight, per-request) that check these flags.

How decisions run (tenant + user)
	•	Per-tenant: train/fitting jobs write <model>_params by store_id; scoring jobs read the latest per tenant—updates happen automatically by Cron.
	•	Per-customer/session: scoring jobs write per-user outputs (CLV, churn, affinities) and per-session candidates (basket); GraphQL fetches the right rows at request time using your context (store/user/session).

Bottom line
	•	No, they’re not just bare equations; many already use navigation-style logs.
	•	To get the specific “clicked X/filled Y → persona Z” behavior, add one small signals job to aggregate those event flags/metrics and feed them into #5/#6/#14/#15 (and light rules in #3/#11/#12 if you want UI-time tweaks).
	•	The current layout is suitable and expects per-tenant, continuously updated parameters and per-customer decisions at runtime.