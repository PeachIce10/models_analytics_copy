Below is a compact **5‑page implementation outline** you can archive and hand off. It’s layout‑first, action‑oriented, and tuned to your stack. It covers: directory roles, what logic lives where, when to expose GraphQL, how Python/C++/Rust fit in, and a concrete **package/install checklist** by capability so a new thread can start wiring models #1–#15 immediately.

---

## 1) Analytics layout — roles, contracts, and data flow

**Runtime shape (batch → store → serve):**

1. **Jobs (batch compute, K8s CronJobs)**

   * Read inputs (Postgres/ClickHouse), run math in `pipeline/*.go`, validate, upsert **dataset rows** (Postgres preferred for serving, CH for edges/scans).
2. **Signals (serving, Deployment/Service)**

   * Thin, read‑only GraphQL/gRPC façades over precomputed datasets. No heavy math. Validates requests, applies paging, returns rows.
3. **Frontend / other services**

   * Next.js/Apollo → `/graphql` (signals). Internal services may use gRPC. Tenancy via `store_id` in context.

**Key directories (under `analytics/`):**

* `common/` — shared helpers used by jobs

  * `config/load.go` → strict YAML loaders (`signals.yaml`, `<model>.yaml`) with env overrides and required keys (SLOs, DSNs).
  * `datastore/postgres.go` / `datastore/clickhouse.go` → **context‑timed** readers/writers; helpers: `SelectFreshness`, `SelectCount`, `UpsertRows(keys=[id,model_version,as_of])`.
  * `timewin/window.go` → rolling windows (`last_30d/90d/180d/365d`, `Custom(start,end)`).

* `configs/` — runtime config (no secrets inside)

  * `signals.yaml` → dataset→backend bindings + **SLOs**: `freshness_hours`, `min_rows`, request timeouts, paging caps.
  * `<model>.yaml` (one per job) → `inputs[] {store, dsn_env}`, `window`, `output {table, store, dsn_env}`, `model_version`, `job_slo {max_runtime_sec, max_retries, backoff_sec}`.

* `jobs/<domain>/<model>/` — **one binary per model**

  * `cmd/main.go` → load config → run pipeline → validate → upsert → log rows/runtime.
  * `pipeline/<model>_pipeline.go` → **all math here** (pure compute + DB reads).
  * `pipeline/<model>_validate.go` → required keys, dedupe `(id,model_version,as_of)`, range checks.
  * `Dockerfile` → static Go build; non‑root user.

* `schemas/models/` — **output contracts** (JSON docs)

  * One file per dataset (e.g., `tiers_clv.schema.json`); columns always include `{id, model_version, as_of, store_id}`.

* `signals/` — **serving only** (GraphQL & gRPC)

  * `cmd/main.go` → boot: load `signals.yaml`, set default request timeout, expose `/healthz` & `/readyz` (SLO checks).
  * `internal/store/*` → backends (`*_store_pg.go`, `*_store_ch.go`), **factory boot checks** (DSN ping, freshness, row counts).
  * `internal/services/*` → thin dataset services (read rows, light filters/paging).
  * `internal/validators/*` → request guards (required IDs, paging clamp, numeric bounds).
  * `internal/graph/schema/*.graphqls` + `*.resolver.go` → GraphQL types/queries/resolvers.
  * `proto/*.proto` → optional gRPC contracts mirroring GraphQL.

**Multi‑tenancy & invariants (apply everywhere):**

* **Partition key:** `store_id` (always present; sometimes `merchant_id` too).
* **Row keys:** include entity id (e.g., `user_id`), `model_version`, `as_of`, `store_id`.
* **Units:** money in **cents**; probs ∈ [0,1]; ISO‑8601 timestamps.
* **SLOs:** Postgres datasets ≤ **36h** fresh; ClickHouse edges ≤ **72h**; signals enforces via `/readyz` & `opsStatus` GraphQL.

**Where the math lives (hard rule):**

* **Heavy compute/training/joins → Jobs** (`pipeline/*.go`).
* **Serving → Signals** (read‑only; at most micro‑scoring like a dot‑product with precomputed weights).

---

## 2) Authoring a new model — minimal file set & per‑model mapping

**Minimal file set (repeat for each model):**

* `configs/<model>.yaml` — IO, window, `model_version`, job SLO.
* `schemas/models/<dataset>.schema.json` — row contract.
* `jobs/<domain>/<model>/cmd/main.go` — orchestration.
* `jobs/<domain>/<model>/pipeline/<model>_pipeline.go` — math.
* `jobs/<domain>/<model>/pipeline/<model>_validate.go` — row checks.
* `jobs/<domain>/<model>/Dockerfile` — static Go build.
* `deploy/k8s/jobs/<model>-cronjob.yaml` — schedule & env.

**If served online add:**

* `signals/proto/<dataset>.proto` (optional gRPC)
* `signals/internal/services/<dataset>_service.go`
* `signals/internal/graph/schema/<dataset>.graphqls` + resolver
* Bind in `configs/signals.yaml: serving.<dataset>`

**Per‑model datasets you’ll create/serve (tie to your Signals→Models map):**

* **#1 CLV** → `tiers_clv (PG)` — `{user_id, clv_180_cents, tier, model_version, as_of, store_id}`
* **#2/#8 Rank** → `rank_weights (PG)` — learned weights (if you serve them)
* **#3 Cart** → `upsell_scores (PG)`
* **#4 Uplift** → `promotion_assignments (PG)`
* **#5 Personas** → `personas (PG)`
* **#6 Affinities** → enrich `product_signals` and/or own table
* **#7 Similarity** → `similarity_artifacts (PG meta)` + ANN engine store
* **#9 Markdown** → `markdown_plans (PG)`
* **#10 Elasticity** → `elasticity (PG)`
* **#11 Upsell** → `upsell_scores (PG)`
* **#12 Cross‑sell** → `cross_sell (PG)`
* **#13 Bundles** → `bundles (PG)`
* **#14 Churn** → `churn_risk (PG)`
* **#15 Lifecycle** → `lifecycle_state (PG)`

**Parameter tables (weekly “fit” jobs where needed):**

* `clv_params (r,alpha,a,b,p,q,gamma)` — #1
* `rank_weights` — #8 (#2 ranker may reuse)
* `cross_sell_params`, `upsell_params`, `churn_params`, `hsmm_params` — #12/#11/#14/#15
* Policy knobs if tuned: `markdown_params`, `bundle_params`, `elasticity_params`, `affinities_params`.

**Two foundation aggregation jobs (add first):**

* `visitor_cohorts` (daily; PG) → runtime fallback for anon sessions.
* `customer_signals` (daily; PG) → per‑user aggregates for #5/#6/#14/#15 (+ light rules in #3/#11/#12).

**Serving decision (When to add GraphQL/gRPC):**

* **Online (serve):** #2, #3, #7, #8, #11, #12 (page needs it now → PDP/PLP/Cart/Checkout).
* **Offline (batch only):** #1, #4, #5, #6, #9, #10, #13, #14, #15 (unless you want dashboards/UI hooks).

---

## 3) GraphQL & gRPC usage — when, how, and request contract

**When to expose GraphQL for a dataset:**

* Page/UI must fetch the decision at request time (e.g., PDP complements, PLP ranking, checkout upsell).
* You want **tenant‑scoped** filtering, **paging**, and **freshness SLOs** enforced.
* Internal tools (Ops, Admin) need health & counts (`opsStatus`).

**Typical GraphQL surfaces (signals):**

* `Query.crossSell(heroId:, limit:)` → `cross_sell` rows (filters: stock, refund risk).
* `Query.upsellScores(cartId:, limit:)` → `upsell_scores`.
* `Query.similar(productId:, k:)` → neighbors via ANN service (metadata in `similarity_artifacts`).
* `Query.rankedProducts(query:, page:, limit:)` → uses `rank_weights` + product features.
* `Query.customerSignals(userId:)`, `Query.visitorCohort(cohortId:)` → optional.
* `Query.opsStatus` → dataset freshness & row counts (green/yellow/red).

**Resolver & service pattern (thin):**

* Validate inputs (`validators/*`), enforce paging caps from `signals.yaml`.
* Read rows via store layer (`store/postgres/*` or `store/clickhouse/*`).
* Apply **only light** filters in service (e.g., price band, low‑stock guard).
* Return typed GraphQL objects; keep IDs/model_version/as_of in responses for audit.

**gRPC (optional):**

* Mirror GraphQL shapes for internal micro‑services; use protobuf for compact payloads and HTTP/2 streaming if bulk.

---

## 4) Language choices & native acceleration — Go by default, Python/C++/Rust when it clearly wins

**Default:** **Go** for jobs & serving. Cheap, fast, easy ops.
**Add Python/C++/Rust** only where a library or kernel gives a **decisive** advantage (profiling‑proven hot spot or niche algo).

### 4.1 In‑process native kernels (no sidecar)

* Layout:

  * `analytics/native/cpp/<lib>/…` or `analytics/native/rust/<lib>/…`
  * `analytics/native/<lib>/wrapper.go` (cgo) exports a clean Go API.
  * Call from `jobs/.../pipeline/*.go`.

* Build notes:

  * `CGO_ENABLED=1`; compile the static lib in Docker build stage; link in Go build; run as non‑root.
  * Memory/threads: allocate/free on one side; functions are **re‑entrant**; return errors as codes.

* When this is worth it:

  * DP/MILP kernels (#4/#9), large ANN build (#7/#2/#12), heavy numeric loops.
  * cgo overhead is negligible when kernels run for **ms+** or process big batches.

### 4.2 Sidecar kernel (gRPC in same Pod)

* Wrap a C++/Rust/GPU routine in a tiny service and call over loopback.
* Easier crash‑isolation and versioning; IPC cost dwarfed by heavy compute time.

### 4.3 Python jobs (containerized)

* Use **only** where library breadth is decisive:

  * **HDBSCAN/personas (#5)**, **Bayesian fits**, **hsmm** variants (#15) if not implemented in Go.
  * Emit rows or parameter tables just like Go jobs; keep serving in Go/signals.

**Decision rule:** Profile → if hotspot remains after Go/CH tuning, isolate just that loop: cgo in batch, or sidecar if online; otherwise stay pure Go.

---

## 5) Package & tool checklist — what to install to cover all 15 models

> Use this as your “install plan.” It’s grouped by capability so you can script `go get`, base Docker images, and optional native toolchains.

### 5.1 Core Go deps (batch & serve)

* **Config/YAML:** `gopkg.in/yaml.v3`
* **DBs:**

  * Postgres: `github.com/jackc/pgx/v5`
  * ClickHouse: **pick one** → `github.com/ClickHouse/ch-go` (fast) or `github.com/ClickHouse/clickhouse-go/v2`
* **GraphQL:** `github.com/99designs/gqlgen`
* **gRPC/Protobuf:** `google.golang.org/grpc`, `google.golang.org/protobuf`
* **Observability:** `github.com/prometheus/client_golang` (+ your logger; e.g., `go.uber.org/zap`)
* **Concurrency utils:** `golang.org/x/sync/errgroup`
* **Math/Stats:** `gonum.org/v1/gonum` (special functions, distributions for #1, #10)
* **HTTP/gzip (if streaming):** stdlib `net/http`, `compress/gzip` (optional)

### 5.2 Model‑specific libs / engines (pick per your path)

* **#7/#2/#12 ANN neighbors (serving ≤50ms):**

  * **OpenSearch k‑NN** (plugin) + Go client: `github.com/opensearch-project/opensearch-go`
  * or **Milvus** + Go SDK: `github.com/milvus-io/milvus-sdk-go`
  * or **FAISS** via C++ + cgo (in‑process for batch neighbor build; sidecar for serve).

* **#9 Markdown / #4 Uplift (knapsack, MILP/LP):**

  * **Google OR‑Tools** (C++ core) + cgo wrapper **or** call Python job with `ortools`
  * Alternative OSS: **CBC/GLPK** (C libs) + Go binding (`go‑glpk`, community)
  * Keep objective/constraints in jobs; outputs in `markdown_plans` / `promotion_assignments`.

* **#10 Elasticity (regression/log‑log + controls):**

  * Go path: build with `gonum/stat` + your design matrix; optional robust SEs.
  * Python alternative: `statsmodels` (if you choose a Python fit job).

* **#5 Personas (HDBSCAN):**

  * Python: `hdbscan`, `scikit‑learn` (emit `personas` table).
  * Rust: crates exist; still easier with Python for now.
  * Go fallback: K‑Means only (already doable with `gonum`).

* **#15 Lifecycle (HSMM + optional Hawkes):**

  * Go: custom HSMM (Poisson emissions) is feasible; Hawkes adds math (can code with `gonum`).
  * Python option: `pomegranate` (HMM/HSMM), or your own via `numpy`.

* **#8 PLP Rank (LTR):**

  * Pairwise ranker in Go (logistic loss) is straightforward; or run a Python training job (XGBoost ranker) → write `rank_weights`.
  * Serving stays in Go with dot‑product.

* **#6 Affinities (decayed counts):**

  * Go + ClickHouse arrays and time‑decay → fast; no extra lib beyond `ch-go`.

### 5.3 Build/CI toolchain & native (optional)

* **Protobuf toolchain:** `protoc`, `protoc‑gen‑go`, `protoc‑gen‑go‑grpc`
* **gqlgen codegen:** `go run github.com/99designs/gqlgen generate`
* **Native build (if cgo):** `build‑essential`/`g++` (for C++), `rustup/cargo` (for Rust) in the **builder** stage
* **Docker base:** `golang:1.xx` (builder) → `distroless` or slim Alpine (final)
* **K8s manifests:** CronJobs per job; Deployment/Service for signals; Secrets for DSNs.

---

## 6) Ops: SLOs, secrets, health, and runbooks (what to wire before coding math)

**Provision (once):**
`[PROVISION SYSTEM]` Container Registry • Kubernetes • Secrets Manager/K8s Secrets (`CORE_PG_DSN`, `CORE_CH_DSN`, `SIGNALS_PG_DSN`, `SIGNALS_CH_DSN`) • Postgres • ClickHouse • Observability (Prometheus/Grafana) • ANN engine (OpenSearch/Milvus/FAISS) • Optimizer (OR‑Tools/CBC/GLPK) • CI/CD runners.

**Signals boot checks (`store/factory.go`):**

* Parse `signals.yaml` → each `serving.<dataset>` has a matching store impl.
* Ping DSNs with timeout; `SELECT MAX(as_of)` + `COUNT(*)` per dataset; compare to SLOs.
* Fail fast on any breach (process doesn’t serve).

**Health & Ops GraphQL:**

* `/healthz` (process + pings OK) • `/readyz` (plus SLOs within bounds)
* `Query.opsStatus` → `{ name, store, ok, rows, maxAgeHours, lastAsOf, message }`

**Metrics to emit (jobs & signals):**

* `signals_dataset_ok{dataset}` • `signals_dataset_rows{dataset}` • `signals_dataset_max_age_hours{dataset}`
* `job_rows_written{job}` • `job_runtime_seconds{job}`

**Runbooks (summarized):**

* **Boot fails:** check `signals.yaml` mapping and DSN envs; logs tell which dataset/store failed.
* **Stale dataset:** run the job manually, confirm rows/as_of, recheck `opsStatus`.
* **Backfill:** run CronJob with window override & `model_version` bump; rows dedupe on `(id,model_version,as_of)`.

**Security:**

* Secrets in K8s Secrets/Secrets Manager (not in repo).
* Dataset access via signals may add **Casbin** RBAC/ABAC (tenant/dataset/role) in services/resolvers.
* JWT/auth handled by your main middleware; signals reads `store_id` from context consistently.

---

## 7) Two worked patterns you’ll reuse

**A) CLV (#1) — BG/NBD + Gamma‑Gamma**

* **Job:** `jobs/customer/clv_bgnbd/…` → reads orders, computes expected transactions (H) × expected margin → writes `tiers_clv`.
* **(Optional) Params fit:** `jobs/customer/clv_fit/…` → writes `clv_params`; scoring job reads latest per `store_id`.
* **Serve?** Usually **no** (batch), but add GraphQL if UI needs tiers.

**B) Cross‑sell (#12)**

* **Job:** `jobs/pdp/cross_sell/…` → latent score from embeddings + lift; guardrails (margin, stock, substitutes) → writes `cross_sell`.
* **Serve:** **Yes** (PDP/cart). GraphQL query returns complements for a hero item; service filters by current stock/price.

---

## 8) “When do I use GraphQL vs keep offline?” (one‑glance guide)

* **Use GraphQL (signals)** if: the page **must** render with this data now (PDP/PLP/Cart/Checkout), or an admin screen needs tenant‑filtered reads with freshness.
* **Keep offline** if: the dataset only feeds other jobs or server‑side orchestration (email audience selection, policy tuning), or is purely for BI.

**Online models:** #2, #3, #7, #8, #11, #12
**Offline models:** #1, #4, #5, #6, #9, #10, #13, #14, #15 (dashboards optional)

---

## 9) Handoff: first steps the “package/install” thread should execute

**Step 0 — Baseline CI/build toolchain**

* Install `protoc`, `protoc-gen-go`, `protoc-gen-go-grpc`, `gqlgen`.
* Prepare builder Docker with `g++` (and `rustup` only if you plan native kernels).

**Step 1 — Core Go modules (one time)**

```bash
go get gopkg.in/yaml.v3 \
  github.com/jackc/pgx/v5 \
  github.com/ClickHouse/ch-go \
  github.com/99designs/gqlgen \
  google.golang.org/grpc google.golang.org/protobuf \
  github.com/prometheus/client_golang \
  golang.org/x/sync/errgroup \
  gonum.org/v1/gonum
```

**Step 2 — Pick & install ANN path (if you’ll serve #7/#2/#12 soon)**

* *Option A:* OpenSearch + k‑NN plugin → `opensearch-go` client.
* *Option B:* Milvus → `milvus-sdk-go`.
* *Option C:* FAISS (C++) → add native kernel path.

**Step 3 — Optimizer path for #4/#9**

* *Option A:* OR‑Tools (C++ core) with cgo wrapper (build in Docker).
* *Option B:* Python job with `ortools` writing plans/assignments to PG.
* *Option C:* CBC/GLPK via Go binding (OSS, easier packaging, slower).

**Step 4 — Create two foundation jobs (daily)**

* `visitor_cohorts` (PG table + CronJob)
* `customer_signals` (PG table + CronJob)

**Step 5 — Add your first model pair**

* **CLV scoring job** + (optional) **clv_fit** param job; add `tiers_clv.schema.json`; Cron + SLO.
* **Cross‑sell** job + GraphQL service & schema; wire `serving.cross_sell` in `signals.yaml`.

**Step 6 — Wire observability**

* Add `/readyz` SLO checks and `opsStatus` GraphQL; attach Prometheus scrape config; drop in the **analytics‑ops** Grafana board.

---

## 10) One‑screen “why this architecture works” (keep with the archive)

* **Jobs** isolate heavy math from request paths; outputs are versioned & auditable.
* **Signals** stays thin and fast; GraphQL/gRPC expose just what pages/services need.
* **Postgres + ClickHouse** split keeps serving cheap & scans fast.
* **Per‑tenant params** update automatically (Cron), **freshness SLOs** enforce health.
* **Language pragmatism:** Go by default; use Python/C++/Rust only for **clear wins** (HDBSCAN, MILP, ANN cores).
* **Native kernels** integrate cleanly: cgo for batch, sidecar for online—no system rewrite.
* **Everything is IaC**: CronJobs, Deployments, Secrets, and DSNs live in manifests/configs; rollbacks and backfills are controlled by `model_version`.

---

### Appendix — Action tags you can grep

* **[PROVISION SYSTEM]** Registry • K8s • Secrets • PG • CH • Observability • ANN engine • Optimizer • CI/CD
* **[ADD NEW FOLDER]** `jobs/<domain>/<model>/` • (optional) `jobs/<domain>/<model>_train/`
* **[ADD NEW FILE]** `configs/<model>.yaml` • `schemas/models/<dataset>.schema.json` • `deploy/k8s/jobs/<model>-cronjob.yaml` • `signals` files if served
* **[EDIT EXISTING FILE]** `common/config/load.go` • `common/datastore/{postgres,clickhouse}.go` • `signals/cmd/main.go` • `signals/internal/store/factory.go` • `signals/internal/validators/*`

---

**Ready for handoff.**
Use Sections **5 & 9** as the install plan; Sections **1–3** as your coding map (where logic goes); Section **4** to decide if/when to add Python/C++/Rust; Section **6** to wire SLOs/ops from day one.




.
├── analytics
│   ├── jobs
│   │   ├── customer
│   │   │   └── rfm
│   │   │       └── Dockerfile
│   │   └── product
│   │       ├── abcxyz
│   │       │   └── Dockerfile
│   │       ├── affinity
│   │       │   └── Dockerfile
│   │       └── velocity
│   │           └── Dockerfile
│   └── signals
│       └── Dockerfile
├── common
│   ├── config
│   │   └── load.go
│   ├── datastore
│   │   ├── clickhouse.go
│   │   └── postgres.go
│   └── timewin
│       └── window.go
├── configs
│   ├── customer_rfm.yaml
│   ├── product_abcxyz.yaml
│   ├── product_affinity.yaml
│   ├── product_velocity.yaml
│   └── signals.yaml
├── dashboards
│   └── analytics-ops.json
├── deploy
│   ├── k8s
│   │   ├── jobs
│   │   │   ├── customer-rfm-cronjob.yaml
│   │   │   ├── product-abcxyz-cronjob.yaml
│   │   │   ├── product-affinity-cronjob.yaml
│   │   │   └── product-velocity-cronjob.yaml
│   │   ├── secrets
│   │   │   ├── core-dsns.yaml
│   │   │   └── signals-dsns.yaml
│   │   └── services
│   │       ├── signals-deployment.yaml
│   │       └── signals-service.yaml
│   └── observability
│       └── prometheus-scrape.yaml
├── docs
│   ├── registry-policy.md
├── env
├── jobs
│   ├── customer
│   │   └── rfm
│   │       ├── cmd
│   │       │   └── main.go
│   │       └── pipeline
│   │           ├── customer_rfm_pipeline.go
│   │           └── customer_rfm_validate.go
│   └── product
│       ├── abcxyz
│       │   ├── cmd
│       │   │   └── main.go
│       │   └── pipeline
│       │       ├── product_abcxyz_pipeline.go
│       │       └── product_abcxyz_validate.go
│       ├── affinity
│       │   ├── cmd
│       │   │   └── main.go
│       │   └── pipeline
│       │       ├── product_affinity_pipeline.go
│       │       └── product_affinity_validate.go
│       └── velocity
│           ├── cmd
│           │   └── main.go
│           └── pipeline
│               ├── product_velocity_pipeline.go
│               └── product_velocity_validate.go
├── layout.yaml
├── saftey.yaml
├── schemas
│   └── models
│       ├── affinity_edges.schema.json
│       ├── customer_segments.schema.json
│       └── product_signals.schema.json
└── signals
    ├── cmd
    │   └── main.go
    ├── internal
    │   ├── graph
    │   │   └── schema
    │   │       ├── affinity_signals.graphqls
    │   │       ├── affinity_signals.resolver.go
    │   │       ├── customer_signals.graphqls
    │   │       ├── customer_signals.resolver.go
    │   │       ├── ops.graphqls
    │   │       ├── ops.resolver.go
    │   │       ├── product_signals.graphqls
    │   │       └── product_signals.resolver.go
    │   ├── models
    │   │   ├── affinity_edge.go
    │   │   ├── customer_segment.go
    │   │   └── product_signal.go
    │   ├── services
    │   │   ├── affinity_signals_service.go
    │   │   ├── customer_signals_service.go
    │   │   └── product_signals_service.go
    │   ├── store
    │   │   ├── clickhouse
    │   │   │   └── affinity_store_ch.go
    │   │   ├── factory.go
    │   │   ├── interfaces.go
    │   │   └── postgres
    │   │       ├── customer_store_pg.go
    │   │       └── product_store_pg.go
    │   └── validators
    │       ├── affinity_signals_validator.go
    │       ├── customer_signals_validator.go
    │       └── product_signals_validator.go
    └── proto
        ├── affinity_signals.proto
        ├── customer_signals.proto
        └── product_signals.proto



layout.yaml: title: Analytics Layout — How Existing Features Plug In
summary: >
  This page explains the final folder/file layout and exactly where logic from existing features
  (product, cart, order, customer) is placed to run the models you select. It is layout-first:
  which files own configuration, input reads, compute, validation, output writes, and serving
  (gRPC/GraphQL). No implementation code here—just where things live and how they connect.

layout:
  root: analytics/
  dirs:
    - path: configs/
      purpose: Central runtime configs:
        - signals.yaml → serving backend per dataset (customer_segments, product_signals, affinity_edges)
        - <model>.yaml → per-job inputs/outputs/time windows (e.g., customer_rfm.yaml)
    - path: common/
      purpose: Shared helpers used by jobs only
      children:
        - config/load.go        # read YAML files under analytics/configs
        - datastore/postgres.go # query helpers to read inputs & write outputs (tables)
        - datastore/clickhouse.go # query helpers for wide scans (inputs)
        - timewin/window.go     # helpers for rolling windows (last_30d, last_90d)
    - path: jobs/
      purpose: Batch compute (one binary per model). Each job = cmd/ + pipeline/
      note: Logic you add from existing features to fetch inputs lives in common/datastore/* and is used here.
    - path: schemas/models/
      purpose: JSON blueprints for output table shapes (docs/contracts; not runtime validation)
    - path: signals/
      purpose: Read-only serving slice that mirrors your feature pattern (models → services → validators → GraphQL → proto)
      note: Reads Postgres/ClickHouse based on configs/signals.yaml; no compute here.

models_jobs:  # where to place logic for each model
  customer_rfm:
    files:
      - jobs/customer/rfm/cmd/main.go            # orchestrates: load configs → read inputs → run pipeline → upsert outputs
      - jobs/customer/rfm/pipeline/customer_rfm_pipeline.go  # compute R/F/M + segment assignment
      - jobs/customer/rfm/pipeline/customer_rfm_validate.go  # validate output rows before write
      - configs/customer_rfm.yaml                 # choose input stores + output store/table + window + model_version
    inputs_from_existing_features:
      order: [user_id, total, created_at]        # add read helpers in common/datastore/{postgres,clickhouse}.go
      customer: [user_id, created_at]            # add read helpers if needed
    output_table:
      name: customer_segments
      schema_contract: schemas/models/customer_segments.schema.json
      write_via: common/datastore/postgres.go    # upsert rows here (even if inputs came from CH)

  product_velocity:
    files:
      - jobs/product/velocity/cmd/main.go
      - jobs/product/velocity/pipeline/product_velocity_pipeline.go
      - jobs/product/velocity/pipeline/product_velocity_validate.go
      - configs/product_velocity.yaml
    inputs_from_existing_features:
      order: [product_id, count, created_at]
    output_table:
      name: product_signals
      schema_contract: schemas/models/product_signals.schema.json
      write_via: common/datastore/postgres.go

  product_abcxyz:
    files:
      - jobs/product/abcxyz/cmd/main.go
      - jobs/product/abcxyz/pipeline/product_abcxyz_pipeline.go
      - jobs/product/abcxyz/pipeline/product_abcxyz_validate.go
      - configs/product_abcxyz.yaml
    inputs_from_existing_features:
      order: [product_id, count, base_price, created_at]
    output_table:
      name: product_signals   # same table as velocity (shared surface)
      schema_contract: schemas/models/product_signals.schema.json
      write_via: common/datastore/postgres.go     # merge tags/abcxyz into same row by product_id

  product_affinity:
    files:
      - jobs/product/affinity/cmd/main.go
      - jobs/product/affinity/pipeline/product_affinity_pipeline.go
      - jobs/product/affinity/pipeline/product_affinity_validate.go
      - configs/product_affinity.yaml
    inputs_from_existing_features:
      order: [order_id, items{product_id, count}] # cart session adds optional enrichment if you ingest events
    output_table:
      name: affinity_edges
      schema_contract: schemas/models/affinity_edges.schema.json
      write_via: common/datastore/{postgres.go|clickhouse.go} # pick in config; table name is stable

serving_signals:  # where to place logic to serve the model outputs
  entrypoint:
    - signals/cmd/main.go  # loads configs/signals.yaml, wires stores → services, starts gRPC/GraphQL

  customer_segments:
    files:
      - signals/internal/models/customer_segment.go
      - signals/internal/services/customer_signals_service.go
      - signals/internal/validators/customer_signals_validator.go
      - signals/internal/graph/schema/customer_signals.graphqls
      - signals/internal/graph/schema/customer_signals.resolver.go
      - signals/proto/customer_signals.proto
    store_binding:
      config_key: serving.customer_segments   # in configs/signals.yaml
      choose_impl_in:
        - signals/internal/store/factory.go
        - signals/internal/store/postgres/customer_store_pg.go  # (exists)

  product_signals:
    files:
      - signals/internal/models/product_signal.go
      - signals/internal/services/product_signals_service.go
      - signals/internal/validators/product_signals_validator.go
      - signals/internal/graph/schema/product_signals.graphqls
      - signals/internal/graph/schema/product_signals.resolver.go
      - signals/proto/product_signals.proto
    store_binding:
      config_key: serving.product_signals
      choose_impl_in:
        - signals/internal/store/factory.go
        - signals/internal/store/postgres/product_store_pg.go    # (exists)

  affinity_edges:
    files:
      - signals/internal/models/affinity_edge.go
      - signals/internal/services/affinity_signals_service.go
      - signals/internal/validators/affinity_signals_validator.go
      - signals/internal/graph/schema/affinity_signals.graphqls
      - signals/internal/graph/schema/affinity_signals.resolver.go
      - signals/proto/affinity_signals.proto
    store_binding:
      config_key: serving.affinity_edges
      choose_impl_in:
        - signals/internal/store/factory.go
        - signals/internal/store/clickhouse/affinity_store_ch.go # (exists)

how_existing_features_feed_models:  # where to put extraction logic; which IDs/metrics are used
  product_feature:
    ids: [product_id, variant_id, brand_id, category_id, sub_category_id]
    metrics: [base_price, created_at]
    extraction_location: common/datastore/{postgres.go,clickhouse.go}  # add read helpers used by jobs
    used_by_jobs: [product_velocity, product_abcxyz, product_affinity]
  cart_feature:
    ids: [session_id, order_by(user_id)]
    metrics: [items{product_id, variant_id, count}, added_at]
    extraction_location: common/datastore/{postgres.go,clickhouse.go}  # if you ingest cart events
    used_by_jobs: [product_affinity (optional enrichment)]
  order_feature:
    ids: [order_id, user_id]
    metrics: [items{product_id, variant_id, count, base_price}, subtotal, total, created_at]
    extraction_location: common/datastore/{postgres.go,clickhouse.go}
    used_by_jobs: [customer_rfm, product_velocity, product_abcxyz, product_affinity]
  customer_feature:
    ids: [user_id]
    metrics: [created_at]
    extraction_location: common/datastore/{postgres.go}
    used_by_jobs: [customer_rfm]

data_flow_end_to_end:
  steps:
    - "Job loads configs/<model>.yaml via common/config/load.go"
    - "Job reads inputs via common/datastore/{postgres,clickhouse}.go"
    - "Job runs pipeline/*.go and validates outputs with *_validate.go"
    - "Job upserts rows into output table (customer_segments | product_signals | affinity_edges)"
    - "signals/cmd/main.go loads configs/signals.yaml and selects store impl per dataset"
    - "signals/internal/services/* serve rows via gRPC (proto/*.proto) and GraphQL (graph/schema/*.graphqls)"

contracts_and_ids:
  canonical_ids:
    product: product_id
    variant: variant_id
    brand: brand_id
    category: category_id
    sub_category: sub_category_id
    customer: user_id
    order: order_id
  output_tables:
    customer_segments: schemas/models/customer_segments.schema.json
    product_signals:   schemas/models/product_signals.schema.json
    affinity_edges:    schemas/models/affinity_edges.schema.json
  keys_and_versioning:
    - "Upsert keys include entity id + model_version + as_of to allow safe backfills"
    - "IDs must be immutable; jobs never rewrite core feature data"

authoring_checklist_for_new_model:
  - "Create job skeleton under jobs/<domain>/<model>/{cmd, pipeline}"
  - "Add inputs and output config under configs/<model>.yaml (choose stores)"
  - "Add/extend read helpers in common/datastore/* for any new inputs"
  - "Define or reuse an output table contract in schemas/models/*.json"
  - "If you want it served, add signals files (model/service/validator/graphql/proto)"
  - "Bind dataset → backend in configs/signals.yaml; keep only the needed store file (pg or ch)"



saftey.yaml: title: Analytics Reliability & Ops — Implementation Outline (Layout-Preserving)
summary: >
  This document explains exactly how to add health/safeguards/observability to the existing Analytics
  layout without changing folders. It defines config keys, where logic lives in current files, what
  gets checked at boot and daily, and how to surface status to a future Super Admin Ops panel via
  the existing signals service (gRPC/GraphQL).

scope:
  goals:
    - Catch misconfigurations at boot (fail fast).
    - Detect stale/missing data early (freshness SLOs).
    - Keep jobs idempotent and bounded (timeouts/retries).
    - Expose a simple Ops status API for dashboards and alarms.
  non_goals:
    - No new directories; only edits to existing files.
    - No runtime JSON-schema validation (schemas/ are docs-only).

configs:
  files:
    - path: analytics/configs/signals.yaml
      role: Serving SLOs + backend mapping per dataset.
      keys:
        serving:
          customer_segments: { store: postgres, dsn_env: SIGNALS_PG_DSN }
          product_signals:   { store: postgres, dsn_env: SIGNALS_PG_DSN }
          affinity_edges:    { store: clickhouse, dsn_env: SIGNALS_CH_DSN }
        slo:
          freshness_hours:
            customer_segments: 36
            product_signals:   36
            affinity_edges:    72
          min_rows:
            customer_segments: 1
            product_signals:   1
            affinity_edges:    1
          timeouts_ms:
            store_ping: 1500
            request_default: 2000
          paging:
            max_limit: 200
    - path: analytics/configs/customer_rfm.yaml
      role: Job runtime bounds & IO selection.
      keys:
        inputs:
          orders:    { store: postgres|clickhouse, dsn_env: CORE_PG_DSN|CORE_CH_DSN }
          customers: { store: postgres, dsn_env: CORE_PG_DSN }
        window: last_180d
        output: { table: customer_segments, store: postgres, dsn_env: SIGNALS_PG_DSN }
        model_version: rfm-1.0.0
        job_slo:
          max_runtime_sec: 900
          max_retries: 2
          backoff_sec: 30
    - path: analytics/configs/product_velocity.yaml
      role: Same pattern; define inputs/output + job_slo.
    - path: analytics/configs/product_abcxyz.yaml
      role: Same pattern; define inputs/output + job_slo.
    - path: analytics/configs/product_affinity.yaml
      role: Same pattern; define inputs/output + job_slo.

service_boot_checks:
  file: signals/internal/store/factory.go
  responsibilities:
    - Load analytics/configs/signals.yaml.
    - Verify each dataset mapping has a matching store impl:
      - postgres → *_store_pg.go
      - clickhouse → *_store_ch.go
    - Ping each backend with timeout (slo.timeouts_ms.store_ping).
    - Check freshness & row count SLOs (slo.freshness_hours, slo.min_rows) via lightweight SELECTs.
    - On any failure: return error → process exits (no partial boot).

service_runtime:
  entrypoint: signals/cmd/main.go
  responsibilities:
    - Read slo.timeouts_ms.request_default and apply as default per-request context timeout.
    - Wire stores from factory and pass into:
      - signals/internal/services/customer_signals_service.go
      - signals/internal/services/product_signals_service.go
      - signals/internal/services/affinity_signals_service.go
    - Start servers and expose health:
      http_health:
        - /healthz : process up + store pings pass
        - /readyz  : all boot checks passed + last freshness checks within SLO
    - Log one line on boot with final bindings:
      - customer_segments=postgres, product_signals=postgres, affinity_edges=clickhouse

service_validators:
  files:
    - signals/internal/validators/customer_signals_validator.go
    - signals/internal/validators/product_signals_validator.go
    - signals/internal/validators/affinity_signals_validator.go
  checks:
    - Required IDs present.
    - Enforce slo.paging.max_limit.
    - Clamp numeric ranges (e.g., velocity filters) if you add them later.

ops_api_for_dashboard:
  add_graphql_types:
    file: signals/internal/graph/schema/ops.graphqls
    types:
      - type OpsDatasetStatus { name: String!, store: String!, ok: Boolean!, rows: Int!, maxAgeHours: Int!, lastAsOf: Time, message: String }
      - type OpsStatus { datasets: [OpsDatasetStatus!]!, startedAt: Time!, version: String! }
    queries:
      - extend type Query { opsStatus: OpsStatus! }
  resolvers:
    file: signals/internal/graph/schema/ops.resolver.go
    sources:
      - Read current bindings from factory (in-memory or injected).
      - For each dataset, run COUNT + MAX(as_of) with short timeouts.
      - Compare to slo.freshness_hours & slo.min_rows; set ok/message.
  purpose:
    - Power a Super Admin panel card with green/yellow/red statuses.
    - Enable quick triage without shell access.

jobs_safeguards:
  per_job_entrypoint: jobs/<domain>/<model>/cmd/main.go
  responsibilities:
    - Load configs/<model>.yaml.
    - Create context with deadline (job_slo.max_runtime_sec).
    - Read inputs via common/datastore/{postgres.go,clickhouse.go}.
    - Run pipeline/* and then *_validate.go checks:
      - Required keys set (id, model_version, as_of).
      - Non-negative numeric fields; dedupe on (id, model_version, as_of).
    - Upsert outputs (Postgres recommended).
    - Retries on transient failures (job_slo.max_retries, backoff).
    - Exit non-zero on failure (external scheduler will alert/retry).

common_datastore_contracts:
  files:
    - common/datastore/postgres.go
    - common/datastore/clickhouse.go
  musts:
    - All queries accept context with deadlines.
    - Short, bounded connection pools.
    - Helpers: SelectFreshness(table), SelectCount(table), UpsertRows(table, rows).

data_slo_definitions:
  datasets:
    - name: customer_segments
      freshness_hours: use configs/signals.yaml (default 36)
      min_rows:        use configs/signals.yaml (default 1)
      as_of_field:     as_of
    - name: product_signals
      freshness_hours: use configs/signals.yaml
      min_rows:        use configs/signals.yaml
      as_of_field:     as_of
    - name: affinity_edges
      freshness_hours: use configs/signals.yaml (default 72 if clickhouse)
      min_rows:        use configs/signals.yaml
      as_of_field:     as_of

alerts_and_reporting:
  triggers:
    - on_boot_failure: factory returns error → process never serves; external supervisor restarts & alerts.
    - on_slo_breach: opsStatus shows ok=false → dashboard red state and page.
  suggested_metrics:  # names only; emit however you prefer
    - signals_boot_ok (gauge 0/1)
    - signals_dataset_ok{dataset} (gauge)
    - signals_dataset_rows{dataset} (gauge)
    - signals_dataset_max_age_hours{dataset} (gauge)
    - job_rows_written{job} (counter)
    - job_runtime_seconds{job} (gauge or histogram)
  minimum_logs:
    - Boot line: bindings + freshness summary per dataset.
    - Per-job completion: rows written, duration, window, model_version.

admin_panel_wiring:
  source: signals GraphQL → Query.opsStatus
  widgets:
    - “Datasets” table: name, store, rows, lastAsOf, maxAgeHours, status.
    - “Recent jobs” (optional later): derive from logs or add a thin audit table.
  actions:
    - Link to configs/ (read-only) and model docs (schemas/models/*.json).

ci_safeguards:
  checks:
    - Validate configs/signals.yaml is present and parseable.
    - Ensure each serving.<dataset>.store has matching *_store_{pg|ch}.go file.
    - Build signals (go build ./signals/cmd).
  optional:
    - Sample row validation against schemas/models/*.json during PRs (doc parity).

runbooks:
  boot_fails:
    - Check signals.yaml → store mapping; ensure matching store file exists.
    - Verify DSN env vars present and reachable.
    - Inspect dataset SLOs vs actual last as_of.
  stale_data:
    - Inspect job logs and rerun affected job with the same configs window.
    - Confirm output rows upserted; recheck opsStatus.

final_notes:
  - No new folders; everything above lives in the existing files/paths.
  - Start with opsStatus GraphQL + boot fail-fast; add freshness/row checks next.
  - Keep SLOs in configs/signals.yaml so operations never require code edits.





BG/NBD + Gamma-Gamma (Customer Lifetime Value / CLV)


Two-Tower Recommender (candidate generation) + Learning-to-Rank (ranking)


Context-Aware Re-Ranking (Session/Basket Re-ranking)


Uplift Modeling (Treatment Effect / Causal ML for promotions)


Customer Segmentation (HDBSCAN / K-Means)


Time-Decayed Affinity Scoring (Dirichlet-smoothed)


Item-Item Collaborative Filtering (co-occurrence / cosine)


Learning-to-Rank (RankNet) for Search/PLP


Markdown Price Optimization (Dynamic Programming / MILP)


Log-Log Price Elasticity + Lerner Index


Upsell Propensity Modeling (Logistic/GBM with profit guardrail)


Market Basket Analysis / Complementary Recommendation (Lift / Graph embeddings)


Product Bundling Optimization (Knapsack / MILP)


Churn Prediction (Logistic/GBM) with economic thresholding


Hidden Semi-Markov Models (HSMM) for Lifecycle Timing (Hawkes optional)


BG/NBD + Gamma-Gamma (Customer Lifetime Value / CLV)


Two Core Models:
A. BG/NBD Model - predict the number of future transactions a customer is likely to make. [Historical purchasing patterns/observation window] — Frequency & Recency of a customer's past orders to estimate how many purchases they’ll likely make in the future. 
B. Gamma-Gamma Model - predicts the average monetary value of each customer will make in future transactions. [Average margin of past transactions to project the monetary value of further ones]
Combining these two core models we can derive an overall customer lifetime value (CLV) prediction that tells us how much revenue (in terms of net margin) we can expect from a customer over a certain period. 

Values for the BG/NBD Sub-Model:


Number of transactions (x): For each customer, how many purchases they made in the observation window (for example, the past 365 days).


Recency (t_x): The time between the customer’s first and last purchase in that window. If they only made one purchase, recency is zero.


Tenure (T): The length of the observation period for that customer, typically the total number of days in the window.


Model hyperparameters (r, α, a, b): These are parameters that define the shape of the BG/NBD model and are usually learned from historical data.


Values for the Gamma–Gamma Sub-Model:


Average margin per transaction (m̄): For each customer, the average net margin of their past purchases in the observation window.


Transaction count (x): The same transaction count used in the BG/NBD model.


Model hyperparameters (p, q, γ): These are parameters that define the shape of the Gamma–Gamma model and are also typically learned from historical data.


Foundational Baseline Calculations Needed:
Gross Revenue Calculation
Net Revenue Calculation
Order Margin Calculation
Transaction Counts and Timing
Average Margin per Transaction

 
Parameters for Core Model A - BG/NBD: (4 parameters)
r, alpha, a, b - how likely a customer is to make a repeat purchase and how quickly they drop off. (Probability distribution of how often customers buy again)

Parameters for Core Model B - Gamma-Gamma: (3 parameters)
p, q, gamma - control the distribution of the average transaction value, adjusting how we predict the monetary value of future purchases. 

How are parameters tuned? - Parameters are tuned through algorithms
Statistical algorithm: 
MLE (maximum likelihood estimation) — best-fitting parameters
Step 1: Likelihood function 
Step 2: Logarithm of Likelihood function  
Step 3: Adjust the parameters to maximize the log-likelihood


Units/Constraints
Time Window or Observation Window: 
Time Unit
Money Unit 




Here’s your outline, updated to include bootstraps + holdout backtests and a few missing math/data details—kept in your style, numbers & formulas only, no other method mentioned.

BG/NBD + Gamma–Gamma (Customer Lifetime Value / CLV)
Two Core Models
A. BG/NBD — predicts the number of future transactions a customer will make from historical frequency & recency in an observation window.
B. Gamma–Gamma — predicts the average monetary value per future transaction from the customer’s average past net margin.
CLV over horizon H:
\text{CLV}_H \;=\; \mathbb{E}[N(H)] \times \mathbb{E}[M]

1) Values for the BG/NBD Sub-Model
x — number of transactions in the observation window W (e.g., last 365d).


t_x — recency: time from first to last purchase inside W (same unit as T). If only one purchase, t_x=0.


T — tenure: length of the customer’s observed window (first purchase → window end).


Hyperparameters (r,\alpha,a,b) — learned per tenant via MLE.



2) Values for the Gamma–Gamma Sub-Model
\bar m — average net margin per transaction (cents) over purchases in W (if x>0).


x — transaction count (same as above).


Hyperparameters (p,q,\gamma) — learned per tenant via MLE.



Foundational Baseline Calculations Needed (order-level → user-level)
Revenue/cost policy (decide once)
Include shipping in revenue? YES/NO


Include tax in revenue? YES/NO


Handle refunds per order (preferred) OR via a horizon-level returns factor (choose one, not both).


Include payment fees? YES/NO


Per-order formulas (cents)
Gross revenue

 \[

 \text{gross\rev} = \text{subtotal} - \text{discount} + [\text{shipping}]{\text{if YES}} + [\text{tax}]_{\text{if YES}}

 \]


Net revenue

 \text{net\_rev} = \text{gross\_rev} - \text{refund} - \text{chargeback}


Order margin


\[
m_i = \text{net\_rev} - \text{COGS} - [\text{payment\fees}]{\text{if tracked}}
\]
Per-customer sufficient stats
Total margin M_{\text{sum}} = \sum_i m_i


Average per-transaction margin \bar m = M_{\text{sum}}/x (only if x>0)


x, t_x, T as above (from timestamps)



Parameters for Core Model A — BG/NBD (4)
(r, \alpha, a, b) — control purchase incidence and dropout hazard.


Parameters for Core Model B — Gamma–Gamma (3)
(p, q, \gamma) — control the distribution of average transaction margin.



How parameters are tuned (statistical algorithm)
MLE (maximum likelihood estimation)
Likelihood: build per-customer likelihood from (x, t_x, T) (BG/NBD) and (x, \bar m) (Gamma–Gamma).


Log-likelihood: sum logs to stabilize numerics.


Maximize: adjust parameters to maximize log-likelihood (per tenant).



Uncertainty & Validation (add now)
A) Bootstrap confidence intervals (CIs)
 — 
numbers & steps only
Goal: intervals for \mathbb{E}[N(H)], \mathbb{E}[M], and \text{CLV}_H per user.
Inputs: tenant’s customer table of (x, t_x, T, \bar m).
Steps (per tenant):
Choose B (e.g., B=100).


For each b=1..B: sample customers with replacement; refit BG/NBD and Gamma–Gamma (MLE).


Recompute \mathbb{E}[N(H)], \mathbb{E}[M], \text{CLV}_H for all customers using the b-th parameter set.


For each customer, take percentiles across b (e.g., 10th/50th/90th or 2.5/50/97.5) → CI bands.


Optionally report P(VIP): share of bootstrap CLVs above VIP threshold.


Outputs: per-customer median CLV, CI low, CI high (and optional P(VIP)).

B) Holdout backtests
 — 
protocol only (no math)
Goal: verify calibration & rank power before production tiering.
Windows:
Calibration W_{\text{cal}} — fit period (e.g., 26–52 weeks).


Holdout H_{\text{test}} — future period (e.g., 13–26 weeks; ≥ ½ of W_{\text{cal}}).


Steps (per tenant):
Fit BG/NBD + Gamma–Gamma on W_{\text{cal}}.


Predict purchases in holdout: \widehat{N}(H_{\text{test}}) and \widehat{\text{GM}}(H_{\text{test}}).


Compare to actual:


RMSE/MAE on holdout purchases (or margin).


Spearman between predicted CLV and realized holdout GM.


Decile lift: top-10% predicted vs average holdout GM.


Calibration plot: observed vs predicted by RFM bins.


Criteria (set once): accept if metrics meet thresholds; otherwise adjust window length, add seasonality guard (see below), or revisit returns policy.



Optional corrections (choose only if needed)
Returns factor (horizon-level):

 \mathbb{E}[M]_{\text{adj}} = (1 - \text{returns\_rate}) \cdot \mathbb{E}[M]

 (Use this instead of per-order refunds if you need a simpler path.)


Seasonality guard: if W misses peak/holiday cycles, multiply \mathbb{E}[N(H)] by a seasonal factor s estimated from historical month/weekday patterns (set once per tenant).



Units / Constraints / Edge cases
Time unit: use days (or hours if business case requires) but keep t_x, T, H consistent.


Money unit: cents. Keep non-negative margins.


x = 0: set \bar m to a tenant prior (e.g., median order margin) and compute \mathbb{E}[M] from Gamma–Gamma using that prior; BG/NBD handles x=0 via likelihood.


Outliers: cap extreme margins or remove obvious test orders (via status_is_active, fraud flags).



Tiering Policy (no math)
Percentile-based (e.g., VIP ≥ P90; High P70–P90; Med P40–P70; Low < P40) or


Absolute cent thresholds (tenant-specific).


Apply to median CLV (from bootstrap); optionally require P(VIP) ≥ threshold for VIP stability.



Summary: what changed vs your original
Added exact order-level formulas (gross, net, margin) and policy choices (shipping/tax/fees/refunds).


Added Bootstrap CIs (steps & outputs) to quantify uncertainty per customer/tenant.


Added Holdout backtest protocol (windows, metrics, acceptance criteria).


Added Seasonality guard and a returns factor option (if you don’t do refunds per order).


Clarified edge cases (x=0) and tiering on median CLV with optional stability rule.


This keeps your method intact while making it validation-ready and uncertainty-aware.



Pareto/NBD (Bayesian) + Gamma–Gamma (Customer Lifetime Value / CLV)
Mode: numbers & formulas only (no arithmetic). Horizon: H=180 days. Window: W=365 days.
Keys: store_id, user_id. Units: money in cents, timestamps ISO-8601.

Two Core Models
A. Pareto/NBD (incidence, Bayesian) — predicts the number of future transactions from historical frequency & recency in an observation window; lifetime is continuous (Exponential) with heterogeneity.
B. Gamma–Gamma (monetary) — predicts the average margin per future transaction from the customer’s average past net margin.
CLV over horizon H:
\text{CLV}_H \;=\; \mathbb{E}[N(H)] \times \mathbb{E}[M]

1) Values for the Pareto/NBD Sub-Model (Bayesian)
x — number of transactions in the observation window W (e.g., last 365d).


t_x — recency: time from first to last purchase in W (same unit as T); if only one purchase, set t_x=0.


T — tenure: time from first purchase to window end (same unit as t_x).


Population hyperparameters (to estimate under priors)


Purchase rate heterogeneity: (r,\alpha) for \lambda \sim \text{Gamma}(r,\alpha)


Lifetime heterogeneity: (s,\beta) for \mu \sim \text{Gamma}(s,\beta)

 (You will specify priors for these and fit via MAP or MCMC/VI.)



2) Values for the Gamma–Gamma Sub-Model
\bar m — average net margin per transaction (cents) over purchases in W (if x>0).


x — transaction count (same as above).


Monetary parameters (to estimate) (p,q,\gamma) — distribution of average transaction margin.



Foundational Baseline Calculations Needed (order-level → user-level)
Revenue/cost policy (decide once)
Include shipping in revenue? YES/NO


Include tax in revenue? YES/NO


Handle refunds per order (preferred) OR via horizon-level returns factor (choose one, not both).


Include payment fees? YES/NO


Per-order formulas (cents)
Gross revenue

 \[

 \text{gross\rev} = \text{subtotal} - \text{discount} + [\text{shipping}]{\text{if YES}} + [\text{tax}]_{\text{if YES}}

 \]


Net revenue

 \text{net\_rev} = \text{gross\_rev} - \text{refund} - \text{chargeback}


Order margin


\[
m_i = \text{net\_rev} - \text{COGS} - [\text{payment\fees}]{\text{if tracked}}
\]
Per-customer sufficient stats
Total margin M_{\text{sum}} = \sum_i m_i


Average per-transaction margin \bar m = M_{\text{sum}}/x (only if x>0)


x, t_x, T as above (from timestamps)



Parameters for Core Model A — Pareto/NBD (Bayesian)
(r,\alpha) for purchase-rate heterogeneity; (s,\beta) for lifetime heterogeneity.


Priors (choose numbers): weakly-informative gamma priors for each hyperparameter (set once per tenant).


Estimation: MAP (fast) or MCMC/VI (full posterior) on per-customer (x,t_x,T).


Parameters for Core Model B — Gamma–Gamma (3)
(p,q,\gamma) — control distribution of average transaction margin; fit via MAP or MLE (choose one and stick to it).



Incidence & Monetary Functions (plug numbers, no arithmetic)
A) Pareto/NBD — expected 
future transactions
 (T,\ T+H]
\boxed{\; \mathbb{E}\big[N(H)\big] \;=\; \texttt{PARETO\_NBD\_ExpectedTransactions}(x,\ t_x,\ T,\ H\ ;\ r,\alpha,\ s,\beta) \;}
(Closed-form in Pareto/NBD; you will plug x,t_x,T,H,r,\alpha,s,\beta.)
Optional gate — Probability alive at T:
P_{\text{alive}} \;=\; \texttt{PARETO\_NBD\_PAlive}(x,\ t_x,\ T\ ;\ r,\alpha,\ s,\beta)
B) Gamma–Gamma — expected 
per-transaction margin
\boxed{\; \mathbb{E}[M \mid x,\bar m;\ p,q,\gamma] \;=\; \frac{(q+x)\,(\gamma + x\,\bar m)}{(q-1)\,(p+x)} \;}
(Returns cents per transaction; requires q>1.)
C) CLV assembly (no discount by default)
\boxed{\; \text{CLV}_H \;=\; \mathbb{E}[N(H)] \times \mathbb{E}[M] \;}
Optional PV (daily discount r_d=d/365):
\[
\text{PV\factor} \approx \frac{1 - (1+r_d)^{-H}}{r_d\,H},\quad
\text{CLV}{H,\text{PV}} = \text{CLV}_{H}\times \text{PV\_factor}
\]

How parameters are tuned (Bayesian)
Specify priors for (r,\alpha,s,\beta) (and optionally (p,q,\gamma)).


Inference option 1 — MAP (recommended default for speed):


Maximize posterior → point estimates \tilde r,\tilde\alpha,\tilde s,\tilde\beta (and \tilde p,\tilde q,\tilde\gamma if Bayesian monetary).


Inference option 2 — MCMC/VI (full uncertainty):


Draw samples \{\theta^{(k)}\}_{k=1..K} from the posterior.


Posterior expectation for each customer:

 \[

 \mathbb{E}[N(H)] \approx \frac{1}{K}\sum_{k=1}^K \texttt{PARETO\_NBD\ExpectedTransactions}(\cdot\mid\theta^{(k)})

 \]

 \mathbb{E}[M] \approx \frac{1}{K}\sum{k=1}^K \texttt{GG\_ExpectedMargin}(\cdot\mid\theta^{(k)})


Report median CLV with credible intervals.



Uncertainty & Validation
A) Posterior credible intervals
 (native, if using MCMC/VI)
For each customer, compute CLV per posterior draw and take quantiles (e.g., 10/50/90 or 2.5/50/97.5).


If using MAP only, you can fall back to bootstrap CIs (same B-resample procedure as in the other outline).


B) Holdout backtests
 — 
protocol only
Windows: Calibration W_{\text{cal}} (fit) and Holdout H_{\text{test}} (future; ≥½ of W_{\text{cal}}).


Steps: fit on W_{\text{cal}}; predict \widehat{N}(H_{\text{test}}) / \widehat{\text{GM}}(H_{\text{test}}); compare to actual via RMSE/MAE, Spearman, decile lift, calibration plot (bins by RFM).


Accept/adjust: set thresholds; if off, adjust W, add seasonality guard, or refine priors.



Optional corrections (choose only if needed)
Returns factor (horizon-level):

 \mathbb{E}[M]_{\text{adj}} = (1 - \text{returns\_rate}) \cdot \mathbb{E}[M]


Seasonality guard: if W misses peak cycles, apply a seasonal factor s to \mathbb{E}[N(H)].



Units / Constraints / Edge cases
Time unit: use days (or hours if business case requires) and keep t_x, T, H consistent.


Money unit: cents. Keep margins non-negative.


x = 0: set \bar m to a tenant prior (e.g., median order margin) for \mathbb{E}[M]; Pareto/NBD handles x=0 via likelihood.


Outliers: cap extreme margins; exclude test/fraud orders (status_is_active, flags).



Tiering Policy (no math)
Percentile-based (e.g., VIP ≥ P90; High P70–P90; Med P40–P70; Low < P40) or


Absolute cent thresholds (tenant-specific).


Apply to median CLV (posterior) or MAP CLV; optionally require minimum credible mass above VIP threshold (e.g., P(\text{CLV} \ge \tau_{\text{VIP}}) \ge 0.6).



Summary (what’s specific here)
Incidence uses Pareto/NBD with continuous (Exponential) lifetime and Gamma heterogeneity; parameters estimated via MAP or MCMC/VI under priors.


Monetary is Gamma–Gamma; same inputs and expectation formula as before.


Uncertainty via posterior credible intervals (or bootstrap if MAP only); holdout protocol identical.


Everything else (order-level margin ledger, units, tiering, optional corrections) mirrors your BG/NBD outline.




Resource Compilation (Deduplicated) — Blocks 1–15
What I did: merged every list, outline, metric, formula, model, header set, and workflow from all scraping blocks; removed repeats and near-duplicates; normalized names (kept the clearest phrasing); collapsed overlapping “how-to” outlines into one canonical version each; excluded obviously duplicate code blobs. No new opinions added.

1) Metrics & Dimensions (deduplicated)
Website & User Behavior
Website Traffic (sessions, users), New vs Returning Users, 1/7/28-day active users, Session Duration, Sessions per User, User Engagement Duration, Engaged Sessions, Engagement Rate, Scroll Depth, Scrolled Users, Page Views, Views per Session, Views per User, Single-Page Sessions (Bounces), Exit Pages/Exit Rate, Number of Pages per Session, Time on Page, Page Sequence / Click Path.


Events & Interactions
Event Count, Event Count per User, Event Value, Events per Session, Button Clicks, Form Submissions, Video Engagement, Newsletter Sign-ups, Social Shares, Customer Support Interaction, Subscription Services.


E-commerce / Product / Sales
Total Revenue, Sales Volume, Average Order Value (AOV), Gross Margin, Gross Profit, Net Profit Margin, COGS, Product Price, Product Views, Add to Cart/Remove/Update, Wishlist, Comparisons, Reviews/Ratings, Items Added/Checked Out/Purchased, Purchase-to-View Rate, Cart-to-View Rate, Checkouts, Payment Success/Failure & Methods, Order Quantity/Value, Order Modification, Shipping Amount, Tax Amount, Item Discount/Refund Amount, Product Return Rate (+ by channel), Inventory Level/Stock, Order Fulfillment Time, Total Purchasers, Transactions, Transactions per Purchaser, Gross Item/Purchase Revenue, Average Purchase Revenue.


Conversions
Conversions, Conversion Rate (site/goal), Session Conversion Rate, User Conversion Rate, Purchaser Conversion Rate, First-time Purchaser Conversion Rate, First-time Purchasers, First-time Purchasers per New User, Last/First Interaction Conversions, Assisted Conversions, Lead Generation Conversions, Lead Conversion Rate, Lead Capture / First Contact / Sales Response Time, Funnel Drop-off Points/Rate, Time to Conversion, Path Length.


Advertising & Marketing
ROAS, CPC, CPM, CTR, CPA (incl. email signup), CPS (Cost per Sale), Profit per Impression (PPI), Profit-based PPC, Impressions, Ad Clicks/Cost, Promotion Views/Clicks, Publisher Ad Clicks/Impressions, RPV (Revenue per Visitor), ARPU, ARPPU, LTV:CAC Ratio, Operating Cash Flow, Expense % of Revenue.


Technical / Device / Performance
Device Type, OS & Version, Browser & Version, Screen Resolution, Language, Network Connection Type, JavaScript Support, (historical) Flash Version, Page Speed (avg load, server response, download, DNS), Core Web Vitals (LCP, FID, CLS).


Traffic Sources & Campaign
Referrers, Direct Entries, Source/Medium, Referral Path, Campaign, Social Network, Channel Grouping, UTM parameters (source, medium, campaign, term, content).


Session & Page
Session Start Time, Session Duration, Entry Page, Exit Page, Number of Pages per Session, Time on Each Page, Page Title/URL, Referrer URL, Scroll Depth.


Real-Time & Enhanced Measurement
Real-time Active Users, Sources, Locations, Content; automatic outbound link/file download/search/scroll/video tracking.


User Identification
User IDs (logged-in), Client IDs (anonymous).


Lifetime / Cohorts
Customer Lifetime Value (CLTV/LTV), Lifetime Behavior, User Cohorts (acquisition/engagement/behavior/e-commerce), Retention Rate by channel.


Google Search Console (GSC)
Search Queries, Clicks, Impressions, CTR, Avg Position, Top Pages/Countries/Devices, Index Coverage Status; Coverage (Indexed, Errors, Warnings, Excluded); URL Inspection; Mobile Usability; Enhancements (AMP, Rich Results, Sitelinks, Events); Security/Manual Actions; Links (top linking sites/pages/text); Sitemaps; International Targeting; Removals.


Demographics / Geo / Tech
Age, Gender, Interests; Country/Region/City/Language; Browser, OS, Device Category.


SaaS / Subscription (MRR/ARR stack)
Starting/New/Expansion/Churned/Net New/Ending MRR & ARR; ARPA/ARPU; Net/Gross Growth Rates; Customer/Logo counts (new, churned, ending); Renewal Rates (# and $).


Funnel & Sales (SaaS)
Visitors → Trials → Purchases; Visitor→Trial %, Trial→Purchase %; Raw Leads, Leads→MQL %, Opportunities, Opportunity→Win %, Win/Loss Ratio; # FTE Sales Reps, Quota per Rep, Forecasted Sales Capacity, Coverage Ratio, Productivity per Rep.


Operations / Support
Inventory Levels, Fulfillment Time; Time to Address/Resolve Customer Issues; Complaint Resolution Time.



2) Formulas & Calculations (deduplicated)
Web/Engagement
Avg Time on Page = Total Time on Page ÷ Visits


Bounce Rate = (Single-Page Sessions ÷ Sessions) × 100%


Views per Session = Total Views ÷ Sessions; Views per User = Total Views ÷ Users


DAU/MAU; WAU/MAU


Marketing & Ads
Conversion Rate = (Conversions ÷ Visitors/Sessions) × 100%


CPC = Ad Spend ÷ Clicks; CPM = (Ad Spend ÷ Impressions) × 1000


CTR = (Clicks ÷ Impressions) × 100%


CPA = Ad Spend ÷ Acquisitions; CPS = Ad Spend ÷ Sales


ROAS = Revenue from Ads ÷ Ad Spend


PPI = (Revenue from Ads − Ad Spend) ÷ Impressions


Profit-based PPC = (Revenue from Ads − Ad Spend) ÷ Clicks


RPV = Total Revenue ÷ Total Visits


E-commerce
Total Revenue = Σ(Price × Quantity)


Sales Volume = Σ(Quantity)


AOV = Total Revenue ÷ Orders


Gross Margin % = ((Revenue − COGS) ÷ Revenue) × 100%


Cart Abandonment % = ((Carts − Orders) ÷ Carts) × 100%


Order Fulfillment Time = Delivered − Placed


Product Return Rate = (Returned ÷ Sold) × 100%


Purchase-to-View % = (Purchases ÷ Product Views) × 100%


Cart-to-View % = (Add-to-Carts ÷ Product Views) × 100%


Transactions per Purchaser = Transactions ÷ Purchasers


Average Purchase Revenue = Purchase Revenue ÷ Purchases


Customer Value & Lifecycle
CLTV (simple) = AOV × Purchase Frequency × Customer Lifespan (× GM%)


Churn Rate = (Customers Lost ÷ Start Customers) × 100%


Revenue Churn Rate = (Lost Revenue ÷ Start Revenue) × 100%


Retention Rate = ((E − N) ÷ S) × 100%


LTV:CAC = LTV ÷ CAC


ARPU = Revenue ÷ Users; ARPPU = Revenue from Paying ÷ Paying Users


Incrementality & Lift
Incrementality % = ((Test − Control) ÷ Control) × 100%


Incremental Lift (abs) = Test − Control


True ROI (Adj.) = Gross ROI × Incrementality %


Sales Lift % = ((Sales_campaign − Baseline) ÷ Baseline) × 100%


Financial Health
Expense % of Revenue = ((Employee + Vendor) ÷ Revenue) × 100%


Operating Cash Flow = Revenue from Ops − OpEx


Net Profit Margin = (Net Income ÷ Revenue) × 100%


Statistical / Power
Power (sample size) ≈ 2 × (Zα/2 + Zβ)² × (s² ÷ δ²)


Difference-in-Differences = (Ypost,test − Ypre,test) − (Ypost,ctrl − Ypre,ctrl)


Lift % (A/B) = ((Metric_B − Metric_A) ÷ Metric_A) × 100%


SaaS (MRR/ARR & Unit Economics)
Net New MRR = New + Expansion − Churned; Ending MRR = Start + Net New; ARR = Ending MRR × 12


Customer Churn % = (Churned Customers ÷ Prior Customers) × 100%


MRR Churn/Expansion/Net % use prior Ending MRR as denominator


LTV (SaaS) = (ARPA × GM%) ÷ Churn Rate


CAC = (Sales + Marketing) ÷ New Customers


Months to Recover CAC = CAC ÷ (ARPA × GM%)


DCF/negative-churn variants (e.g., LTV_DCF) are acknowledged but kept as a single canonical note to avoid duplication.

3) Analytical Methods, Models & Techniques (deduplicated)
Segmentation & Clustering: K-means, Hierarchical, HDBSCAN; RFM scoring & named segments (Champions, Loyalists, At-Risk, Lost, etc.).
 Prediction/ML: Logistic Regression, Decision Trees, Random Forests, Gradient Boosting/XGBoost, Survival Analysis, LSTM/Prophet/ARIMA/SARIMA, Demand Forecasting.
 Attribution & Affinity: Markov Chains, Shapley Value; Market Basket Analysis (Apriori); Brand/Category Affinities (time-decayed + Dirichlet smoothing).
 Experimentation: A/B & Multivariate Testing, Uplift Modeling, Power Analysis, PSM, DiD; Incrementality & Sales Lift design.
 Search/Ranking: BM25 + ANN features; RankNet/LTR for PLP/Search.
 Pricing/Revenue: Price Elasticity (log-log), Markdown Optimization (ladders), Bundle/Attach/Cannibalization, Profit-guarded Upsell, Cross-sell complements.

4) Canonical Outlines (deduplicated)
Incrementality & Sales Lift — One Canonical Workflow
Define Metrics & Goals → 2) Plan & Segment → 3) Set Up Control/Test (PSM optional; power calc) →


Execute & Monitor → 5) Collect/Analyze (DiD, uplift) → 6) Compute Incremental KPIs & ROI →


Validate vs Attribution → 8) Report & Decide → 9) Iterate & Optimize.


Subset checklists (kept once): Control-group prep, pre-test windows, attribution window tuning, sizing/power, granular collection, audience-level analysis, exclusions, data integration, tracking quality, ongoing monitoring, privacy/compliance.
Micro → Macro Analytics Layers
Micro 1: Raw logs/events (page views, clicks, UTMs/GCLID/FBCLID, product actions, stock/ship, support, perf, CWV).


Micro 2: Basic descriptives (sessions, users, transactions, gross revenue, inventory, tickets).


Micro 3: Derived performance (conversion %, AOV, cart abandon %, ARPU/ARPPU, response/resolve times).


Micro 4: Behavioral/cohort/efficiency (RFM, cohorts, LTV:CAC, ROAS/CAC, funnels, path/heatmaps, attribution).


Micro 5: Predictive/optimization (churn/demand/CLV, personalization, dynamic pricing, anomaly detection).


Macro 1–4: Departmental KPIs → Cross-department P&L/forecasts → Strategic (growth/valuation/ESG) → Executive (governance/expansion).



5) Technical Signals & Identifiers (deduplicated)
Identifiers: Client ID (anon), User ID (auth), UUID cookies; UTMs; ad click IDs (GCLID/FBCLID).
 HTTP/Client Hints (consolidated): Sec-CH-UA, -Mobile, -Platform, -Platform-Version, -Model, -Arch, -Bitness, -Form-Factors, -Full-Version-List, -WoW64.
 Common Headers for analytics/debug: accept, accept-language, accept-encoding, referer, host, x-forwarded-*, cache-control, cookie, origin, range, pragma, via, etc.
 Browser/Device APIs (high-level list): Navigator (userAgent, languages, platform, hardwareConcurrency, deviceMemory), Network Information (effectiveType, downlink, rtt), Geolocation (consent), Storage (local/session), Beacon, Performance, Media/Pointer/Fullscreen.
 Core Web Vitals: LCP, FID, CLS.
Note: verbose middleware/code examples were collapsed into this single canonical list.

6) SEO / SEM (deduplicated)
SEO: Technical (structure, sitemaps, CWV, mobile, SSL, structured data), On-Page (meta, headings, alt, canonical, hreflang, accessibility, internal links), Local/International/E-com specifics, Content & Keyword strategy.
 SEM: Bidding strategies (CPC/eCPC/Target CPA/ROAS), device/time/location, retargeting & attribution, incrementality tests; Do’s/Don’ts & Checklist kept once (keywords → ads → budget → site speed/LP → UTM → continuous testing).
 UTMs: utm_source, utm_medium, utm_campaign, utm_term, utm_content.

7) SaaS / Finance Library (deduplicated)
Recurring Revenue Stack: MRR/ARR components and waterfalls; Net/Gross dollar churn; NRR; ARPA(-new/installed).
 Unit Economics: LTV (simple & DCF note), CAC, LTV:CAC, Months to Recover CAC, Sales Efficiency (gross/net), Magic Number (GAAP variant).
 Cash & P&L: Billings vs Revenue vs Deferred Revenue, COGS, Gross Margin %, Total Expenses (S&M/R&D/G&A), EBITDA, Change/Ending Cash.
 Funnel & Sales Ops: Visitors→Trials→Purchases; lead/MQL/opportunity conversions; # reps, quota, capacity, coverage, productivity.
 Cohorts & Retention: Cohort size/payment curves, early-month churn emphasis; negative churn via expansion.
 Hiring/Ramp: Single-AE and multi-hire ramp concepts (ramp curve, payback, trough).

8) Compliance, Privacy, Security (deduplicated)
GDPR/CCPA alignment; consent and transparency; first-party data emphasis; secure data handling; access controls/CI-CD; data quality & lineage.


Note: device-fingerprinting techniques exist; apply only within legal/consent frameworks.



9) Canonical Formulas & IDs (SaaS addendum — kept once)
Bookings Formula (quick): Bookings = Top-of-funnel Leads × Conversion Rate × Avg Deal Size.


Customer Lifetime: 1 ÷ churn.


LTV (SaaS simple): (ARPA × GM%) ÷ churn.


Magic Number (GAAP): (Δ GAAP Revenue_q × 4) ÷ Prior-qtr S&M.



10) Tools (deduplicated, high-level)
Analytics/BI/Exp: GA4/BigQuery, Data Studio/Looker/Tableau/Power BI, Optimizely/Hotjar; Ads APIs; CRM (SFDC/Zoho); ETL/ELT (Airbyte/Fivetran/Airflow); Streaming (Kafka).


Identity/Access & Policy: Casbin/OPA (RBAC/ABAC), consent managers.



11) Single Canonical Checklists (kept once)
SEM Checklist: keywords → ad groups/creatives → budget/auction insights → LP speed/relevance → UTM tagging → continuous test/opt.
 Incrementality Readiness: control design & sizing/power, tracking quality, audience & exclusions, window tuning, analysis plan, privacy.

Notes on Deduplication
Multiple repeated outlines for incrementality, cohorts, sales funnels, and SaaS unit economics were merged into the single canonical versions above.


Large code snippets/middleware were collapsed to the Technical Signals & Identifiers section to avoid repetition.


Obvious near-synonyms (e.g., “Exit Pages” / “Exit Rate”) are listed once.



End of Deduplicated Resource v1


Signals → Models Map v1
(scope/keys are always tenant‐scoped via store_id; money in cents; timestamps iso8601; raw from MongoDB; analytics in Postgres/ClickHouse; real-time GraphQL expected for 2,3,7,8,11,12)

1) Customer CLV (BG/NBD + Gamma–Gamma)
Required
order_gross_margin_cents — customer — mongo:orders — keys: store_id, customer_id, order_id — cents — core monetary for CLV.


purchase_count_365d — customer — pg/ch:orders_agg — store_id, customer_id — count — frequency for BG/NBD.


recency_days — customer — pg/ch:orders_agg — store_id, customer_id — days — time since last order.


tenure_days — customer — pg/ch:customers_agg — store_id, customer_id — days — time since first order.


Useful
returns_rate_pct — customer — pg/ch:returns_agg — store_id, customer_id — % — net out refunds.


avg_order_margin_cents — customer — pg/ch:orders_agg — store_id, customer_id — cents — stabilizes Gamma–Gamma.


Nice-to-have
engaged_sessions_90d — customer — pg/ch:sessions_agg — store_id, customer_id — count — survival proxy.


Subset flows
subset_flow_for:1 returns_adjustment → returns_rate_pct, order_gross_margin_cents.


Excluded signals
Heatmaps/session recordings; admin/ops dashboards — not incidence/monetary drivers.



2) Two-Stage Recommender (PDP/PLP)
Required
product_margin_cents — product — mongo:products — store_id, product_id — cents — profit-aware rank.


stock_units — product — mongo:inventory — store_id, product_id — count — block low/zero stock.


product_view_count_30d — product — pg/ch:events_agg — store_id, product_id — count — candidate recall signal.


co_view_cosine — product×product — pg/ch:behavior_graph — store_id, product_id, neighbor_id — score — similarity for retrieval/ranking.


Useful
product_return_rate_pct — product — pg/ch:returns_agg — store_id, product_id — % — penalize high returns.


price_cents — product — mongo:products — store_id, product_id — cents — price normalization.


Nice-to-have
category_click_rate_30d — product — pg/ch:events_agg — store_id, category_id — % — popularity prior.


Subset flows
subset_flow_for:2 low_stock_demoter → stock_units.


subset_flow_for:2 recent_interest_boost → product_view_count_30d.


Excluded signals
GSC index coverage; staff KPIs — not product ranking.



3) Basket / Session Re-rank (Cart)
Required
cart_items_jsonb — session — mongo:carts — store_id, session_id — jsonb — context for add-ons.


attach_lift — product×hero — pg/ch:mba_lift — store_id, product_id, hero_product_id — lift — MBA complement strength.


candidate_margin_cents — product — mongo:products — store_id, product_id — cents — net gain term.


stock_units — product — mongo:inventory — store_id, product_id — count — availability check.


Useful
session_conversion_rate_baseline — session — pg/ch:session_metrics — store_id, session_id — % — CVR guardrail.


product_return_rate_pct — product — pg/ch:returns_agg — store_id, product_id — % — risk penalty.


Nice-to-have
co_buy_ppmi — product×hero — pg/ch:behavior_graph — store_id, ids — score — tie-break similarity.


Subset flows
subset_flow_for:3 checkout_intent_cohort → session_conversion_rate_baseline, cart_items_jsonb.


Excluded signals
Heatmaps; email NPS — not cart rerank inputs.



4) Promotion Uplift (multi-treatment)
Required
campaign_exposure_log — customer — mongo:campaign_logs — store_id, campaign_id, customer_id — jsonb — arms/control history.


offer_cost_cents — store/tenant — mongo:campaigns — store_id, offer_id — cents — budget/caps.


pre_offer_features — customer — pg/ch:customer_snapshot — store_id, customer_id — jsonb — predictors for uplift.


Useful
post_purchase_margin_cents_7d — customer — pg/ch:orders_win — store_id, customer_id — cents — near-term effect.


contact_fatigue_score — customer — pg/ch:mktg_agg — store_id, customer_id — score — suppress over-messaging.


Nice-to-have
channel_last_touch — customer — pg/ch:attribution — store_id, customer_id — string — allocation prior.


Subset flows
subset_flow_for:4 budget_knapsack → offer_cost_cents, predicted uplift.


Excluded signals
Device hints/headers — not causal at assignment.



5) Customer Personas (clusters)
Required
rfm_vector — customer — pg/ch:rfm_agg — store_id, customer_id — jsonb — recency/frequency/monetary basis.


category_share_vector_90d — customer — pg/ch:orders_by_category — store_id, customer_id — jsonb — mix signal.


Useful
engagement_rate_90d — customer — pg/ch:sessions_agg — store_id, customer_id — % — behavior texture.


promo_redeem_rate_180d — customer — pg/ch:promo_agg — store_id, customer_id — % — deal-seeking trait.


Nice-to-have
geo_region — customer — pg/ch:customers_agg — store_id, customer_id — string — messaging nuance.


Subset flows
subset_flow_for:5 persona_enrichment → category_share_vector_90d, engagement_rate_90d.


Excluded signals
Real-time stock; SEO coverage — not needed for static clustering.



6) Affinities (Brand/Category; time-decayed)
Required
decayed_view_counts_30d — customer×brand/category — pg/ch:events_decay — store_id, customer_id, brand_id|category_id — count — recent interest.


decayed_purchase_counts_180d — customer×brand/category — pg/ch:orders_decay — store_id, customer_id, ids — count — stronger signal.


Useful
dirichlet_alpha — store/tenant — pg/ch:affinity_cfg — store_id — float — smoothing for sparse users.


Nice-to-have
price_band_pref — customer — pg/ch:price_bands — store_id, customer_id — band — slotting aid.


Subset flows
subset_flow_for:6 time_decay_ensemble → both decayed counts + smoothing.


Excluded signals
GSC “Links” pages — not taste.



7) Product Similarity / Clustering
Required
co_buy_lift — product×product — pg/ch:mba_lift — store_id, product_id, neighbor_id — lift — complements graph.


co_view_cosine — product×product — pg/ch:behavior_graph — store_id, ids — score — discovery graph.


Useful
attribute_vector — product — mongo:products — store_id, product_id — jsonb — cold-start.


stock_units — product — mongo:inventory — store_id, product_id — count — avoid out-of-stock neighbors.


Nice-to-have
cluster_id — product — pg/ch:product_clusters — store_id, product_id — id — merchandising tools.


Subset flows
subset_flow_for:7 ann_index_weekly → materialize neighbor lists for serving.


Excluded signals
Admin calendars; HR metrics.



8) PLP/Search Ranking (LTR)
Required
bm25_score — session/query×product — pg/ch:search_features — store_id, query_id, product_id — score — lexical relevance.


click_through_rate_7d — query×product — pg/ch:search_logs — store_id, query_id, product_id — % — behavior feature.


stock_units — product — mongo:inventory — store_id, product_id — count — freshness/guardrail.


Useful
product_margin_cents — product — mongo:products — store_id, product_id — cents — profit-aware ranking.


promo_flag — product — mongo:products — store_id, product_id — bool — business rule.


Nice-to-have
price_zscore — product — pg/ch:search_features — store_id, product_id — score — normalize price.


Subset flows
subset_flow_for:8 freshness_guard → stock_units, promo_flag.


Excluded signals
GSC coverage/security actions — external; not site search ranker inputs.



9) Markdown Optimizer
Required
stock_units — product — mongo:inventory — store_id, product_id — count — constraint.


ladder_prices_cents[] — product — mongo:pricing — store_id, product_id — cents — decision set.


demand_quantile_p50/p90 — product×week×price — pg/ch:forecast — store_id, product_id — count — risk-aware sales.


cogs_cents — product — mongo:products — store_id, product_id — cents — GM objective.


Useful
product_age_days — product — mongo:inventory — store_id, product_id — days — urgency prior.


promo_overlap_flag — product×week — pg/ch:calendar — store_id, product_id — bool — avoid confounding.


Nice-to-have
salvage_value_cents — product — pg/ch:pricing — store_id, product_id — cents — terminal value.


Subset flows
subset_flow_for:9 ladder_guardrails → enforce min margin at each step.


Excluded signals
Session metrics; heatmaps — not price planning.



10) Price Elasticity (gated)
Required
weekly_price_cents — product — pg/ch:price_history — store_id, product_id, week — cents — independent var.


weekly_units — product — pg/ch:price_history — store_id, product_id, week — count — dependent var.


promo_flag_week — product×week — mongo:promos — store_id, product_id — bool — control.


Useful
seasonality_woy — store/tenant — pg/ch:calendar — store_id, week_of_year — index — seasonality control.


cogs_cents — product — mongo:products — store_id, product_id — cents — Lerner guardrails.


Nice-to-have
competitor_price_band_cents — product — pg:competitive_intel — store_id, product_id — cents — external anchor.


Subset flows
subset_flow_for:10 eligibility_gate → require ≥5 distinct price points/SKU.


Excluded signals
GSC, device hints — irrelevant to elasticity.



11) Upsell Propensity (profit-guarded)
Required
upsell_show_accept_log — session/product — pg/ch:offers_log — store_id, session_id, product_id — jsonb — acceptance prior.


upsell_margin_cents — product — mongo:upsell_catalog — store_id, product_id — cents — profit term.


base_order_cvr_drop_pct — session — pg/ch:experiments — store_id, session_id — % — guardrail penalty.


Useful
hero_candidate_similarity — product×hero — pg/ch:behavior_graph — store_id, ids — score — relevance.


stock_units — product — mongo:inventory — store_id, product_id — count — ensure availability.


Nice-to-have
recent_category_views_7d — customer — pg/ch:events_agg — store_id, customer_id, category_id — count — short-term interest.


Subset flows
subset_flow_for:11 profit_guard → upsell_margin_cents, base_order_cvr_drop_pct.


Excluded signals
Staff performance; admin boards.



12) Cross-sell (Complements)
Required
co_buy_lift — product×hero — pg/ch:mba_lift — store_id, ids — lift — complement strength.


candidate_margin_cents — product — mongo:products — store_id, product_id — cents — value filter.


stock_units — product — mongo:inventory — store_id, product_id — count — avoid low stock.


Useful
product_return_rate_pct — product — pg/ch:returns_agg — store_id, product_id — % — de-risk picks.


substitute_flag — product×hero — pg/ch:catalog_rules — store_id, ids — bool — exclude substitutes.


Nice-to-have
shipping_compat_flag — product — mongo:products — store_id, product_id — bool — fulfillment friction.


Subset flows
subset_flow_for:12 no_subs_low_stock → substitute_flag, stock_units.


Excluded signals
SEO console; device hints.



13) Bundle Optimizer (slow-mover + hero)
Required
slow_mover_flag — product — pg/ch:inventory_agg — store_id, product_id — bool — candidate gating.


hero_margin_cents — product — mongo:products — store_id, product_id — cents — protect hero GM.


attach_rate_prior — pair — pg/ch:attach_priors — store_id, s_product_id, h_product_id — % — expected take.


cannibalization_prior_pct — pair — pg/ch:attach_priors — store_id, ids — % — net out cannibalization.


stock_units — product — mongo:inventory — store_id, product_id — count — feasibility.


Useful
bundle_discount_cost_cents — pair — pg/ch:discounts — store_id, ids — cents — cost term.


brand_rule_violation_flag — pair — pg/ch:catalog_rules — store_id, ids — bool — constraints.


Nice-to-have
cluster_id — product — pg/ch:product_clusters — store_id, product_id — id — diversity.


Subset flows
subset_flow_for:13 bundle_knapsack → attach_rate_prior, cannibalization_prior_pct, bundle_discount_cost_cents.


Excluded signals
Real-time site search features.



14) Churn Propensity
Required
recency_weeks — customer — pg/ch:orders_agg — store_id, customer_id — weeks — strongest churn driver.


orders_last_90d — customer — pg/ch:orders_agg — store_id, customer_id — count — frequency.


gm_90d_cents — customer — pg/ch:orders_agg — store_id, customer_id — cents — value at risk.


active_days_30d — customer — pg/ch:sessions_agg — store_id, customer_id — count — engagement.


Useful
support_tickets_90d — customer — pg/ch:support — store_id, customer_id — count — dissatisfaction proxy.


email_open_click_30d — customer — pg/ch:messaging — store_id, customer_id — count — responsiveness.


Nice-to-have
return_events_90d — customer — pg/ch:returns_agg — store_id, customer_id — count — negative signal.


Subset flows
subset_flow_for:14 treatment_value_gate → gm_90d_cents vs action cost.


Excluded signals
Price ladders; SEO link data.



15) Lifecycle Timing (HSMM)
Required
weekly_visits — customer×week — pg/ch:events_weekly — store_id, customer_id, week — count — state emissions.


weekly_orders — customer×week — pg/ch:orders_weekly — store_id, customer_id, week — count — transition evidence.


state_duration_priors — store/tenant — pg/ch:lifecycle_cfg — store_id — jsonb — dwell time priors.


Useful
weekly_email_open_clicks — customer×week — pg/ch:messaging_weekly — store_id, customer_id, week — count — auxiliary signal.


visit_trend_slope_4w — customer — pg/ch:events_weekly — store_id, customer_id — slope — momentum.


Nice-to-have
purchase_value_rolling_cents — customer — pg/ch:orders_weekly — store_id, customer_id — cents — weighting.


Subset flows
subset_flow_for:15 posterior_trigger → threshold on p_state_at_risk to fire win-backs.


Excluded signals
Pricing/markdown features; device headers.



Global Excluded Signals (with reasons)
Admin/employee tools, task/OKR boards — operational, not predictive inputs.


Heavy SEO console admin (coverage, security actions) — external to on-site behavior ranking (except query-level content work, which is not in these models).


Low-level HTTP headers/client-hints — privacy-sensitive; weak direct lift for these models.



Notes
Scopes/granularity are encoded per line (customer / visitor / session / device / browser / product / store/tenant).


Sources respect: raw in MongoDB (orders, products, carts, pricing, promos); analytics in Postgres/ClickHouse (aggregates, graphs, forecasts).


Serving: 2/3/7/8/11/12 must be hydrated in near real-time (≤120 s freshness); others batch daily/weekly unless campaign cadence dictates otherwise.



Start
A) One-time pre-flight (used for all 15 models)

1) Data sources (where values come from)
	•	orders: order_id, user_id, items{product_id, qty, price}, discounts, returns, subtotal, total, created_at.
	•	products: product_id, brand_id, category_id, base_price, margin, stock_cover_days, attributes.
	•	events (nav): session_id, user_id?, page_type, product_id, action{view, click, fav, atc, search}, ts, referrer, channel.
	•	customers: user_id, created_at, email perms, last_login.
	•	price_history: sku, week, price, promo_flag.
	•	search_logs: query, impressions, clicks, purchases, features.
	•	campaign_logs: arm, send_ts, open/click, convert.
	•	support (optional): tickets, NPS flags.

Note: map these to app features you mentioned (product_feature, order_feature, customer_feature, etc.).

2) Identity & scope
	•	user (known): user_id + store_id; per-customer outputs (CLV, churn, personas, affinities, lifecycle).
	•	visitor (anon): session_id; cohort features (segment, channel, page). Use fallback hierarchy: user → cohort → tenant default.
	•	tenant split: all jobs partition by store_id (and merchant_id where needed).

3) Metric classes (how values are obtained)
	•	Raw: timestamps, amounts (cents), product/category/brand ids, quantities, channel, referrer.
	•	Derived: AOV, frequency, recency, stock cover, co-buy counts, cosine, time-decay counts.
	•	Extrapolated: fitted params/weights (e.g., RankNet, churn), BG/NBD hypergeometric term, optimization output (markdown plan, bundles).

4) Parameters/coefficients (what’s “known” vs “learned”)
	•	Fixed knobs: horizons H, windows W, floors/caps, guardrails (λ, α, N), tier thresholds.
	•	Tenant-level learned params: CLV (r, α, a, b, p, q, γ), RankNet weights, churn/upsell coefficients, cross-sell β’s.
	•	Update cadence: train weekly (params table), score daily (outputs). Latest per store_id.

5) Flows (where models apply online)
	•	PDP: #7 Similarity, #12 Cross-sell, #2 Recommender.
	•	PLP/Search: #8 Rank, #2 Recommender.
	•	Cart/Checkout: #3 Basket re-rank, #11 Upsell.
	•	Email/Promo: #4 Promotion Uplift (assign offline; serve to orchestration).
	•	Global personalization: #6 Affinities, #5 Personas, #1 CLV, #14 Churn, #15 Lifecycle (usually offline; optional serve).

6) Units, keys, SLOs
	•	Money: cents only. Probabilities: [0,1].
	•	Keys: (id, model_version, as_of, store_id).
	•	Freshness: PG datasets ≤36h; CH edges ≤72h.

⸻

B) Per-model whiteboard template (copy this 15×)

Model # / Name / Goal
Inputs (tables → fields, tenant scope)
Known params (fixed knobs)
Learned params (if any) + cadence
Core expression (plain English, then symbol if needed)
Decision rule / Score
Guardrails (caps, λ, stock, excludes)
Outputs (dataset → columns): { primary_id(s), fields…, model_version, as_of, store_id }
Serving (Y/N; query name if Y)
Cron: Fit (Y/N, weekly), Score (daily)
Unit checks: cents/probs/ranges
Notes: visitor fallback (cohort) if user unknown

⸻

C) CSV mock sheets you should draft (for whiteboard demos)
	•	orders.csv: order_id, user_id?, product_id, qty, price_cents, discount_cents, returned, created_at, store_id.
	•	events.csv: session_id, user_id?, page_type, product_id, action, ts, channel, store_id.
	•	products.csv: product_id, brand_id, category_id, base_price_cents, margin_cents, stock_cover_days.
	•	search_logs.csv: query, product_id, clicked (0/1), features…, store_id.
	•	price_history.csv: sku, week, price_cents, promo_flag, store_id.

⸻

D) Gaps you should add (so we don’t get blocked later)
	•	Visitor cohorts: simple session → cohort builder (channel, device, referrer, page mix) → visitor_cohorts table.
	•	Customer signals (optional richer nav): per-customer aggregates (views_by_brand/category, click_depth, email_engaged) → customer_signals.
	•	Params tables: <model>_params per tenant for any model with learned coefficients.

⸻

E) Quick mapping: which models need richer nav/context
	•	Needs nav/session: #2, #3, #7, #8, #11, #12.
	•	Benefits from nav: #5 Personas, #6 Affinities, #14 Churn, #15 Lifecycle.
	•	Mostly transactional: #1 CLV, #9 Markdown, #10 Elasticity, #13 Bundle, #4 Uplift (with campaign logs).

⸻
End


Start
1) Visitor Cohorts (anon/session flows)

Purpose: real-time fallback when user_id is unknown; cohort-level targeting.

Add
	•	Job: jobs/customer/visitor_cohorts/{cmd/main.go, pipeline/visitor_cohorts_pipeline.go, visitor_cohorts_validate.go}
	•	Table (PG): visitor_cohorts
	•	store_id, cohort_id (hash of {channel, device, referrer, page_mix})
	•	channel, device, referrer_domain
	•	page_mix (top categories/brands in last N pages)
	•	avg_depth, bounce_rate, atc_rate, purchase_rate
	•	size, last_seen_at, as_of, model_version
	•	Inputs: events (views/clicks/ATC/checkout), products (brand/category)
	•	Cadence: daily (24h); Serving: optional GraphQL visitorCohort(cohortId)
	•	Used by: #2, #3, #7, #8, #11, #12 (runtime fallback); improves #5, #6, #14, #15 features.

2) Customer Signals (logged-in flows)

Purpose: per-user aggregates for persona/affinities/churn/lifecycle; light runtime rules.

Add
	•	Job: jobs/customer/signals/{cmd/main.go, pipeline/customer_signals_pipeline.go, customer_signals_validate.go}
	•	Table (PG): customer_signals
	•	store_id, user_id
	•	views_by_brand (jsonb top-K), views_by_category (jsonb)
	•	last_brand, last_category, last_channel
	•	email_open_rate_90d, email_click_rate_90d, email_engaged (bool)
	•	session_count_30d, atc_count_30d, purchase_count_90d
	•	avg_view_to_atc_sec, avg_atc_to_buy_sec
	•	as_of, model_version
	•	Inputs: events, orders, email/campaign logs (if available), products
	•	Cadence: daily; Serving: optional GraphQL customerSignals(userId)
	•	Used by: #5 Personas, #6 Affinities, #14 Churn, #15 Lifecycle; as features or joins. Also light rules in #3/#11/#12.

3) Parameter Tables (learned coefficients per tenant)

Pattern: train weekly → write <model>_params (PG) → scoring jobs read latest per store_id.

Add (as needed)
	•	#1 CLV: clv_params(store_id, r, alpha, a, b, p, q, gamma, as_of, model_version); job clv_fit.
	•	#8 RankNet: rank_weights(store_id, feature, weight, intercept, as_of, model_version); job ranknet_train.
	•	#12 Cross-sell: cross_sell_params(store_id, beta0..3, alpha_lsp, m0, N, as_of, model_version); job cross_sell_train (if learning).
	•	#11 Upsell: upsell_params(store_id, lambda, basegm_prior, as_of, model_version); (or learned GBM/logit coef_*).
	•	#14 Churn: churn_params(store_id, coef_*, as_of, model_version); job churn_train.
	•	#15 Lifecycle: hsmm_params(store_id, duration_means, emission_rates, as_of, model_version); job lifecycle_fit.
	•	#2 Recommender (ranker): reuse rank_weights or reco_params(store_id, coef_*, caps, as_of, model_version).
	•	#10 Elasticity: elasticity_params(store_id, priors, guardrails, as_of, model_version) (optional).
	•	#4 Uplift: uplift_params(store_id, arm_costs, caps, as_of, model_version) (if learned τ model).
	•	#9 Markdown / #13 Bundle: store policy knobs if tuned: markdown_params, bundle_params.

Cadence: train weekly; score daily.

4) DB placement (where to store & read)
	•	ClickHouse (CH): wide logs and graph edges: events, affinity_edges, heavy scans, similarity builds.
	•	Postgres (PG): serving & params: customer_segments, product_signals, tiers_clv, cross_sell, upsell_scores, bundles, markdown_plans, elasticity, personas, churn_risk, lifecycle_state, visitor_cohorts, customer_signals, all <model>_params, and rank_weights.

5) Model ↔ signals/params mapping (quick)
	•	Needs visitor cohorts runtime: #2, #3, #7, #8, #11, #12
	•	Uses customer signals features: #5, #6, #14, #15 (also boosts #3/#11/#12 rules)
	•	Has params table (learned): #1, #8, #12, #11, #14, #15 (and optionally #2, #4, #10, #9, #13 for policy)
	•	Batch-only math: #1, #4, #5, #6, #9, #10, #13, #14, #15
	•	Online serve: #2, #3, #7, #8, #11, #12 (GraphQL)

6) “Done when” checklist (so it’s automatic, per tenant)
	•	Jobs exist: visitor_cohorts (daily), customer_signals (daily), each <model>_train (weekly), each <model> scoring (daily).
	•	Tables exist (PG/CH) as above.
	•	Scoring jobs read latest <model>_params by store_id.
	•	Signals exposes only datasets needed at runtime; resolvers apply tenant/user/session context.
	•	SLOs: freshness 36h (PG) / 72h (CH) met; opsStatus green.
End


Short answer: yes—everything you wrote is implementable without LLMs, and you can stand the whole system up and run it end-to-end with Infrastructure as Code (IaC). IaC won’t “do the math,” but it provisions, wires, deploys, schedules, and monitors all the services that run your (classic stats/ML) code.

Here’s the practical picture so you’re 100% sure:

What IaC handles
	•	Data & storage: buckets/DBs/warehouse tables; streams (Kafka/Kinesis/Pub/Sub); schemas & retention.
	•	Batch compute: scheduled jobs for training/scoring (Airflow/Composer/Step Functions/Prefect), Spark/Dataproc/EMR, or containerized Python jobs.
	•	Online serving: APIs behind a gateway+LB, autoscaling (K8s/EKS/GKE/AKS or Cloud Run/Lambda), feature cache (Redis), and ANN services (OpenSearch k-NN, FAISS/Milvus).
	•	Model assets: container registries, artifacts, versioned config.
	•	Ops: dashboards/alerts, logs/metrics, secrets, IAM.

What your app code handles (no LLM needed)
	•	The algorithms themselves: BG/NBD + Gamma-Gamma, RankNet, GBM/logit, HDBSCAN, Hawkes, markdown DP/MILP, cosine/PPMI, etc. (all standard libraries).
	•	A few special functions (e.g., hypergeometric in BG/NBD) via numeric libs—again, normal code.

Quick mapping (how you’ll deploy each with IaC)
	•	Batch models: #1, #4, #5, #6, #7 (clustering build), #9, #10, #13, #14, #15 (micro-batch)
Provision scheduled containers + warehouse tables + output sinks (feature tables, tiers, bundles, schedules).
	•	Online/near-real-time models: #2, #3, #8, #11, #12, #7 (ANN serving)
Provision API services + Redis feature cache + vector index (for #2/#7/#12) + health checks and autoscaling.

Typical stacks that fit your spec
	•	Terraform or Pulumi + Docker + Kubernetes (or serverless) for services;
	•	Airflow/Composer/Step Functions/Prefect for batch DAGs;
	•	OpenSearch k-NN / FAISS / Milvus for ANN;
	•	PostgreSQL/BigQuery/Redshift/Snowflake for aggregates;
	•	Prometheus/Grafana or CloudWatch/Stackdriver for SLOs;
	•	Feast (optional) for a simple feature store.

Bottom line: your assumption is sound. These are all conventional statistical/ML components that IaC can deploy and operate reliably—no generative AI or LLMs required. If you want, I can sketch a minimal Terraform module layout for one online model (say #2) and one batch model (say #1) so your team has a concrete starting blueprint.


Legend
	•	[PROVISION SYSTEM] provision external infra (outside repo)
	•	[ADD NEW FOLDER] create folder (lists required files inside)
	•	[ADD NEW FILE] create file
	•	[EDIT EXISTING FILE] modify code inside an existing file

⸻

0) System Packages to Provision (must have)
	•	[PROVISION SYSTEM] Container Registry — store versioned images for all analytics/jobs/** and analytics/signals.
	•	[PROVISION SYSTEM] Kubernetes Cluster — run CronJobs (batch jobs) and Deployment/Service (serving).
	•	[PROVISION SYSTEM] Secrets Manager / K8s Secrets — hold DSNs and credentials (CORE_PG_DSN, CORE_CH_DSN, SIGNALS_PG_DSN, SIGNALS_CH_DSN).
	•	[PROVISION SYSTEM] Postgres — inputs + serving outputs (customer_segments, product_signals, and new datasets below).
	•	[PROVISION SYSTEM] ClickHouse — wide scans; stores affinity_edges.
	•	[PROVISION SYSTEM] Observability (Prometheus + Grafana or cloud metrics) — scrape metrics (jobs + signals), alert on SLO breaches.
	•	[PROVISION SYSTEM] CI/CD Runners (GitHub/GitLab) — build/test, docker build/push, apply K8s manifests.
	•	[PROVISION SYSTEM] ANN Engine (OpenSearch k-NN or Milvus or FAISS-serving) — required for #7/#2/#12 if you want ANN lookup ≤50 ms at serving.
	•	[PROVISION SYSTEM] Optimizer Runtime (OR-Tools or CBC/GLPK) — required for #4 knapsack and #9 markdown DP/MILP batch jobs.

⸻

1) Repo Root: analytics/ (what to keep vs change)

1.1 analytics/common/
	•	[EDIT EXISTING FILE] analytics/common/config/load.go
Purpose: config loader for both signals.yaml and per-model YAMLs.
Add inside this file:
	•	LoadSignalsConfig(path): read YAML → apply env overrides for dsn_env fields → validate required keys → return SignalsConfig.
	•	LoadJobConfig(path): same pattern for per-model YAMLs → validate required keys and job_slo.
	•	applyEnvOverrides(cfg): for each dsn_env, set the actual DSN value from os.Getenv.
	•	validateSignals(cfg): require serving.<dataset>.store, serving.<dataset>.dsn_env, slo.freshness_hours, slo.min_rows, slo.timeouts_ms, slo.paging.max_limit.
	•	validateJob(cfg): require inputs[].store, inputs[].dsn_env, output.table, output.store, output.dsn_env, window, model_version, job_slo.{max_runtime_sec,max_retries,backoff_sec}.
	•	Fail fast (return error) on any missing key or empty env.
	•	[EDIT EXISTING FILE] analytics/common/datastore/postgres.go
Purpose: read inputs, upsert outputs in Postgres.
Add inside this file:
	•	All queries use context.WithTimeout.
	•	Helpers: SelectFreshness(table), SelectCount(table), UpsertRows(table, rows, keys=[id,model_version,as_of]).
	•	New readers you’ll need: returns, price history, search logs (for CLV/Elasticity/PLP).
	•	Logging with rows affected; bubble up errors with context.
	•	[EDIT EXISTING FILE] analytics/common/datastore/clickhouse.go
Purpose: wide scans + affinity writes.
Add inside this file: same timeouts + SelectFreshness/SelectCount helpers; efficient inserts; error wrapping.
	•	[EDIT EXISTING FILE] analytics/common/timewin/window.go
Purpose: rolling window helpers.
Add inside this file: presets last_30d, last_90d, last_180d, last_365d + Custom(start,end).

⸻

1.2 analytics/configs/
	•	[EDIT EXISTING FILE] analytics/configs/signals.yaml
Purpose: bind datasets to stores + SLOs for serving.
Add inside this file:
	•	serving for every dataset you’ll serve (see §3 datasets list).
	•	slo: freshness_hours, min_rows, timeouts_ms, paging.max_limit.
	•	Confirm DSN env names match your Secrets.
	•	[ADD NEW FILE] analytics/configs/<model>.yaml (one per model you implement)
Purpose: per-job IO and bounds.
Contents template:
	•	inputs: array of sources with {store: postgres|clickhouse, dsn_env: CORE_*} and any table/filters.
	•	window: one of last_30d/90d/180d/365d or custom.
	•	output: {table: <dataset_name>, store: postgres|clickhouse, dsn_env: SIGNALS_*}
	•	model_version: semantic tag (e.g., clv-1.0.0).
	•	job_slo: {max_runtime_sec, max_retries, backoff_sec}.

⸻

1.3 analytics/jobs/ (batch compute)

Global job pattern (applies to all jobs below):
	•	[ADD NEW FILE] cmd/main.go
	•	Load configs/<model>.yaml via LoadJobConfig.
	•	Create context with job_slo.max_runtime_sec.
	•	Read inputs using common/datastore/* by store setting.
	•	Run pipeline/<model>_pipeline.go.
	•	Run pipeline/<model>_validate.go (required IDs, dedupe by (id,model_version,as_of), numeric bounds).
	•	Upsert outputs using Postgres helper (even if inputs from CH).
	•	Retry transient failures (max_retries, backoff_sec).
	•	Log: rows written, duration, window, model_version; exit non-zero on failure.
	•	[ADD NEW FILE] pipeline/<model>_pipeline.go
	•	Implement the model’s computation (pure code; no network except DB reads).
	•	Keep money in cents; keep probabilities [0,1].
	•	Return well-typed rows matching the schema.
	•	[ADD NEW FILE] pipeline/<model>_validate.go
	•	Enforce required fields, non-negative numbers, dedupe keys.
	•	Enforce sensible bounds (e.g., cover days ≥ 0).
	•	[ADD NEW FILE] Dockerfile (in each job folder)
	•	Build static Go binary; copy configs mount path; set non-root user.
	•	[ADD NEW FILE] deploy/k8s/jobs/<model>-cronjob.yaml (manifests folder; see §2.2)
	•	Reference image tag; schedule; env (DSNs); mount configs.
	•	Set activeDeadlineSeconds, backoffLimit, resource requests/limits.

Jobs to add (one folder each) aligned to your 15 models
(These are anticipated additions—they are part of the plan, not unexpected gaps.)

	•	[ADD NEW FOLDER] analytics/jobs/customer/clv_bgnbd/ — CLV scoring (BG/NBD + Gamma–Gamma) → outputs tiers_clv.
	•	[ADD NEW FOLDER] analytics/jobs/search/ranknet_train/ — learn rank weights from logs → outputs rank_weights.
	•	[ADD NEW FOLDER] analytics/jobs/campaign/promotion_uplift/ — incremental GM + assignment → outputs promotion_assignments.
	•	[ADD NEW FOLDER] analytics/jobs/cart/upsell_propensity/ — upsell decision scores → outputs upsell_scores.
	•	[ADD NEW FOLDER] analytics/jobs/pdp/cross_sell/ — complements lists → outputs cross_sell.
	•	[ADD NEW FOLDER] analytics/jobs/assortment/bundle_optimizer/ — bundle picks → outputs bundles.
	•	[ADD NEW FOLDER] analytics/jobs/customer/personas_hdbscan/ — persona labels → outputs personas.
	•	[ADD NEW FOLDER] analytics/jobs/pricing/markdown_optimizer/ — markdown ladder → outputs markdown_plans.
	•	[ADD NEW FOLDER] analytics/jobs/pricing/elasticity/ — price elasticity → outputs elasticity.
	•	[ADD NEW FOLDER] analytics/jobs/customer/churn_propensity/ — churn risk → outputs churn_risk.
	•	[ADD NEW FOLDER] analytics/jobs/customer/lifecycle_hsmm/ — lifecycle state → outputs lifecycle_state.
	•	[ADD NEW FOLDER] analytics/jobs/product/similarity_index/ — build/load ANN index artifacts (vectors or normalized co-buy); publish similarity_artifacts.
	•	(You already have) analytics/jobs/customer/rfm/, analytics/jobs/product/velocity/, analytics/jobs/product/abcxyz/, analytics/jobs/product/affinity/ — keep and align to the same pattern.

⸻

1.4 analytics/schemas/models/ (output contracts)

Add a schema JSON for every dataset you will output or serve:
	•	[ADD NEW FILE] tiers_clv.schema.json — { user_id, clv_180_cents, tier, model_version, as_of }.
	•	[ADD NEW FILE] rank_weights.schema.json — { feature, weight, intercept?, model_version, as_of }.
	•	[ADD NEW FILE] promotion_assignments.schema.json — { user_id, arm, delta_gm_cents, ec_cents, model_version, as_of }.
	•	[ADD NEW FILE] upsell_scores.schema.json — { hero_product_id, candidate_id, net_score_cents, model_version, as_of }.
	•	[ADD NEW FILE] cross_sell.schema.json — { hero_product_id, complements:[{id,score}], model_version, as_of }.
	•	[ADD NEW FILE] bundles.schema.json — { slow_id, hero_id, score_cents, constraints, model_version, as_of }.
	•	[ADD NEW FILE] markdown_plans.schema.json — { sku_id, weeks:[{price,margin,qty}], total_gm_cents, model_version, as_of }.
	•	[ADD NEW FILE] elasticity.schema.json — { sku_id, elasticity, conf_low?, conf_high?, model_version, as_of }.
	•	[ADD NEW FILE] personas.schema.json — { user_id, persona_id, stability, model_version, as_of }.
	•	[ADD NEW FILE] churn_risk.schema.json — { user_id, p_churn, decision_value_cents, model_version, as_of }.
	•	[ADD NEW FILE] lifecycle_state.schema.json — { user_id, state, time_in_state, state_probs?, model_version, as_of }.
	•	[ADD NEW FILE] similarity_artifacts.schema.json — { index_id, version, path, built_at } (metadata row if you track artifacts in DB).
	•	(Already exist and remain) customer_segments.schema.json, product_signals.schema.json, affinity_edges.schema.json.

⸻

1.5 analytics/signals/ (serving: gRPC + GraphQL)
	•	[EDIT EXISTING FILE] signals/cmd/main.go
	•	Load signals.yaml with LoadSignalsConfig.
	•	Set default per-request timeout from slo.timeouts_ms.request_default.
	•	Expose GET /healthz (process + store pings OK) and GET /readyz (boot checks + freshness within SLO).
	•	Log one boot line: dataset→store binding and freshness snapshot.
	•	[EDIT EXISTING FILE] signals/internal/store/factory.go
	•	On boot: verify each serving.<dataset> has a matching store impl (*_store_pg.go or *_store_ch.go).
	•	Ping DSNs with timeout; probe SelectFreshness and SelectCount per dataset; fail fast if any breach.
	•	Hold bindings for later use by services and ops resolver.
	•	[EDIT EXISTING FILE] signals/internal/services/*_signals_service.go
	•	Enforce paging from slo.paging.max_limit.
	•	Emit metrics: signals_dataset_ok{dataset}, signals_dataset_rows{dataset}, signals_dataset_max_age_hours{dataset}.
	•	[EDIT EXISTING FILE] signals/internal/validators/*_validator.go
	•	Required IDs present; numeric clamps; paging limits.
	•	[ADD NEW FILE] signals/internal/graph/schema/ops.graphqls
	•	type OpsDatasetStatus { name: String!, store: String!, ok: Boolean!, rows: Int!, maxAgeHours: Int!, lastAsOf: Time, message: String }
	•	type OpsStatus { datasets: [OpsDatasetStatus!]!, startedAt: Time!, version: String! }
	•	extend type Query { opsStatus: OpsStatus! }
	•	[ADD NEW FILE] signals/internal/graph/schema/ops.resolver.go
	•	Build status from factory bindings; run fast COUNT + MAX(as_of); compare to SLO; return statuses.

Serving new datasets (add these only for datasets you plan to query via signals):
	•	For each dataset below, add proto + service + validator + GraphQL schema/resolver if you need it served:
	•	tiers_clv → [ADD NEW FILE] signals/proto/tiers_clv.proto • [ADD NEW FILE] signals/internal/services/tiers_clv_service.go • [ADD NEW FILE] signals/internal/graph/schema/tiers_clv.graphqls • [ADD NEW FILE] tiers_clv.resolver.go • validator as needed.
	•	rank_weights → add proto/service if you want to fetch weights online.
	•	promotion_assignments, upsell_scores, cross_sell, bundles, markdown_plans, elasticity, personas, churn_risk, lifecycle_state, similarity_artifacts → same pattern as needed.

⸻

2) Containerization & Deployment

2.1 Dockerfiles
	•	[ADD NEW FILE] analytics/signals/Dockerfile — build service; expose GraphQL/gRPC ports; run as non-root.
	•	[ADD NEW FILE] analytics/jobs/<domain>/<model>/Dockerfile — build static binary; set entrypoint to cmd/main.

2.2 K8s Manifests
	•	[ADD NEW FILE] deploy/k8s/services/signals-deployment.yaml — Deployment with liveness/readiness, env DSNs, resource limits.
	•	[ADD NEW FILE] deploy/k8s/services/signals-service.yaml — Service/Ingress for /graphql and gRPC port.
	•	[ADD NEW FILE] deploy/k8s/secrets/core-dsns.yaml — K8s Secret holding CORE_PG_DSN, CORE_CH_DSN.
	•	[ADD NEW FILE] deploy/k8s/secrets/signals-dsns.yaml — Secret holding SIGNALS_PG_DSN, SIGNALS_CH_DSN.
	•	[ADD NEW FILE] deploy/k8s/jobs/<model>-cronjob.yaml — one per job: image, schedule, env, config mount, deadlines, retries.

2.3 CI/CD & Tooling
	•	[ADD NEW FILE] .github/workflows/ci.yml — go build/test; docker build; push to registry (tag = model_version).
	•	[ADD NEW FILE] .github/workflows/deploy.yml — kubectl/Helm/Terraform apply for manifests.
	•	[ADD NEW FILE] Makefile — local build, docker, push, deploy.
	•	[ADD NEW FILE] docs/registry-policy.md — image repos, tag format, promotion rules.
	•	[ADD NEW FILE] env/.env.example — DSN var names expected by jobs/services.

2.4 Observability
	•	[ADD NEW FILE] deploy/observability/prometheus-scrape.yaml — scrape jobs and signals.
	•	[ADD NEW FILE] dashboards/analytics-ops.json — Grafana board (dataset_ok, rows, maxAgeHours, job rows, runtime).
	•	[EDIT EXISTING FILE] jobs & signals code — instrument counters/gauges per safety.yaml (e.g., job_rows_written{job}, job_runtime_seconds{job}).

⸻

3) Datasets you will output/serve (map from models → tables)
	•	#1 CLV → tiers_clv (Postgres): user_id, clv_180_cents, tier, model_version, as_of.
	•	#2/#8 Ranking → rank_weights (Postgres): learned feature weights; used by serving or batch scoring.
	•	#3 Cart Re-rank → upsell_scores (Postgres).
	•	#4 Promo Uplift → promotion_assignments (Postgres).
	•	#5 Personas → personas (Postgres).
	•	#6 Affinities → enrich existing; may also write product_signals.
	•	#7 Similarity → similarity_artifacts (meta); neighbors re-served from ANN engine or from DB.
	•	#9 Markdown → markdown_plans (Postgres).
	•	#10 Elasticity → elasticity (Postgres).
	•	#11 Upsell → upsell_scores (already listed).
	•	#12 Cross-sell → cross_sell (Postgres).
	•	#13 Bundles → bundles (Postgres).
	•	#14 Churn → churn_risk (Postgres).
	•	#15 Lifecycle → lifecycle_state (Postgres).

⸻

4) Periodic Parameter Adjustment (where “training/fit” jobs go)

(No LLMs. These are plain batch jobs that compute parameters from logs.)
	•	RankNet weights (#8):
	•	[ADD NEW FOLDER] analytics/jobs/search/ranknet_train/ (see §1.3 pattern).
	•	[ADD NEW FILE] analytics/configs/ranknet_train.yaml — inputs=search logs; output=rank_weights.
	•	[ADD NEW FILE] deploy/k8s/jobs/ranknet-train-cronjob.yaml — nightly or weekly.
	•	CLV hyperparams (#1):
	•	Either embed fitting into clv_bgnbd job once per window, or
	•	[ADD NEW FOLDER] analytics/jobs/customer/clv_fit/ — fit r, α, a, b, p, q, γ; output small clv_params table; scoring job reads them.
	•	Add YAML + CronJob as above.
	•	Promotion uplift model (#4), churn model (#14), upsell model (#11):
	•	If you use learned models (GBM/logit), create corresponding *_train jobs to produce coefficients; otherwise keep rule-based and skip.
	•	Similarity index (#7):
	•	[ADD NEW FOLDER] analytics/jobs/product/similarity_index/ — builds ANN index files and loads into the ANN engine; records metadata row in similarity_artifacts.

⸻

5) Security / Access (brief)
	•	[PROVISION SYSTEM] Secrets store DSNs;
	•	[ADD NEW FILE] deploy/k8s/rbac/serviceaccounts.yaml (if needed) — least privilege;
	•	DB roles: read-only for signals; read+write for jobs.

⸻

6) Runbooks (brief)
	•	Boot fails: read signals logs; fix signals.yaml mapping or DSNs; re-deploy.
	•	Stale dataset: check opsStatus; re-run the job; confirm rows and as_of.
	•	Backfill: run CronJob with override window and model_version bump.

⸻

Final Notes
	•	All “training/fit” mentions are plain batch code deployed like any other job (CronJob). No LLMs.
	•	Whenever I write [EDIT EXISTING FILE], it means put the new logic inside that file (not a new file).
	•	Whenever I write [ADD NEW FILE]/[ADD NEW FOLDER], it means create it at that exact path.








Minimal file set per model (place these)
	•	configs/<model>.yaml — per-job IO, window, model_version, job_slo.
	•	schemas/models/<dataset>.schema.json — output row contract (includes id, model_version, as_of).
	•	jobs/<domain>/<model>/
	•	cmd/main.go — load config → run pipeline → validate → upsert.
	•	pipeline/<model>_pipeline.go — core computation.
	•	pipeline/<model>_validate.go — required keys, dedupe (id,model_version,as_of).
	•	Dockerfile
	•	deploy/k8s/jobs/<model>-cronjob.yaml — schedule, image, env (DSNs).
	•	(Serve it via signals?) add:
	•	signals/proto/<dataset>.proto
	•	signals/internal/services/<dataset>_service.go
	•	signals/internal/graph/schema/<dataset>.graphqls
	•	signals/internal/graph/schema/<dataset>.resolver.go
	•	Edit configs/signals.yaml to bind <dataset> → store/DSN.

⸻

Example A — #1 CLV (BG/NBD + Gamma–Gamma)

Dataset: tiers_clv
	•	configs/clv_bgnbd.yaml
	•	schemas/models/tiers_clv.schema.json
	•	jobs/customer/clv_bgnbd/
	•	cmd/main.go
	•	pipeline/clv_bgnbd_pipeline.go
	•	pipeline/clv_bgnbd_validate.go
	•	Dockerfile
	•	deploy/k8s/jobs/clv-bgnbd-cronjob.yaml
	•	(Serve via signals)
	•	signals/proto/tiers_clv.proto
	•	signals/internal/services/tiers_clv_service.go
	•	signals/internal/graph/schema/tiers_clv.graphqls
	•	signals/internal/graph/schema/tiers_clv.resolver.go
	•	Edit configs/signals.yaml → add serving.tiers_clv.

⸻

Example B — #12 Cross-sell (complements for hero item)

Dataset: cross_sell
	•	configs/cross_sell.yaml
	•	schemas/models/cross_sell.schema.json
	•	jobs/pdp/cross_sell/
	•	cmd/main.go
	•	pipeline/cross_sell_pipeline.go
	•	pipeline/cross_sell_validate.go
	•	Dockerfile
	•	deploy/k8s/jobs/cross-sell-cronjob.yaml
	•	(Serve via signals)
	•	signals/proto/cross_sell.proto
	•	signals/internal/services/cross_sell_service.go
	•	signals/internal/graph/schema/cross_sell.graphqls
	•	signals/internal/graph/schema/cross_sell.resolver.go
	•	Edit configs/signals.yaml → add serving.cross_sell.

⸻

Notes (same for any two you pick)
	•	If a model also needs periodic parameter fitting (e.g., RankNet weights), add a sibling jobs/<domain>/<model>_train/ folder + its config and CronJob; the serving/scoring job reads those params.
	•	Don’t forget DSN secrets in K8s and image tags in your CronJobs.





Yes—I’ll map exactly where each piece lives in your layout, using your patterns. I’ll show the math file, the storage file, and the parameter-update file for two concrete models (CLV and Cross-sell), then a 3-step pattern you can reuse for any of the 15.

⸻

A) Example: #1 CLV (BG/NBD + Gamma–Gamma)

Math (from your PDFs) — where it goes
	•	jobs/customer/clv_bgnbd/pipeline/clv_bgnbd_pipeline.go
	•	Implements BG/NBD expected transactions and Gamma–Gamma monetary; multiplies to CLV for horizon H.
	•	Reads inputs via common/datastore/* per configs/clv_bgnbd.yaml.

Orchestrate + validate + write
	•	jobs/customer/clv_bgnbd/cmd/main.go
	•	Loads configs/clv_bgnbd.yaml → runs pipeline → calls validate → upserts rows.
	•	jobs/customer/clv_bgnbd/pipeline/clv_bgnbd_validate.go
	•	Ensures {user_id, clv_180_cents, tier, model_version, as_of} present; dedup on (user_id, model_version, as_of).
	•	schemas/models/tiers_clv.schema.json
	•	Contract for the output row (CLV + tier).
	•	Writes via common/datastore/postgres.go::UpsertRows(...) into tiers_clv.

Serving (no math here)
	•	signals/internal/services/tiers_clv_service.go + GraphQL/proto (if you expose CLV)
	•	Reads tiers_clv and returns rows; request validation in signals/internal/validators/*.

Parameters & periodic adjustment
	•	If fixed hyperparameters (r, α, a, b, p, q, γ): store them in configs/clv_bgnbd.yaml; bump model_version when changed.
	•	If fitted:
	•	Training/fit job: jobs/customer/clv_fit/
	•	pipeline/clv_fit_pipeline.go: MLE fit of BG/NBD + GG params from the last W days.
	•	Output table: clv_params with schemas/models/clv_params.schema.json.
	•	Config: configs/clv_fit.yaml; Cron: deploy/k8s/jobs/clv-fit-cronjob.yaml.
	•	Scoring job (clv_bgnbd) reads the latest clv_params before computing CLV.
	•	You tweak cadence (weekly/monthly) by editing the CronJob; you can also adjust tier thresholds in clv_bgnbd.yaml.

⸻

B) Example: #12 Cross-sell (complements for hero item)

Math (from your PDFs) — where it goes
	•	jobs/pdp/cross_sell/pipeline/cross_sell_pipeline.go
	•	Computes latent score z from features (cosine, lift, substitute flag), converts to attach probability, applies low-stock penalty, outputs Net; selects top-N per hero.

Orchestrate + validate + write
	•	jobs/pdp/cross_sell/cmd/main.go → load configs/cross_sell.yaml → run pipeline → validate → upsert.
	•	jobs/pdp/cross_sell/pipeline/cross_sell_validate.go → required keys {hero_id, complement_id, net_cents, model_version, as_of}; dedupe.
	•	schemas/models/cross_sell.schema.json → row contract.
	•	Writes via common/datastore/postgres.go::UpsertRows(...) into cross_sell.

Serving (no math here)
	•	signals/internal/services/cross_sell_service.go (+ GraphQL/proto if exposed) → reads cross_sell.

Parameters & periodic adjustment
	•	If rule-based: keep coefficients (e.g., beta0, beta1, beta2, beta3, alpha, m0, N) in configs/cross_sell.yaml; bump model_version when edited.
	•	If data-fit:
	•	Training job: jobs/pdp/cross_sell_train/
	•	pipeline/cross_sell_train_pipeline.go: fit coefficients from recent attach outcomes; write cross_sell_params (with schemas/models/cross_sell_params.schema.json).
	•	Config: configs/cross_sell_train.yaml; Cron: deploy/k8s/jobs/cross-sell-train-cronjob.yaml.
	•	Scoring job (cross_sell) loads latest cross_sell_params at start and scores with them.

⸻

Reusable pattern (for any model)
	1.	Math/Scoring code (PDF → code)
	•	jobs/<domain>/<model>/pipeline/<model>_pipeline.go — pure compute.
	•	jobs/<domain>/<model>/pipeline/<model>_validate.go — row checks, dedupe, bounds.
	•	jobs/<domain>/<model>/cmd/main.go — load config → pipeline → validate → upsert.
	•	Output schema in schemas/models/<dataset>.schema.json; rows written via common/datastore/postgres.go.
	2.	Serving (if needed)
	•	signals/internal/services/<dataset>_service.go (+ proto + GraphQL schema/resolver).
	•	No math; just read rows, validate request, return.
	3.	Parameter updates (if you adjust from results)
	•	Config-only tweaks: edit configs/<model>.yaml (knobs), bump model_version, rerun job.
	•	Data-driven fit: add a sibling train job jobs/<domain>/<model>_train/ that computes new params from logs, writes a <model>_params table (with schema). Your scoring job reads those params at runtime.
	•	Wire Cron in deploy/k8s/jobs/<model>-train-cronjob.yaml to control cadence.

This keeps your anemic models clean (structs only), math isolated to pipeline files, persistence centralized via datastore helpers, serving immutable in signals, and parameter adjustment either config-based or via dedicated train jobs.



Short answer: Yes—your layout supports automatic, per-tenant parameter updates.
It becomes automatic once you add the “train” jobs + Cron and read the latest params per tenant in scoring.

How to wire it (per model)
	1.	Train job (per-tenant fit)
	•	Job: jobs/<domain>/<model>_train/…
	•	Input: tenant-scoped data (use store_id/merchant_id in queries).
	•	Output table: <model>_params with columns like
store_id, model_version, as_of, param_blob/json (or columns), quality_metrics.
	•	Schedule: Cron in deploy/k8s/jobs/<model>-train-cronjob.yaml (e.g., nightly).
	2.	Scoring job (uses latest params)
	•	Job: jobs/<domain>/<model>/…
	•	At start: read latest params for the tenant
WHERE store_id = :store AND as_of = (SELECT MAX(as_of)… ).
	•	Compute: generate outputs (e.g., CLV, cross-sell) using those params.
	•	Write: tenant-scoped rows with model_version, as_of.
	3.	Serving (signals)
	•	Resolvers/services pass the tenant context (store_id) → read outputs (and, if needed, show param version/freshness).
	•	No heavy math; just fetch the tenant’s latest rows.

Multi-tenancy details (what makes it “automatic” per tenant)
	•	Partition everything by store_id (and merchant_id if used).
	•	Train job loops all tenants (one process, many partitions) or sharded per tenant—either is fine.
	•	Freshness SLOs in configs/signals.yaml catch stale tenants; Ops query shows red if a tenant’s params/outputs lag.
	•	Fail-safe: if a tenant’s train fails, scoring uses the last good params; signals keeps serving last good outputs.

Cadence & triggers
	•	Default: time-based Cron (nightly/weekly) → automatic updates.
	•	Optional: add “retrain if drift” logic in the train job (e.g., fit only if error/volume changed past a threshold).
	•	Manual override: bump model_version in config to roll a new param set; jobs write alongside and switch reads by version.

Where each piece lives (files)
	•	Training math: jobs/<domain>/<model>_train/pipeline/<model>_train_pipeline.go
	•	Scoring math: jobs/<domain>/<model>/pipeline/<model>_pipeline.go
	•	Param table schema: schemas/models/<model>_params.schema.json
	•	Cron: deploy/k8s/jobs/<model>-train-cronjob.yaml
	•	Tenant scoping: in datastore helpers/queries and services, using your existing context keys

Bottom line: once you add the train jobs and point scoring to “latest params per store_id,” the system continuously adapts per tenant with no manual steps, and your serving stays fast and stable.




Yes. Those are exactly the cases where you’d expose data via GraphQL so the UI can fetch per-request and update the page in real time.

Models that typically need online fetching (GraphQL/gRPC)
	•	#2 Recommender (PDP/PLP) – fetch ranked items for the current user/page.
	•	#3 Basket / Session Re-rank – fetch add-ons for the current cart.
	•	#7 Similarity / Neighbors – “you may also like” on PDP/PLP.
	•	#8 PLP/Search Ranking – results listing; often service-side but UI still queries the ranked list.
	•	#11 Upsell Propensity – show/decline upsell in checkout.
	•	#12 Cross-sell – complements for the current hero item.

(All served by signals with thin services + GraphQL queries; tiny per-request math allowed.)

Models that are usually offline (no GraphQL unless you want dashboards)
	•	#1 CLV, #5 Personas, #6 Affinities (can be served if you drive personalization from UI),
	•	#4 Promotion Uplift (assignments read server-side),
	•	#9 Markdown, #10 Elasticity, #13 Bundles,
	•	#14 Churn, #15 Lifecycle (used for triggers/targeting).

Rule of thumb: if the page needs it now to render or react to user behavior, expose it via GraphQL; otherwise keep it batch-only and consume server-side.






Both—driven by context + precomputed data. Here’s how the system “knows” what to do:

How it decides at request time
	•	Context keys (from middleware): storeID, merchantID, userID, sessionID/cartID, page type (PDP/PLP/Cart/Checkout).
	•	Resolver selects dataset(s):
	•	PDP: similarity + cross_sell (and filter by stock/price).
	•	PLP/Search: ranked list (rank_weights + product signals).
	•	Cart/Checkout: basket re-rank or upsell_scores with guardrails.
	•	Scope: queries are per-tenant (store) and, when relevant, per-customer/session.

What’s per-customer vs aggregated
	•	Per-customer (user-level): CLV, affinities, churn_risk, lifecycle_state.
	•	Per-session (cart/page): basket re-rank, upsell decision, cross-sell for current hero.
	•	Per-product / pair / global: product_signals, affinity_edges, rank_weights, markdown_plans, elasticity, bundles.

Where the numbers come from
	•	Jobs compute and refresh:
	•	Per-tenant parameters (weights, priors) via train jobs on a schedule.
	•	Per-customer / per-product outputs via scoring jobs (partitioned by storeID, and userID when applicable).
	•	Services combine: lightweight, real-time filters (stock, price, OOS caps) on top of the precomputed rows.

Fallback logic
	•	Try per-customer/session first → if sparse, use segment/cohort → else use tenant-level defaults.
	•	Guardrails (e.g., don’t harm base conversion) are checked per request.

Net: models adapt per tenant continuously (scheduled jobs) and act per customer/session at runtime (GraphQL calls), using the right dataset for the page and the current context.


Short answer: they’re not all bare-bones—several already depend on per-customer/session interaction signals. But yes, some flows/feature pipelines you’re imagining (e.g., “if they clicked X or filled Y, infer Z persona”) are not yet defined and would need one small aggregation job added.

What you have now (by model)

Mostly “formula/optimization” (math + standard inputs)
	•	#1 CLV, #9 Markdown, #10 Elasticity, #13 Bundle
These run with transactional data (orders, prices, stock) and store-level params. No special nav tracking required.

Already interaction-driven (per customer/session/page)
	•	#2 Recommender, #3 Basket re-rank, #6 Affinities, #7 Similarity, #8 PLP RankNet, #11 Upsell, #12 Cross-sell, #14 Churn, #15 Lifecycle
These expect logs like views/clicks/searches/ATC/purchases (by user/session, with timestamps). Affinities is explicitly “navigational” via time-decayed events; RankNet uses search logs; Basket/Upsell/Cross-sell use current cart/page context.

“Persona” can use nav patterns—if you feed them
	•	#5 Personas (HDBSCAN): your spec lists R/F/M + engagement + category shares; you can (and should) add navigation features (e.g., brand/category view time, click depth, form submits) to make personas reflect browsing intent—this is just features, not a new model.

What’s missing if you want rules like “clicked X → persona Y”

You need a tiny feature aggregation job that turns raw events into per-customer signals the models can consume.
	•	Add once (if you want richer nav signals):
	•	configs/customer_signals.yaml (inputs: events; output: customer_signals)
	•	jobs/customer/signals/{cmd/main.go, pipeline/customer_signals_pipeline.go, customer_signals_validate.go}
	•	schemas/models/customer_signals.schema.json (e.g., views_by_brand, views_by_category, click_depth, form_flags, recency)
	•	Optionally expose via signals (customer_signals GraphQL), or just read it inside #5 Personas, #6 Affinities, #14 Churn, #15 Lifecycle.

With that in place, your “if they clicked this object or filled that form” logic becomes features feeding Personas/Affinities/Churn/Lifecycle. You can also add simple rules in the Basket/Upsell/Cross-sell services (lightweight, per-request) that check these flags.

How decisions run (tenant + user)
	•	Per-tenant: train/fitting jobs write <model>_params by store_id; scoring jobs read the latest per tenant—updates happen automatically by Cron.
	•	Per-customer/session: scoring jobs write per-user outputs (CLV, churn, affinities) and per-session candidates (basket); GraphQL fetches the right rows at request time using your context (store/user/session).

Bottom line
	•	No, they’re not just bare equations; many already use navigation-style logs.
	•	To get the specific “clicked X/filled Y → persona Z” behavior, add one small signals job to aggregate those event flags/metrics and feed them into #5/#6/#14/#15 (and light rules in #3/#11/#12 if you want UI-time tweaks).
	•	The current layout is suitable and expects per-tenant, continuously updated parameters and per-customer decisions at runtime.

  What your stack needs to run all models via IaC

Compute & Runtime
	•	Containers: Build images for each job under analytics/jobs/** and for signals/cmd.
	•	Orchestrator: Kubernetes (CronJobs for batch; Deployments for services) or equivalent.
	•	Artifact registry: Store versioned images for jobs and the signals service.
	•	Schedulers: One CronJob per model job; pass config via env/ConfigMap/Secret.

Data Stores & Schemas
	•	Core inputs: Postgres and/or ClickHouse DSNs for reads per configs/*.yaml.
	•	Outputs: Postgres tables for customer_segments, product_signals; ClickHouse for affinity_edges if chosen.
	•	Indexes: Add keys on id + model_version + as_of per schemas/models/*.json.
	•	Retention/backups: Enable backups and retention on Postgres/ClickHouse.

Config & Secrets
	•	Runtime configs: Mount analytics/configs/*.yaml (jobs) and signals.yaml (serving).
	•	Secrets: Env vars for DSNs (CORE_PG_DSN, CORE_CH_DSN, SIGNALS_PG_DSN, SIGNALS_CH_DSN).
	•	SLOs: Honor freshness_hours, min_rows, timeouts_ms defined in signals.yaml.

Batch Jobs (analytics/jobs)
	•	Entrypoints: cmd/main.go loads config → runs pipeline/*.go → calls *_validate.go → upserts.
	•	Windows: Use common/timewin/window.go for last_30d/90d/180d etc.
	•	Datastore helpers: Implement reads/writes in common/datastore/{postgres.go,clickhouse.go}.
	•	SLOs: Enforce job_slo (timeout, retries, backoff) per job in the process context.
	•	Idempotency: Upsert on (id, model_version, as_of); dedupe in *_validate.go.

Serving Layer (signals/)
	•	Service: Deploy signals as a long-running service (gRPC + GraphQL).
	•	Store factory: Bind datasets to stores per signals/internal/store/factory.go.
	•	Validators: Enforce paging limits and required fields in signals/internal/validators/*.
	•	Schemas & resolvers: Expose customer_segments, product_signals, affinity_edges via GraphQL and proto.

Monitoring & Ops (from safety.yaml)
	•	Boot checks: Ping stores, verify mappings, check freshness/row count; fail fast on error.
	•	Health: Expose /healthz and /readyz on signals with SLO awareness.
	•	Ops API: Add opsStatus GraphQL for dashboard status.
	•	Metrics: Emit gauges/counters (boot ok, dataset ok, rows, maxAgeHours, job rows, job runtime).
	•	Logs: One boot line with bindings; per-job completion lines with rows and duration.

Security & Access
	•	Network: Allow signals and jobs to reach Postgres/ClickHouse over private endpoints.
	•	IAM/roles: Least-privilege DB roles; read-only for serving; read+write for jobs.
	•	Secrets mgmt: Kubernetes Secrets or cloud secret manager; never hardcode DSNs.

CI/CD
	•	Build: go build for each job binary and for signals; dockerize.
	•	Test: Lint configs; ensure signals.yaml parses and bindings exist.
	•	Deploy: Apply Terraform/Pulumi for DBs, networks, clusters, and K8s manifests for jobs/services.
	•	Versioning: Tag images with model version; propagate into configs/<model>.yaml.

Model-Specific Notes
	•	#1 CLV (BG/NBD + Gamma-Gamma): Add a small fitting routine (once per store/segment) to estimate r, α, a, b, p, q, γ; store in config or a small table; use same job pattern.
	•	#2/#7/#12 Similarity/ANN (optional): If you choose ANN, provision FAISS/Milvus/OpenSearch-kNN and load vectors; otherwise serve co-buy cosine from tables.
	•	#8 RankNet (optional train): If you train pairwise weights offline, add a training job and persist weights; scoring runs in service or batch.
	•	All jobs: Keep money in cents; enforce store/tenant scoping in datastore helpers.

Operational Runbooks
	•	Backfill: Trigger jobs with a specific window and model_version via ad-hoc CronJob runs.
	•	Staleness: Check opsStatus; rerun affected job; confirm upserts.
	•	Schema drift: Update schemas/models/*.json, add DB migration, bump model_version.

Bottom line: containerize jobs, wire DSNs and configs, schedule with CronJobs, serve via signals, and enforce the SLO/health rules from safety.yaml.



Below is a five‑page, layout‑first outline of your Analytics slice—focused on folders & files, what logic lives where, how the 15 models plug in, the databases in play, and the heavy system‑wide packages/services you’ll want. It’s intentionally neutral on language choices beyond Go being the shell for analytics jobs and signals; alternative runtimes/kernels are called out as optional.

⸻

Page 1 — Topology, Datastores, Conventions

1) Runtime topology (batch → store → serve)
	•	Jobs (batch compute; K8s CronJobs):
Read inputs (Postgres/ClickHouse), run model math in pipeline/*.go, validate, upsert rows to serving tables. No UI latency.
	•	Signals (serving service; K8s Deployment/Service):
Thin GraphQL/gRPC façade for datasets. Per‑request: validate + read rows + light filters/paging. No heavy math here.
	•	Frontend / other services:
Next.js (Apollo) → GraphQL (signals). Internal services can use gRPC (protobuf). Tenant/user context injected by your existing middleware (store_id, merchant_id, user_id, etc.).

2) Datastores (analytics slice)
	•	Postgres (PG): canonical serving store for model outputs & parameter tables.
Examples: tiers_clv, cross_sell, upsell_scores, markdown_plans, rank_weights, *_params.
	•	ClickHouse (CH): wide scans, edges, high‑volume logs.
Examples: affinity_edges, raw event aggregations, similarity build inputs.
	•	(Core app still uses Mongo) for operational entities—unchanged; analytics reads primarily PG/CH.

3) Conventions & SLOs (apply to all models)
	•	Keys & versioning: each row includes the entity key (e.g., user_id or product_id), model_version, as_of, and store_id.
	•	Units: money in cents; probabilities in [0,1]; ISO‑8601 timestamps.
	•	Freshness SLOs: PG datasets ≤ 36h; CH edges ≤ 72h. Signals enforces at boot (/readyz) and via opsStatus GraphQL.
	•	Tenancy: every query/row is partitioned by store_id (and merchant_id if applicable).

4) What “compute vs serve” means here
	•	Compute (jobs): joins/aggregations, training/parameter fitting, optimizers (DP/MILP), similarity builds, ranker fits.
	•	Serve (signals): fast reads of precomputed rows; only micro‑scoring allowed (e.g., dot‑product with weights).

⸻

Page 2 — Repository Layout & File Responsibilities

Your analytics/ tree (summarized; only files that matter for logic):

1) analytics/common/ — shared plumbing (jobs only)
	•	config/load.go
Role: strict loaders for configs/signals.yaml & configs/<model>.yaml.
Logic: parse YAML → apply env overrides to DSN fields → validate required keys (serving bindings, SLOs, job SLOs). Fail fast on missing keys/envs.
	•	datastore/postgres.go
Role: timed reads & upserts into PG.
Logic: SelectFreshness(table), SelectCount(table), UpsertRows(table, rows, keys=[id,model_version,as_of,store_id]). Return affected counts, wrap errors with context deadlines.
	•	datastore/clickhouse.go
Role: timed CH reads/writes for wide scans/edges.
Logic: same helpers as PG, optimized inserts (batch), error wrapping.
	•	timewin/window.go
Role: rolling windows & custom ranges.
Logic: last_30d/90d/180d/365d, Custom(start,end).

2) analytics/configs/ — declarative IO & SLOs
	•	signals.yaml
Role: dataset→backend map + serving SLOs.
Keys: serving.<dataset> {store: postgres|clickhouse, dsn_env: ...}, slo.freshness_hours, slo.min_rows, slo.timeouts_ms, slo.paging.max_limit.
	•	<model>.yaml (one per job)
Role: job IO + bounds.
Keys: inputs[] {store, dsn_env, table/filters}, window, output {table, store, dsn_env}, model_version, job_slo {max_runtime_sec,max_retries,backoff_sec}.

3) analytics/jobs/<domain>/<model>/
	•	cmd/main.go
Role: orchestrator.
Logic: load <model>.yaml → deadline ctx → read inputs via datastore helpers → run pipeline → run *_validate.go → upsert → log rows & duration.
	•	pipeline/<model>_pipeline.go
Role: all model math & shaping.
Logic: assemble inputs; compute features/scores/decisions; write well‑typed rows matching schema.
	•	pipeline/<model>_validate.go
Role: row guards.
Logic: required fields present; dedupe on (id,model_version,as_of,store_id); numeric bounds; clamp or drop bad rows.
	•	Dockerfile
Role: static build; non‑root runtime.
Notes: add build tools only when optional native kernels are used.

4) analytics/schemas/models/
	•	<dataset>.schema.json
Role: contract docs for output tables.
Logic: no runtime enforcement; use *_validate.go for gates.

5) analytics/signals/ — read‑only serving layer
	•	cmd/main.go
Boot: load signals.yaml, configure default per‑request timeout, expose /healthz (pings) & /readyz (SLO checks).
	•	internal/store/factory.go
Boot checks: each serving.<dataset> has a matching store impl (*_store_pg.go/*_store_ch.go); ping DSNs; MAX(as_of) & COUNT(*) vs SLOs; fail fast on breach.
	•	internal/services/*
Role: thin dataset readers; paging/filters; no heavy compute.
	•	internal/validators/*
Role: request guards (IDs, paging cap, numeric ranges).
	•	internal/graph/schema/*.graphqls + *.resolver.go
Role: GraphQL types/queries; resolvers call services.
	•	proto/*.proto
Role: optional gRPC contracts echoing GraphQL types.

6) deploy/ & dashboards/
	•	deploy/k8s/jobs/*.yaml → CronJobs per model (image tag, schedule, DSN envs, deadlines/retries).
	•	deploy/k8s/services/signals-*.yaml → signals Deployment/Service/Ingress.
	•	deploy/k8s/secrets/*.yaml → DSN secrets.
	•	deploy/observability/prometheus-scrape.yaml → scrape jobs/signals.
	•	dashboards/analytics-ops.json → Grafana board for dataset health & job metrics.

⸻

Page 3 — Pipeline Contract, Ops & Parameterization

1) Job contract (every model)
	•	Inputs: tables + filters from configs/<model>.yaml (PG/CH).
	•	Compute: features → scores/decisions; never reach back to network except DB.
	•	Validate: enforce required IDs, units, ranges; dedupe keys.
	•	Upsert: PG preferred for serving; CH for edges/heavy logs.
	•	Log & metrics: rows written, duration; Prometheus counters (e.g., job_rows_written{job}, job_runtime_seconds{job}).

2) Parameter tables & cadence (for learned models)
	•	Pattern: weekly fit job → write <model>_params (PG) → scoring job reads latest per store_id.
	•	Examples:
clv_params (r,alpha,a,b,p,q,gamma) • rank_weights • churn_params • upsell_params • cross_sell_params • hsmm_params • policy knobs (markdown/bundle/elasticity/affinities).
	•	Freshness: enforce with SLOs; scoring uses last good params if a fit fails.

3) Signals service contract
	•	Request: tenant context from middleware (store_id/merchant_id/user_id), paging inputs, entity IDs.
	•	Response: rows with model_version and as_of so UI/Admin can display provenance & detect staleness.
	•	Ops GraphQL (opsStatus): dataset name, store, rows, maxAgeHours, lastAsOf, ok/message.

4) Observability & health
	•	/healthz: process up; backends ping OK.
	•	/readyz: includes SLO checks (freshness & row counts).
	•	Metrics:
	•	Serving: signals_dataset_ok{dataset}, signals_dataset_rows{dataset}, signals_dataset_max_age_hours{dataset}.
	•	Jobs: job_rows_written{job}, job_runtime_seconds{job}.

5) Multi‑tenancy & RBAC
	•	Queries: always filter by store_id (and merchant_id if applicable).
	•	RBAC (optional): enforce dataset access via Casbin in signals services/resolvers (tenant/dataset/role).
	•	Backfills: run jobs with window override + bump model_version; rows dedupe by keys.

⸻

Page 4 — Model Index & Wiring Map (15 models + 2 signals jobs)

Each entry: Dataset, Inputs (PG/CH tables), Param table?, Serve?, Cadence. Keep dataset names stable; keep param tables small.

Foundation signals jobs (add first):
	•	Visitor Cohorts → visitor_cohorts (PG)
Inputs: CH events (views/clicks/ATC/checkout), products (brand/category).
Serve: optional (visitorCohort(cohortId)); Daily.
	•	Customer Signals → customer_signals (PG)
Inputs: events, orders, campaign logs (if present), products.
Serve: optional (customerSignals(userId)); Daily.

1) CLV (BG/NBD + Gamma–Gamma)
	•	Dataset: tiers_clv (PG)
	•	Inputs: orders (margins/refunds/placed_at), customers (first_purchase_at/status).
	•	Params: clv_params (optional weekly fit).
	•	Serve: usually No (used by targeting), optional for admin; Score daily.

2) Recommender (two‑stage)
	•	Dataset: reuse rank_weights (if a learned ranker) + possibly product_signals features.
	•	Inputs: products (margin, stock), events (recent views), returns rates.
	•	Params: rank_weights (weekly).
	•	Serve: Yes (PDP/PLP). Daily refresh + stock gate.

3) Basket / Session Re‑rank
	•	Dataset: upsell_scores (or a basket table if you split)
	•	Inputs: cart items, co‑buy lift, margins, stock (compat/substitute flags, returns risk).
	•	Params: optional thresholds (config).
	•	Serve: Yes (cart/checkout). Daily + runtime stock guard.

4) Promotion Uplift
	•	Dataset: promotion_assignments (PG)
	•	Inputs: offers log w/ holdout; past GM; RFME features; campaign constraints.
	•	Params: uplift_params if cost caps/τ model learned.
	•	Serve: usually No (assignment → orchestration). Weekly.

5) Personas (HDBSCAN/K‑Means)
	•	Dataset: personas (PG)
	•	Inputs: RFMT + engagement + category share.
	•	Params: personas_params (optional thresholds).
	•	Serve: No (unless UI uses it). Weekly/Monthly.

6) Affinities (decayed counts)
	•	Dataset: enrich product_signals or own customer_affinities table.
	•	Inputs: events (views/buys by brand/category); smoothing params.
	•	Params: affinities_params (half‑life/alpha).
	•	Serve: Optional; Daily.

7) Similarity / Neighbors
	•	Dataset: similarity_artifacts (PG meta); neighbors served from ANN engine.
	•	Inputs: co‑buy pairs; attributes/price for cold start.
	•	Params: ANN index metadata.
	•	Serve: Yes (PDP/PLP). Weekly index build (daily possible).

8) PLP/Search Rank (LTR)
	•	Dataset: rank_weights (PG)
	•	Inputs: search logs (pairs), product features (bm25/semantic/margin/stock/price/returns).
	•	Params: rank_weights Weekly.
	•	Serve: Yes (PLP/Search). Daily features update.

9) Markdown Optimizer (DP/MILP)
	•	Dataset: markdown_plans (PG)
	•	Inputs: inventory (units/age), price ladder, demand quantiles, COGS/salvage.
	•	Params: markdown_params (policy knobs).
	•	Serve: No (batch plan). Weekly (or ad‑hoc).

10) Price Elasticity (gated)
	•	Dataset: elasticity (PG)
	•	Inputs: price_history (price/units by SKU/week), promo flags, COGS, seasonality dummies.
	•	Params: elasticity_params (optional priors/guards).
	•	Serve: No. Weekly/Monthly.

11) Upsell Propensity (profit‑guarded)
	•	Dataset: upsell_scores (PG)
	•	Inputs: upsell catalog (pairs, delta price/margin), offers log (shown/accepted), margins, lambda guard.
	•	Params: upsell_params (lambda/baseGM or learned coef_*).
	•	Serve: Yes (checkout). Daily scoring.

12) Cross‑sell (graph embeddings)
	•	Dataset: cross_sell (PG)
	•	Inputs: item embeddings & co‑buy lift; margins/stock/category role; substitutes.
	•	Params: cross_sell_params (optional β’s).
	•	Serve: Yes (PDP/cart). Daily.

13) Bundle Optimizer (slow‑mover + hero)
	•	Dataset: bundles (PG)
	•	Inputs: slow movers, margins, attach priors, MAP/brand rules, max bundles.
	•	Params: bundle_params (constraints).
	•	Serve: No. Weekly.

14) Churn Propensity
	•	Dataset: churn_risk (PG)
	•	Inputs: recency, orders_90d, ln_margin_90d, tenure, active_days_30d; action costs/gains.
	•	Params: churn_params (coef_*, thresholds).
	•	Serve: No (or admin only). Daily/Weekly.

15) Lifecycle Timing (HSMM; Hawkes opt.)
	•	Dataset: lifecycle_state (PG)
	•	Inputs: weekly events (visits/opens/clicks), weekly orders (count/spend), duration PMFs; optional Hawkes params; subscription renewals (plugin).
	•	Params: hsmm_params.
	•	Serve: No (or admin). Weekly.

⸻

Page 5 — System‑Wide Packages & Services (Heavy), CI/K8s, and Handoff

1) “Heavy” packages/services to plan for (install orchestrations handled later)

Required (core)
	•	Go libs:
gopkg.in/yaml.v3 (YAML) • github.com/jackc/pgx/v5 (PG) • github.com/ClickHouse/ch-go (CH) • github.com/99designs/gqlgen (GraphQL) • google.golang.org/grpc, google.golang.org/protobuf (gRPC) • github.com/prometheus/client_golang (metrics) • golang.org/x/sync/errgroup (concurrency) • gonum.org/v1/gonum (math/stats).
	•	Tooling: protoc, protoc-gen-go, protoc-gen-go-grpc, gqlgen generator.
	•	Infra: Postgres, ClickHouse, Kubernetes (CronJobs + Deployment), Container Registry, Secrets Manager/K8s Secrets, Prometheus + Grafana.

Optional (model‑driven; pick per need)
	•	ANN engine (#7/#2/#12):
OpenSearch k‑NN (+ opensearch-go) or Milvus (+ milvus-sdk-go) or FAISS via C++.
	•	Optimizers (#4/#9):
OR‑Tools (C++ core; cgo wrapper or Python job) or CBC/GLPK (OSS libs + Go bindings).
	•	Python job containers (selective):
HDBSCAN/personas (#5), advanced time‑series/HSMM (#15), XGBoost ranker (#8), OR‑Tools if you prefer Python.
	•	Native kernels (optional for hotspots):
C++/Rust libs + cgo wrapper placed under analytics/native/…. Only when profiling justifies (ANN build, DP/MILP, heavy numeric).

2) CI/CD & K8s (what exists/what to add per model)
	•	CI: build/test Go; docker build/push (tag by model_version).
	•	CronJobs: one per job under deploy/k8s/jobs/*.yaml with schedule and DSN envs.
	•	Signals Deployment: deploy/k8s/services/signals-deployment.yaml + Service/Ingress.
	•	Secrets: deploy/k8s/secrets/* hold DSNs (CORE_PG_DSN, CORE_CH_DSN, SIGNALS_PG_DSN, SIGNALS_CH_DSN).
	•	Observability: deploy/observability/prometheus-scrape.yaml + dashboards/analytics-ops.json.

3) Add‑a‑model checklist (copy/paste ritual)
	1.	Create schema doc: schemas/models/<dataset>.schema.json.
	2.	Add job config: configs/<model>.yaml (inputs, window, output table, model_version, job SLO).
	3.	Scaffold job: jobs/<domain>/<model>/{cmd/main.go, pipeline/*.go, Dockerfile}.
	4.	Cron: deploy/k8s/jobs/<model>-cronjob.yaml (image, schedule, DSNs, deadlines/retries).
	5.	Serve? If yes, add: signals/proto/*.proto (optional), signals/internal/services/*, signals/internal/graph/schema/*.graphqls + resolver, and bind serving.<dataset> in configs/signals.yaml.
	6.	Params? If yes, create sibling fit job writing <model>_params; scoring reads “latest per store_id”.

4) Runbooks (ops‑centric)
	•	Boot failure (signals): check signals.yaml mapping & DSN envs; factory log shows which dataset/store failed; fix & redeploy.
	•	SLO breach (stale/missing rows): run the producing job; confirm as_of & row count; verify /readyz & opsStatus.
	•	Backfill: run CronJob with date override and bump model_version; dedupe keys prevent clobbering.

5) Language neutrality (for the next thread)
	•	This outline keeps compute in jobs and serving in signals, independent of the underlying math runtime.
	•	For any model, you can choose: pure Go, Go + native kernel, or a small Python fit job that writes params/artifacts for Go to serve. The file/folder responsibilities above don’t change with that choice.

⸻

Use this as the handoff bundle.
	•	Pages 1–2 tell where every piece lives and how it talks.
	•	Page 3 locks down the job & signals contracts, SLOs, and parameterization.
	•	Page 4 gives the 15‑model wiring map (datasets/inputs/serve/cadence).
	•	Page 5 inventories the heavy packages & services and the repeatable add‑a‑model checklist.

Pass this to the next thread to finalize package installs and pick the implementation runtimes per model (Go only vs Go+native vs Python fit), then begin scaffolding the first jobs and signals surfaces.